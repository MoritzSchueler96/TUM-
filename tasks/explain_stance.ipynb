{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forword"
   ]
  },
  {
   "source": [
    "In this notebook we will generate SAGE plots, that will help to understand the model predictions of the stance classifier.\n",
    "For more information about how SAGE works, we kindly refer you to this excellent blog post: https://iancovert.com/blog/understanding-shap-sage/\n",
    "\n",
    "For more details on the implementation, here is the official git repository: https://github.com/iancovert/sage"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from nlp_utils.data_module import SemEvalDataModule\n",
    "from nlp_utils.model import CustomDistilBertModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from glob import glob\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'notebookDir' in globals():\n",
    "    notebookDir = os.getcwd()\n",
    "print('notebookDir: ' + notebookDir)\n",
    "os.chdir(notebookDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get folder where logs are stored\n",
    "save_folder = \"../logs/StancePrediction_SemEval/lightning_logs/\"\n",
    "save_folder = os.path.join(notebookDir, save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Dropdown(description='Select a checkpoint:', options=('version_26/checkpoints/epoch=0-val_loss=1.96-val_epoch_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca6a4711b2aa45b1b5ef36682ca88f41"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Select a model\n",
    "os.chdir(save_folder)\n",
    "w = widgets.Dropdown(\n",
    "    options=glob(os.path.join('*/checkpoints/*.ckpt')),\n",
    "    description='Select a checkpoint:'\n",
    ")\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'dataset_path': '../../data/raw/SemEval/',\n",
       "  'learning_rate': 0.0030806995333433384,\n",
       "  'batch_size': 16,\n",
       "  'epochs': 20,\n",
       "  'num_trials': 50,\n",
       "  'vocab_size': 30522,\n",
       "  'target_encoding': {0: 'Atheism',\n",
       "   1: 'Climate Change is a Real Concern',\n",
       "   2: 'Feminist Movement',\n",
       "   3: 'Hillary Clinton',\n",
       "   4: 'Legalization of Abortion'},\n",
       "  'stance_encoding': {0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'}},\n",
       " 'version_26')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "os.chdir(notebookDir)\n",
    "model_path = os.path.join(save_folder, w.value)\n",
    "model_version = re.findall(\"version_[0-9]+\", model_path)[0]\n",
    "model = CustomDistilBertModel.load_from_checkpoint(model_path)\n",
    "data_module = SemEvalDataModule(num_workers=4, config=model.config)\n",
    "data_module.setup('')\n",
    "\n",
    "model.config, model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Testing:  99%|█████████▊| 78/79 [00:56<00:00,  1.47it/s]\n",
      "\n",
      "============\n",
      "Results\t\t\t\t \n",
      "============\n",
      "FAVOR     precision: 0.3222 recall: 0.9211 f-score: 0.4774\n",
      "AGAINST   precision: 0.9286 recall: 0.2727 f-score: 0.4216\n",
      "------------\n",
      "Macro F: 0.4495\n",
      "\n",
      "Testing: 100%|██████████| 79/79 [00:56<00:00,  1.40it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_epoch_F1': 0.656525194644928,\n",
      " 'test_epoch_target_F1': 0.656525194644928,\n",
      " 'test_loss': 0.9024659395217896}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 0.9024659395217896,\n",
       "  'test_epoch_target_F1': 0.656525194644928,\n",
       "  'test_epoch_F1': 0.656525194644928}]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# check performance\n",
    "trainer = pl.Trainer(deterministic=True)\n",
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "source": [
    "## Auto select best model from Raytune hyperparameter optimization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best config\n",
    "def get_best_config(path):\n",
    "    scores = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if \".ckpt\" in name:\n",
    "                score = name.split(\"val\")\n",
    "                score = [re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)[-1] for s in score]\n",
    "                score.append(os.path.join(root, name))\n",
    "\n",
    "                scores.append(score)\n",
    "\n",
    "    # filter scores for best version\n",
    "    df = pd.DataFrame(scores, columns=[\"epoch\", \"loss\", \"F1\", \"path\"])\n",
    "    ckpt = df.sort_values(by=[\"F1\", \"loss\"], ascending=[False,True]).head(1).path.values[0]\n",
    "    return ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'dataset_path': '../../data/raw/SemEval/',\n",
       "  'learning_rate': 0.0013774978663536918,\n",
       "  'batch_size': 32,\n",
       "  'epochs': 20,\n",
       "  'num_trials': 50,\n",
       "  'vocab_size': 30522,\n",
       "  'target_encoding': {0: 'Atheism',\n",
       "   1: 'Climate Change is a Real Concern',\n",
       "   2: 'Feminist Movement',\n",
       "   3: 'Hillary Clinton',\n",
       "   4: 'Legalization of Abortion'},\n",
       "  'stance_encoding': {0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'}},\n",
       " 'version_28')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# take best model\n",
    "best_model_path = get_best_config(save_folder)\n",
    "best_model_version = re.findall(\"version_[0-9]+\", best_model_path)[0]\n",
    "\n",
    "best_model = CustomDistilBertModel.load_from_checkpoint(best_model_path)\n",
    "best_data_module = SemEvalDataModule(num_workers=4, config=model.config)\n",
    "best_data_module.setup('')\n",
    "\n",
    "best_model.config, best_model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Testing:  99%|█████████▊| 78/79 [00:42<00:00,  2.02it/s]\n",
      "\n",
      "============\n",
      "Results\t\t\t\t \n",
      "============\n",
      "FAVOR     precision: 0.6468 recall: 0.5000 f-score: 0.5640\n",
      "AGAINST   precision: 0.7426 recall: 0.7385 f-score: 0.7405\n",
      "------------\n",
      "Macro F: 0.6523\n",
      "\n",
      "Testing: 100%|██████████| 79/79 [00:42<00:00,  1.84it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_epoch_F1': 0.7502001523971558,\n",
      " 'test_epoch_target_F1': 0.7502001523971558,\n",
      " 'test_loss': 0.7219581007957458}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7219581007957458,\n",
       "  'test_epoch_target_F1': 0.7502001523971558,\n",
       "  'test_epoch_F1': 0.7502001523971558}]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# check performance\n",
    "trainer = pl.Trainer(deterministic=True)\n",
    "trainer.test(best_model, datamodule=best_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model\n",
    "model = best_model\n",
    "# data_module = data_module\n",
    "data_module = best_data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import spacy\n",
    "import pickle\n",
    "import json\n",
    "import sage\n",
    "import nltk\n",
    "import string\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from transformers import DistilBertTokenizer\n",
    "from numpy.random import default_rng\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download vocabularies - this might take some time for the first time\n",
    "!python -m spacy download en &> /dev/null\n",
    "!python -m spacy download en_core_web_lg &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'},\n",
       " {0: 'Atheism',\n",
       "  1: 'Climate Change is a Real Concern',\n",
       "  2: 'Feminist Movement',\n",
       "  3: 'Hillary Clinton',\n",
       "  4: 'Legalization of Abortion'})"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# display encoding\n",
    "data_module.stance_encoding, data_module.target_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_module, part=\"all\"):\n",
    "    # train\n",
    "    text, label = data_module.trainset.texts, data_module.trainset.labels\n",
    "    df_train = pd.DataFrame(data=(text), columns=[\"Text\"])\n",
    "    df_train[\"Stance\"] = label[0]\n",
    "    df_train[\"Target\"] = label[1]\n",
    "\n",
    "    # val\n",
    "    text, label = data_module.valset.texts, data_module.valset.labels\n",
    "    df_val = pd.DataFrame(data=(text), columns=[\"Text\"])\n",
    "    df_val[\"Stance\"] = label[0]\n",
    "    df_val[\"Target\"] = label[1]\n",
    "    #test\n",
    "    text, label = data_module.testset.texts, data_module.testset.labels\n",
    "    df_test = pd.DataFrame(data=(text), columns=[\"Text\"])\n",
    "    df_test[\"Stance\"] = label[0]\n",
    "    df_test[\"Target\"] = label[1]\n",
    "\n",
    "    if part == \"all\":\n",
    "        df = pd.concat([df_train, df_val, df_test])\n",
    "        return df\n",
    "    elif part == \"train\":\n",
    "        return df_train\n",
    "    elif part == \"val\":\n",
    "        return df_val\n",
    "    else:\n",
    "        return df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text   Stance  \\\n",
       "0  Don't get it twisted. A major presidential can...  AGAINST   \n",
       "1  The Dukes of Hazzard has been on tv for 36 yea...  AGAINST   \n",
       "2  #BlackLivesMatter unless they are pre born bla...  AGAINST   \n",
       "3  Of mothers advising their daughter's to abort ...  AGAINST   \n",
       "4  If you want to empower women, you need to dise...    FAVOR   \n",
       "\n",
       "                     Target  \n",
       "0           Hillary Clinton  \n",
       "1           Hillary Clinton  \n",
       "2  Legalization of Abortion  \n",
       "3  Legalization of Abortion  \n",
       "4         Feminist Movement  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Stance</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Don't get it twisted. A major presidential can...</td>\n      <td>AGAINST</td>\n      <td>Hillary Clinton</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The Dukes of Hazzard has been on tv for 36 yea...</td>\n      <td>AGAINST</td>\n      <td>Hillary Clinton</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#BlackLivesMatter unless they are pre born bla...</td>\n      <td>AGAINST</td>\n      <td>Legalization of Abortion</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Of mothers advising their daughter's to abort ...</td>\n      <td>AGAINST</td>\n      <td>Legalization of Abortion</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>If you want to empower women, you need to dise...</td>\n      <td>FAVOR</td>\n      <td>Feminist Movement</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# display data for inspection\n",
    "df1 = df.copy()\n",
    "df1[\"Stance\"] = df1[\"Stance\"].transform(lambda x: data_module.stance_encoding[x])\n",
    "df1[\"Target\"] = df1[\"Target\"].transform(lambda x: data_module.target_encoding[x])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_clusters(df, similarity_func, num_clusters):\n",
    "    X = df[\"Text\"]\n",
    "    print(\"Create Vocabulary...\")\n",
    "    stripped_of_syllables_vocab, stripped = create_vocab(X)\n",
    "    print(\"Cluster Vocabulary...\")\n",
    "    dictionary_of_vocab = cluster_vocab(X, stripped_of_syllables_vocab, stripped, similarity_func, num_clusters)\n",
    "    print(\"Finished...\")\n",
    "    \n",
    "    return dictionary_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(X):\n",
    "    nlp_bigger = spacy.load('en_core_web_lg')\n",
    "    list_of_list = [tokenizer.tokenize(x) for x in X.values]\n",
    "    flat_list = [item for sublist in list_of_list for item in sublist]\n",
    "    small_vocab = list(set(flat_list))\n",
    "    spacy_vocab = [nlp_bigger(x) for x in small_vocab] \n",
    "    vocab_2d = [[x] for x in spacy_vocab]\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_vocab = [w for w in spacy_vocab if not str(w) in stop_words]\n",
    "    filtered_stopwords = [str(w) for w in spacy_vocab if str(w) in stop_words]\n",
    "\n",
    "    #removing punctuation from vocab\n",
    "    #print(string.punctuation)\n",
    "    stripped_vocab = [w for w in filtered_vocab if not str(w) in string.punctuation] \n",
    "    stripped_punctuation = [str(w) for w in filtered_vocab if str(w) in string.punctuation]\n",
    "\n",
    "    #removing digits from vocab\n",
    "    stripped_of_digits_vocab = [w for w in stripped_vocab if not str(w).isdigit()]\n",
    "    stripped_digits = [str(w) for w in stripped_vocab if str(w).isdigit()]\n",
    "\n",
    "    #removing syllables from vocab\n",
    "    stripped_of_syllables_vocab = [w for w in stripped_of_digits_vocab if not \"#\" in str(w)] \n",
    "    stripped_syllables = [str(w) for w in stripped_of_digits_vocab if \"#\" in str(w)]\n",
    "    \n",
    "    stripped = (filtered_stopwords, stripped_punctuation, stripped_digits, stripped_syllables)\n",
    "\n",
    "    return stripped_of_syllables_vocab, stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_func(u, v):\n",
    "    if u[0].text == \"#\":\n",
    "        token_one = u[len(u)-1]\n",
    "    else:\n",
    "        token_one = u[0]\n",
    "    if v[0].text == \"#\":\n",
    "        token_two = v[len(v)-1]\n",
    "    else:\n",
    "        token_two = v[0]\n",
    "    return token_one.similarity(token_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_vocab(X, stripped_of_syllables_vocab, stripped, similarity_func, num_clusters):\n",
    "    stripped_of_syllables_vocab_2d = [[x] for x in stripped_of_syllables_vocab]\n",
    "    dists = pdist(np.array(stripped_of_syllables_vocab_2d), similarity_func) \n",
    "    similarity_matrix = squareform(dists)\n",
    "    cluster_model = AgglomerativeClustering(affinity='precomputed', n_clusters=num_clusters, linkage='complete').fit(1-similarity_matrix)\n",
    "\n",
    "    clustered_vocab = [[] for x in range(num_clusters)]\n",
    "    _ = [clustered_vocab[cluster_model.labels_[index]].append(str(x)) for index, x in enumerate(stripped_of_syllables_vocab)]\n",
    "\n",
    "    token_groupings = {str(index) : element for index, element in enumerate(clustered_vocab)} \n",
    "\n",
    "    token_groupings[\"stop_words\"] = stripped[0]\n",
    "    token_groupings[\"punctuation\"] = stripped[1]\n",
    "    token_groupings[\"digits\"] = stripped[2]\n",
    "    token_groupings[\"syllables\"] = stripped[3]\n",
    "\n",
    "    dictionary_of_vocab = {\n",
    "    \"description\": \"\",\n",
    "    \"text\": (X.values),\n",
    "    \"num_clusters\": num_clusters,\n",
    "    \"similarity_matrix\": similarity_matrix,\n",
    "    \"cluster_model\": cluster_model,\n",
    "    \"clustered_vocab\": clustered_vocab,\n",
    "    \"token_groupings\": token_groupings\n",
    "    }\n",
    "\n",
    "    return dictionary_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_dict(dictionary_of_vocab, filename):\n",
    "    file = open(os.path.join(notebookDir, \"../logs/StancePrediction_SemEval/vocab/\" + filename), 'wb')\n",
    "    pickle.dump(dictionary_of_vocab, file)\n",
    "    file.close()\n",
    "\n",
    "def open_vocab_dict(filename):\n",
    "    dictionary_of_vocab = pickle.load(open(os.path.join(notebookDir, \"../logs/StancePrediction_SemEval/vocab/\" + filename), 'rb'))\n",
    "    return dictionary_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up imputer object\n",
    "class Imputer_groupings:\n",
    "    def __init__(self, model, number_of_groups):\n",
    "        self.model = model\n",
    "        self.num_groups = number_of_groups\n",
    "    \n",
    "    def __call__(self, input_array, S):\n",
    "        max_length = 0\n",
    "        reconstructed_array = []\n",
    "        for index, sentence in enumerate(input_array):\n",
    "            length_of_sentence = max(x[0] for group in sentence for x in group)\n",
    "            if length_of_sentence > max_length:\n",
    "                max_length = length_of_sentence\n",
    "\n",
    "            original_input_ids = [None] * (length_of_sentence+1)\n",
    "            for sub_index, group in enumerate(sentence):\n",
    "                \n",
    "                if S[index][sub_index]: #put in '[MASK]' elements if needed\n",
    "                    for x in group:\n",
    "                        original_input_ids[x[0]]=x[1]\n",
    "                else: \n",
    "                    for x in group:\n",
    "                        original_input_ids[x[0]]=103 #id of '[MASK]'\n",
    "\n",
    "            original_input_ids.append(102)\n",
    "            original_input_ids.insert(0, 101)\n",
    "            reconstructed_array.append(original_input_ids)\n",
    "\n",
    "        max_length+=3\n",
    "\n",
    "        for index, sentence in enumerate(reconstructed_array):\n",
    "            if len(sentence) < max_length:\n",
    "                reconstructed_array[index].extend([0 for i in range(max_length-len(sentence))])\n",
    "\n",
    "        input_ids = np.array(reconstructed_array)\n",
    "\n",
    "        # \"handmade\" attention mask -> basically just set everything to one, except '[PAD]'s which are zero\n",
    "        am = np.ones(input_ids.shape)\n",
    "        am[input_ids == 0] = 0 #id of '[PAD]'\n",
    "        tensor_attention_mask = torch.tensor(am)\n",
    "        tensor_attention_mask = tensor_attention_mask.to(model.device)\n",
    "\n",
    "\n",
    "        tensor_input_ids = torch.tensor(input_ids)\n",
    "        tensor_input_ids = tensor_input_ids.to(model.device)\n",
    "\n",
    "        encoded_text = {\"input_ids\": tensor_input_ids, \"attention_mask\": tensor_attention_mask}\n",
    "        \n",
    "        #predict with model\n",
    "        outputs = self.model(encoded_text)[0]       \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        \n",
    "        score_most_prob = [max(x) for x in outputs]\n",
    "\n",
    "        return np.array(score_most_prob) # sp.special.logit(score_most_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_sage_input(dictionary_of_vocab_clusters, number_of_groups):\n",
    "    current_groupings = dictionary_of_vocab_clusters[\"token_groupings\"]\n",
    "    feature_names = list(current_groupings.keys())\n",
    "    sage_input_all_instances = []\n",
    "    for text_element in dictionary_of_vocab_clusters[\"text\"]:\n",
    "        sage_input_groupings = [[] for x in range(number_of_groups)]\n",
    "        input_text = tokenizer.tokenize(text_element)\n",
    "        input_ids = tokenizer(text_element, add_special_tokens=False)[\"input_ids\"]\n",
    "        groups_to_text = [next((key for key, value in current_groupings.items() if x in value)) for x in input_text]\n",
    "\n",
    "        positioned_ids = [(i, x) for i, x in enumerate(input_ids)]\n",
    "        _ = [sage_input_groupings[list(current_groupings.keys()).index(groups_to_text[index])].append(x)  for index, x in enumerate(positioned_ids)]\n",
    "        sage_input_all_instances.append(sage_input_groupings)\n",
    "\n",
    "    sage_input_all_instances_array = np.array(sage_input_all_instances, dtype=object)\n",
    "    return sage_input_all_instances_array"
   ]
  },
  {
   "source": [
    "### Automatically run a number of cluster sizes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Cluster Vocabulary...\n",
      "/home/user/miniconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/user/miniconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "Finished...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/tasks/vocab/dictionary_of_vocab_2cluster.pickle'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_216094/3287991346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdictionary_of_vocab_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_word_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msave_vocab_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_of_vocab_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dictionary_of_vocab_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cluster.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_216094/1693916749.py\u001b[0m in \u001b[0;36msave_vocab_dict\u001b[0;34m(dictionary_of_vocab, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_vocab_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_of_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebookDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vocab/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_of_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/tasks/vocab/dictionary_of_vocab_2cluster.pickle'"
     ]
    }
   ],
   "source": [
    "# create and save clustered vocabularies for a number of different cluster sizes and save them to the logs\n",
    "cluster_sizes = [3, 5, 7, 9, 11, 15, 17, 19, 21, 25, 29, 35, 45, 55]\n",
    "for n in cluster_sizes:\n",
    "    dictionary_of_vocab_clusters = create_word_clusters(df, similarity_func, n)\n",
    "    save_vocab_dict(dictionary_of_vocab_clusters, \"dictionary_of_vocab_\" + str(n) + \"cluster.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 18%|█▊        | 0.1806/1 [00:12<00:57, 69.70s/it]StdDev Ratio = 0.3530 (Converge at 0.1500)\n",
      " 21%|██▏       | 0.2142/1 [00:25<01:31, 116.84s/it]StdDev Ratio = 0.3241 (Converge at 0.1500)\n",
      " 25%|██▌       | 0.2512/1 [00:38<01:53, 151.96s/it]StdDev Ratio = 0.2993 (Converge at 0.1500)\n",
      " 39%|███▊      | 0.3861/1 [00:53<01:25, 138.70s/it]StdDev Ratio = 0.2414 (Converge at 0.1500)\n",
      " 51%|█████     | 0.5121/1 [01:11<01:08, 139.41s/it]StdDev Ratio = 0.2096 (Converge at 0.1500)\n",
      " 49%|████▉     | 0.4927/1 [01:25<01:27, 173.06s/it]StdDev Ratio = 0.2137 (Converge at 0.1500)\n",
      " 59%|█████▉    | 0.5919/1 [01:39<01:08, 168.35s/it]StdDev Ratio = 0.1950 (Converge at 0.1500)\n",
      " 53%|█████▎    | 0.5259/1 [01:52<01:41, 214.08s/it]StdDev Ratio = 0.2068 (Converge at 0.1500)\n",
      " 65%|██████▌   | 0.6528/1 [02:06<01:07, 194.53s/it]StdDev Ratio = 0.1856 (Converge at 0.1500)\n",
      " 64%|██████▎   | 0.6363/1 [02:19<01:19, 219.64s/it]StdDev Ratio = 0.1881 (Converge at 0.1500)\n",
      " 60%|██████    | 0.6029/1 [02:33<01:41, 254.91s/it]StdDev Ratio = 0.1932 (Converge at 0.1500)\n",
      " 62%|██████▏   | 0.6232/1 [02:46<01:40, 267.64s/it]StdDev Ratio = 0.1900 (Converge at 0.1500)\n",
      " 62%|██████▏   | 0.6233/1 [03:02<01:50, 293.38s/it]StdDev Ratio = 0.1900 (Converge at 0.1500)\n",
      " 63%|██████▎   | 0.6321/1 [03:16<01:54, 311.37s/it]StdDev Ratio = 0.1887 (Converge at 0.1500)\n",
      " 80%|████████  | 0.8026/1 [03:28<00:51, 259.57s/it]StdDev Ratio = 0.1674 (Converge at 0.1500)\n",
      " 72%|███████▏  | 0.7233/1 [03:43<01:25, 309.65s/it]StdDev Ratio = 0.1764 (Converge at 0.1500)\n",
      " 77%|███████▋  | 0.7703/1 [03:56<01:10, 306.72s/it]StdDev Ratio = 0.1709 (Converge at 0.1500)\n",
      " 75%|███████▍  | 0.7451/1 [04:08<01:25, 333.72s/it]StdDev Ratio = 0.1738 (Converge at 0.1500)\n",
      " 83%|████████▎ | 0.8277/1 [04:19<00:54, 313.79s/it]StdDev Ratio = 0.1649 (Converge at 0.1500)\n",
      " 80%|███████▉  | 0.7969/1 [04:32<01:09, 341.88s/it]StdDev Ratio = 0.1680 (Converge at 0.1500)\n",
      " 87%|████████▋ | 0.8651/1 [04:44<00:44, 329.29s/it]StdDev Ratio = 0.1613 (Converge at 0.1500)\n",
      " 90%|████████▉ | 0.8985/1 [04:56<00:33, 329.63s/it]StdDev Ratio = 0.1583 (Converge at 0.1500)\n",
      "100%|██████████| 1/1 [05:07<00:00, 307.78s/it]StdDev Ratio = 0.1486 (Converge at 0.1500)\n",
      "Detected convergence\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/tasks/plots/Stance_feature_importance_2cluster.png'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_216094/3026631921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstance_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msage_values_groupings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_fig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# save plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mstance_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebookDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"plots/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_feature_importance_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cluster.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# sage for target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2309\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2218\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    510\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    511\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/test/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/tasks/plots/Stance_feature_importance_2cluster.png'"
     ]
    }
   ],
   "source": [
    "# create sage plots for the different cluster sizes and save them to the logs\n",
    "for n in cluster_sizes:\n",
    "    dictionary_of_vocab_clusters = open_vocab_dict(\"dictionary_of_vocab_\" + str(n) + \"cluster.pickle\")\n",
    "    current_groupings = dictionary_of_vocab_clusters[\"token_groupings\"]\n",
    "    number_of_groups = len(dictionary_of_vocab_clusters[\"token_groupings\"])\n",
    "    feature_names = list(current_groupings.keys())\n",
    "\n",
    "    sage_input = calc_sage_input(dictionary_of_vocab_clusters, number_of_groups)\n",
    "    imputer = Imputer_groupings(model, number_of_groups)\n",
    "    groupings_estimator = sage.PermutationEstimator(imputer, \"mse\")\n",
    "\n",
    "    # sage for stance\n",
    "    label=\"Stance\"\n",
    "    sage_values_groupings = groupings_estimator(sage_input, np.array(df[label].values), batch_size=64, verbose=True,thresh=0.15) \n",
    "    stance_plot = sage_values_groupings.plot(feature_names, return_fig=True) \n",
    "    # save plot\n",
    "    stance_plot.savefig(os.path.join(notebookDir, \"../logs/StancePrediction_SemEval/plots/\", label + \"_feature_importance_\" + str(n) + \"cluster.png\"), format=\"png\")\n",
    "\n",
    "    # sage for target\n",
    "    label=\"Target\"\n",
    "    sage_values_groupings = groupings_estimator(sage_input, np.array(df[label].values), batch_size=64, verbose=True,thresh=0.15) \n",
    "    target_plot = sage_values_groupings.plot(feature_names, return_fig=True) \n",
    "    # save plot\n",
    "    target_plot.savefig(os.path.join(notebookDir, \"../logs/StancePrediction_SemEval/plots/\", label + \"_feature_importance_\" + str(n) + \"cluster.png\"), format=\"png\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4374d16277cd59720eda5e9a892d33ee7e53ac8b7c0031fbe42f60839aa8916a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('test': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}