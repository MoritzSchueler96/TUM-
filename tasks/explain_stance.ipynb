{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlp_utils.data_module import SemEvalDataModule\n",
    "from nlp_utils.model import CustomDistilBertModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from glob import glob\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebookDir: /home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/tasks\n"
     ]
    }
   ],
   "source": [
    "if not 'notebookDir' in globals():\n",
    "    notebookDir = os.getcwd()\n",
    "print('notebookDir: ' + notebookDir)\n",
    "os.chdir(notebookDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get folder where logs are stored\n",
    "save_folder = \"../logs/StancePrediction_SemEval/lightning_logs/\"\n",
    "save_folder = os.path.join(notebookDir, save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120a8bb41f2647a1aad82b8d9272d235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select a checkpoint:', options=('/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NL…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select a model\n",
    "w = widgets.Dropdown(\n",
    "    options=glob(os.path.join(save_folder, '*/checkpoints/*.ckpt')),\n",
    "    description='Select a checkpoint:'\n",
    ")\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dataset_path': '../../data/raw/SemEval/',\n",
       "  'learning_rate': 0.0030806995333433384,\n",
       "  'batch_size': 16,\n",
       "  'epochs': 20,\n",
       "  'num_trials': 50,\n",
       "  'vocab_size': 30522,\n",
       "  'target_encoding': {0: 'Atheism',\n",
       "   1: 'Climate Change is a Real Concern',\n",
       "   2: 'Feminist Movement',\n",
       "   3: 'Hillary Clinton',\n",
       "   4: 'Legalization of Abortion'},\n",
       "  'stance_encoding': {0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'}},\n",
       " 'version_26')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = re.findall(\"version_[0-9]+\", w.value)[0]\n",
    "model = CustomDistilBertModel.load_from_checkpoint(w.value)\n",
    "data_module = SemEvalDataModule(num_workers=4, config=model.config)\n",
    "\n",
    "model.config, model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████████████████████████████▌| 78/79 [00:33<00:00,  2.80it/s]\n",
      "\n",
      "============\n",
      "Results\t\t\t\t \n",
      "============\n",
      "FAVOR     precision: 0.3222 recall: 0.9211 f-score: 0.4774\n",
      "AGAINST   precision: 0.9286 recall: 0.2727 f-score: 0.4216\n",
      "------------\n",
      "Macro F: 0.4495\n",
      "\n",
      "Testing: 100%|██████████████████████████████████| 79/79 [00:33<00:00,  2.37it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_epoch_F1': 0.656525194644928,\n",
      " 'test_epoch_target_F1': 0.656525194644928,\n",
      " 'test_loss': 0.9024659395217896}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.9024659395217896,\n",
       "  'test_epoch_target_F1': 0.656525194644928,\n",
       "  'test_epoch_F1': 0.656525194644928}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check performance\n",
    "trainer = pl.Trainer(deterministic=True)\n",
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "source": [
    "## Auto select best model from Raytune hyperparameter optimization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best config\n",
    "def get_best_config(path):\n",
    "    scores = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if \".ckpt\" in name:\n",
    "                score = name.split(\"val\")\n",
    "                score = [re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)[-1] for s in score]\n",
    "                score.append(os.path.join(root, name))\n",
    "\n",
    "                scores.append(score)\n",
    "\n",
    "    # filter scores for best version\n",
    "    df = pd.DataFrame(scores, columns=[\"epoch\", \"loss\", \"F1\", \"path\"])\n",
    "    ckpt = df.sort_values(by=[\"F1\", \"loss\"], ascending=[False,True]).head(1).path.values[0]\n",
    "    return ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dataset_path': '../../data/raw/SemEval/',\n",
       "  'learning_rate': 0.0013774978663536918,\n",
       "  'batch_size': 32,\n",
       "  'epochs': 20,\n",
       "  'num_trials': 50,\n",
       "  'vocab_size': 30522,\n",
       "  'target_encoding': {0: 'Atheism',\n",
       "   1: 'Climate Change is a Real Concern',\n",
       "   2: 'Feminist Movement',\n",
       "   3: 'Hillary Clinton',\n",
       "   4: 'Legalization of Abortion'},\n",
       "  'stance_encoding': {0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'}},\n",
       " 'version_28')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take best model\n",
    "best_model_path = get_best_config(save_folder)\n",
    "best_model_version = re.findall(\"version_[0-9]+\", best_model_path)[0]\n",
    "\n",
    "best_model = CustomDistilBertModel.load_from_checkpoint(best_model_path)\n",
    "best_data_module = SemEvalDataModule(num_workers=4, config=model.config)\n",
    "\n",
    "best_model.config, best_model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████████████████████████████▌| 78/79 [00:32<00:00,  2.84it/s]\n",
      "\n",
      "============\n",
      "Results\t\t\t\t \n",
      "============\n",
      "FAVOR     precision: 0.6468 recall: 0.5000 f-score: 0.5640\n",
      "AGAINST   precision: 0.7426 recall: 0.7385 f-score: 0.7405\n",
      "------------\n",
      "Macro F: 0.6523\n",
      "\n",
      "Testing: 100%|██████████████████████████████████| 79/79 [00:32<00:00,  2.40it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_epoch_F1': 0.7502001523971558,\n",
      " 'test_epoch_target_F1': 0.7502001523971558,\n",
      " 'test_loss': 0.7219581007957458}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7219581007957458,\n",
       "  'test_epoch_target_F1': 0.7502001523971558,\n",
       "  'test_epoch_F1': 0.7502001523971558}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check performance\n",
    "trainer = pl.Trainer(deterministic=True)\n",
    "trainer.test(best_model, datamodule=best_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import spacy\n",
    "import pickle\n",
    "import json\n",
    "import sage\n",
    "import nltk\n",
    "import string\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from transformers import DistilBertTokenizer\n",
    "from numpy.random import default_rng\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'},\n",
       " {0: 'Atheism',\n",
       "  1: 'Climate Change is a Real Concern',\n",
       "  2: 'Feminist Movement',\n",
       "  3: 'Hillary Clinton',\n",
       "  4: 'Legalization of Abortion'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display encoding\n",
    "data_module.stance_encoding, data_module.target_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_module, part=\"all\"):\n",
    "    # train\n",
    "    text, label = data_module.trainset.texts, data_module.trainset.labels\n",
    "    df_train = pd.DataFrame(data=(text), columns=[\"Text\"])\n",
    "    df_train[\"Stance\"] = label[0]\n",
    "    df_train[\"Target\"] = label[1]\n",
    "\n",
    "    # val\n",
    "    text, label = data_module.valset.texts, data_module.valset.labels\n",
    "    df_val = pd.DataFrame(data=(text), columns=[\"Text\"])\n",
    "    df_val[\"Stance\"] = label[0]\n",
    "    df_val[\"Target\"] = label[1]\n",
    "    #test\n",
    "    text, label = data_module.testset.texts, data_module.testset.labels\n",
    "    df_test = pd.DataFrame(data=(text), columns=[\"Text\"])\n",
    "    df_test[\"Stance\"] = label[0]\n",
    "    df_test[\"Target\"] = label[1]\n",
    "\n",
    "    if part == \"all\":\n",
    "        df = pd.concat([df_train, df_val, df_test])\n",
    "        return df\n",
    "    elif part == \"train\":\n",
    "        return df_train\n",
    "    elif part == \"val\":\n",
    "        return df_val\n",
    "    else:\n",
    "        return df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset(data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't get it twisted. A major presidential can...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Dukes of Hazzard has been on tv for 36 yea...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#BlackLivesMatter unless they are pre born bla...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Of mothers advising their daughter's to abort ...</td>\n",
       "      <td>AGAINST</td>\n",
       "      <td>Legalization of Abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you want to empower women, you need to dise...</td>\n",
       "      <td>FAVOR</td>\n",
       "      <td>Feminist Movement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text   Stance  \\\n",
       "0  Don't get it twisted. A major presidential can...  AGAINST   \n",
       "1  The Dukes of Hazzard has been on tv for 36 yea...  AGAINST   \n",
       "2  #BlackLivesMatter unless they are pre born bla...  AGAINST   \n",
       "3  Of mothers advising their daughter's to abort ...  AGAINST   \n",
       "4  If you want to empower women, you need to dise...    FAVOR   \n",
       "\n",
       "                     Target  \n",
       "0           Hillary Clinton  \n",
       "1           Hillary Clinton  \n",
       "2  Legalization of Abortion  \n",
       "3  Legalization of Abortion  \n",
       "4         Feminist Movement  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display data for inspection\n",
    "df1 = df.copy()\n",
    "df1[\"Stance\"] = df1[\"Stance\"].transform(lambda x: data_module.stance_encoding[x])\n",
    "df1[\"Target\"] = df1[\"Target\"].transform(lambda x: data_module.target_encoding[x])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_clusters(df, similarity_func, num_clusters):\n",
    "    X = df[\"Text\"]\n",
    "    print(\"Create Vocabulary...\")\n",
    "    stripped_of_syllables_vocab, stripped = create_vocab(X)\n",
    "    print(\"Cluster Vocabulary...\")\n",
    "    dictionary_of_vocab = cluster_vocab(X, stripped_of_syllables_vocab, stripped, similarity_func, num_clusters)\n",
    "    print(\"Finished...\")\n",
    "    \n",
    "    return dictionary_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(X):\n",
    "    nlp_bigger = spacy.load('en_core_web_lg')\n",
    "    list_of_list = [tokenizer.tokenize(x) for x in X.values]\n",
    "    flat_list = [item for sublist in list_of_list for item in sublist]\n",
    "    small_vocab = list(set(flat_list))\n",
    "    spacy_vocab = [nlp_bigger(x) for x in small_vocab] \n",
    "    vocab_2d = [[x] for x in spacy_vocab]\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_vocab = [w for w in spacy_vocab if not str(w) in stop_words]\n",
    "    filtered_stopwords = [str(w) for w in spacy_vocab if str(w) in stop_words]\n",
    "\n",
    "    #removing punctuation from vocab\n",
    "    #print(string.punctuation)\n",
    "    stripped_vocab = [w for w in filtered_vocab if not str(w) in string.punctuation] \n",
    "    stripped_punctuation = [str(w) for w in filtered_vocab if str(w) in string.punctuation]\n",
    "\n",
    "    #removing digits from vocab\n",
    "    stripped_of_digits_vocab = [w for w in stripped_vocab if not str(w).isdigit()]\n",
    "    stripped_digits = [str(w) for w in stripped_vocab if str(w).isdigit()]\n",
    "\n",
    "    #removing syllables from vocab\n",
    "    stripped_of_syllables_vocab = [w for w in stripped_of_digits_vocab if not \"#\" in str(w)] \n",
    "    stripped_syllables = [str(w) for w in stripped_of_digits_vocab if \"#\" in str(w)]\n",
    "    \n",
    "    stripped = (filtered_stopwords, stripped_punctuation, stripped_digits, stripped_syllables)\n",
    "\n",
    "    return stripped_of_syllables_vocab, stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_func(u, v):\n",
    "    if u[0].text == \"#\":\n",
    "        token_one = u[len(u)-1]\n",
    "    else:\n",
    "        token_one = u[0]\n",
    "    if v[0].text == \"#\":\n",
    "        token_two = v[len(v)-1]\n",
    "    else:\n",
    "        token_two = v[0]\n",
    "    return token_one.similarity(token_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_vocab(X, stripped_of_syllables_vocab, stripped, similarity_func, num_clusters):\n",
    "    stripped_of_syllables_vocab_2d = [[x] for x in stripped_of_syllables_vocab]\n",
    "    dists = pdist(np.array(stripped_of_syllables_vocab_2d), similarity_func) \n",
    "    similarity_matrix = squareform(dists)\n",
    "    cluster_model = AgglomerativeClustering(affinity='precomputed', n_clusters=num_clusters, linkage='complete').fit(1-similarity_matrix)\n",
    "\n",
    "    clustered_vocab = [[] for x in range(num_clusters)]\n",
    "    _ = [clustered_vocab[cluster_model.labels_[index]].append(str(x)) for index, x in enumerate(stripped_of_syllables_vocab)]\n",
    "\n",
    "    token_groupings = {str(index) : element for index, element in enumerate(clustered_vocab)} \n",
    "\n",
    "    token_groupings[\"stop_words\"] = stripped[0]\n",
    "    token_groupings[\"punctuation\"] = stripped[1]\n",
    "    token_groupings[\"digits\"] = stripped[2]\n",
    "    token_groupings[\"syllables\"] = stripped[3]\n",
    "\n",
    "    dictionary_of_vocab = {\n",
    "    \"description\": \"\",\n",
    "    \"text\": (X.values),\n",
    "    \"num_clusters\": num_clusters,\n",
    "    \"similarity_matrix\": similarity_matrix,\n",
    "    \"cluster_model\": cluster_model,\n",
    "    \"clustered_vocab\": clustered_vocab,\n",
    "    \"token_groupings\": token_groupings\n",
    "    }\n",
    "\n",
    "    return dictionary_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_dict(dictionary_of_vocab, filename):\n",
    "    file = open(os.path.join(notebookDir, \"vocab/\" + filename), 'wb')\n",
    "    pickle.dump(dictionary_of_vocab, file)\n",
    "    file.close()\n",
    "\n",
    "def open_vocab_dict(filename):\n",
    "    dictionary_of_vocab = pickle.load(open(os.path.join(notebookDir, \"../logs/StancePrediction_SemEval/vocab/\" + filename), 'rb'))\n",
    "    return dictionary_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up imputer object\n",
    "class Imputer_groupings:\n",
    "    def __init__(self, model, number_of_groups):\n",
    "        self.model = model\n",
    "        self.num_groups = number_of_groups\n",
    "    \n",
    "    def __call__(self, input_array, S):\n",
    "        max_length = 0\n",
    "        reconstructed_array = []\n",
    "        for index, sentence in enumerate(input_array):\n",
    "            length_of_sentence = max(x[0] for group in sentence for x in group)\n",
    "            if length_of_sentence > max_length:\n",
    "                max_length = length_of_sentence\n",
    "\n",
    "            original_input_ids = [None] * (length_of_sentence+1)\n",
    "            for sub_index, group in enumerate(sentence):\n",
    "                \n",
    "                if S[index][sub_index]: #put in '[MASK]' elements if needed\n",
    "                    for x in group:\n",
    "                        original_input_ids[x[0]]=x[1]\n",
    "                else: \n",
    "                    for x in group:\n",
    "                        original_input_ids[x[0]]=103 #id of '[MASK]'\n",
    "\n",
    "            original_input_ids.append(102)\n",
    "            original_input_ids.insert(0, 101)\n",
    "            reconstructed_array.append(original_input_ids)\n",
    "\n",
    "        max_length+=3\n",
    "\n",
    "        for index, sentence in enumerate(reconstructed_array):\n",
    "            if len(sentence) < max_length:\n",
    "                reconstructed_array[index].extend([0 for i in range(max_length-len(sentence))])\n",
    "\n",
    "        input_ids = np.array(reconstructed_array)\n",
    "\n",
    "        # \"handmade\" attention mask -> basically just set everything to one, except '[PAD]'s which are zero\n",
    "        am = np.ones(input_ids.shape)\n",
    "        am[input_ids == 0] = 0 #id of '[PAD]'\n",
    "        tensor_attention_mask = torch.tensor(am)\n",
    "        tensor_attention_mask = tensor_attention_mask.to(model.device)\n",
    "\n",
    "\n",
    "        tensor_input_ids = torch.tensor(input_ids)\n",
    "        tensor_input_ids = tensor_input_ids.to(model.device)\n",
    "\n",
    "        encoded_text = {\"input_ids\": tensor_input_ids, \"attention_mask\": tensor_attention_mask}\n",
    "        \n",
    "        #predict with model\n",
    "        outputs = self.model(encoded_text)[0]       \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        \n",
    "        score_most_prob = [max(x) for x in outputs]\n",
    "\n",
    "        return np.array(score_most_prob) # sp.special.logit(score_most_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_sage_input(dictionary_of_vocab_clusters, number_of_groups):\n",
    "    current_groupings = dictionary_of_vocab_clusters[\"token_groupings\"]\n",
    "    feature_names = list(current_groupings.keys())\n",
    "    sage_input_all_instances = []\n",
    "    for text_element in dictionary_of_vocab_clusters[\"text\"]:\n",
    "        sage_input_groupings = [[] for x in range(number_of_groups)]\n",
    "        input_text = tokenizer.tokenize(text_element)\n",
    "        input_ids = tokenizer(text_element, add_special_tokens=False)[\"input_ids\"]\n",
    "        groups_to_text = [next((key for key, value in current_groupings.items() if x in value)) for x in input_text]\n",
    "\n",
    "        positioned_ids = [(i, x) for i, x in enumerate(input_ids)]\n",
    "        _ = [sage_input_groupings[list(current_groupings.keys()).index(groups_to_text[index])].append(x)  for index, x in enumerate(positioned_ids)]\n",
    "        sage_input_all_instances.append(sage_input_groupings)\n",
    "\n",
    "    sage_input_all_instances_array = np.array(sage_input_all_instances, dtype=object)\n",
    "    return sage_input_all_instances_array"
   ]
  },
  {
   "source": [
    "### Automatically run a number of cluster sizes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n",
      "Create Vocabulary...\n",
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Cluster Vocabulary...\n",
      "Finished...\n"
     ]
    }
   ],
   "source": [
    "# create and save clustered vocabularies for a number of different cluster sizes and save them to the logs\n",
    "cluster_sizes = [3, 5, 7, 9, 11, 15, 17, 19, 21, 25, 29, 35, 45, 55]\n",
    "for n in cluster_sizes:\n",
    "    dictionary_of_vocab_clusters = create_word_clusters(df, similarity_func, n)\n",
    "    save_vocab_dict(dictionary_of_vocab_clusters, \"dictionary_of_vocab_\" + str(n) + \"cluster.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 0.0875/1 [4:09:18<43:20:00, 170958.92s/it]\n",
      " 28%|██▊       | 0.2755/1 [00:34<01:30, 124.40s/it]StdDev Ratio = 0.2858 (Converge at 0.1500)\n",
      " 36%|███▋      | 0.3626/1 [01:11<02:05, 196.42s/it]StdDev Ratio = 0.2491 (Converge at 0.1500)\n",
      " 35%|███▌      | 0.35/1 [01:46<03:17, 303.45s/it]  StdDev Ratio = 0.2535 (Converge at 0.1500)\n",
      " 33%|███▎      | 0.3262/1 [02:26<05:03, 450.41s/it]StdDev Ratio = 0.2626 (Converge at 0.1500)\n",
      " 51%|█████     | 0.5065/1 [03:27<03:22, 410.33s/it]StdDev Ratio = 0.2108 (Converge at 0.1500)\n",
      " 46%|████▌     | 0.4551/1 [04:32<05:26, 599.40s/it]StdDev Ratio = 0.2224 (Converge at 0.1500)\n",
      " 56%|█████▋    | 0.5647/1 [05:18<04:05, 563.96s/it]StdDev Ratio = 0.1996 (Converge at 0.1500)\n",
      " 72%|███████▏  | 0.7195/1 [06:23<02:29, 533.49s/it]StdDev Ratio = 0.1768 (Converge at 0.1500)\n",
      " 96%|█████████▌| 0.9589/1 [07:35<00:19, 474.56s/it]StdDev Ratio = 0.1532 (Converge at 0.1500)\n",
      "100%|██████████| 1/1 [08:33<00:00, 513.69s/it]StdDev Ratio = 0.1469 (Converge at 0.1500)\n",
      "Detected convergence\n",
      "\n",
      " 15%|█▌        | 0.1537/1 [01:02<05:42, 405.27s/it]StdDev Ratio = 0.3826 (Converge at 0.1500)\n",
      " 21%|██        | 0.2115/1 [02:01<07:32, 574.25s/it]StdDev Ratio = 0.3261 (Converge at 0.1500)\n",
      " 40%|███▉      | 0.3992/1 [03:00<04:31, 451.37s/it]StdDev Ratio = 0.2374 (Converge at 0.1500)\n",
      " 55%|█████▌    | 0.5516/1 [03:56<03:12, 429.15s/it]StdDev Ratio = 0.2020 (Converge at 0.1500)\n",
      " 71%|███████   | 0.7104/1 [04:59<02:02, 421.33s/it]StdDev Ratio = 0.1780 (Converge at 0.1500)\n",
      "100%|██████████| 1/1 [05:56<00:00, 356.60s/it]StdDev Ratio = 0.1455 (Converge at 0.1500)\n",
      "Detected convergence\n",
      "\n",
      " 30%|███       | 0.3048/1 [01:02<02:21, 204.21s/it]StdDev Ratio = 0.2717 (Converge at 0.1500)\n",
      " 65%|██████▍   | 0.6463/1 [02:04<01:08, 193.03s/it]StdDev Ratio = 0.1866 (Converge at 0.1500)\n",
      "100%|██████████| 1/1 [03:14<00:00, 194.83s/it]StdDev Ratio = 0.1433 (Converge at 0.1500)\n",
      "Detected convergence\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18 is out of bounds for axis 0 with size 17",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_428234/2113424791.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Stance\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msage_values_groupings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroupings_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msage_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mstance_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msage_values_groupings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_fig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# save plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstance_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebookDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"plots/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_feature_importance_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cluster.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp2/lib/python3.7/site-packages/sage/core.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, feature_names, sort_features, max_features, orientation, error_bars, confidence_level, capsize, color, title, title_size, tick_size, tick_rotation, label_size, figsize, return_fig)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0merror_bars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             tick_size, tick_rotation, label_size, figsize, return_fig)\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     def comparison(self,\n",
      "\u001b[0;32m~/miniconda3/envs/nlp2/lib/python3.7/site-packages/sage/plotting.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(explanation, feature_names, sort_features, max_features, orientation, error_bars, confidence_level, capsize, color, title, title_size, tick_size, tick_rotation, label_size, figsize, return_fig)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Remove extra features if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18 is out of bounds for axis 0 with size 17"
     ]
    }
   ],
   "source": [
    "# create sage plots for the different cluster sizes and save them to the logs\n",
    "for n in cluster_sizes:\n",
    "    dictionary_of_vocab_clusters = open_vocab_dict(\"dictionary_of_vocab_\" + str(n) + \"cluster.pickle\")\n",
    "    number_of_groups = len(dictionary_of_vocab_clusters[\"token_groupings\"])\n",
    "    sage_input = calc_sage_input(dictionary_of_vocab_clusters, number_of_groups)\n",
    "    imputer = Imputer_groupings(model, number_of_groups)\n",
    "    groupings_estimator = sage.PermutationEstimator(imputer, \"mse\")\n",
    "\n",
    "    # sage for stance\n",
    "    label=\"Stance\"\n",
    "    sage_values_groupings = groupings_estimator(sage_input, np.array(df[label].values), batch_size=64, verbose=True,thresh=0.15) \n",
    "    stance_plot = sage_values_groupings.plot(feature_names, return_fig=True) \n",
    "    # save plot\n",
    "    stance_plot.savefig(os.path.join(notebookDir, \"plots/\", label + \"_feature_importance_\" + str(n) + \"cluster.png\"), format=\"png\")\n",
    "\n",
    "    # sage for target\n",
    "    label=\"Target\"\n",
    "    sage_values_groupings = groupings_estimator(sage_input, np.array(df[label].values), batch_size=64, verbose=True,thresh=0.15) \n",
    "    target_plot = sage_values_groupings.plot(feature_names, return_fig=True) \n",
    "    # save plot\n",
    "    target_plot.savefig(os.path.join(notebookDir, \"plots/\", label + \"_feature_importance_\" + str(n) + \"cluster.png\"), format=\"png\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "268c4b8a86823f78ba4be7bd11a772eb39cae60b6512913bc2e241ba9f507db1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp2': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}