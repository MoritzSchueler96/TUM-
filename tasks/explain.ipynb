{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('nlp2': conda)"
  },
  "interpreter": {
   "hash": "268c4b8a86823f78ba4be7bd11a772eb39cae60b6512913bc2e241ba9f507db1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from nlp_utils.data_module import SemEvalDataModule\n",
    "from nlp_utils.model import CustomDistilBertModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from glob import glob\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sb\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "notebookDir: /home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/tasks\n"
     ]
    }
   ],
   "source": [
    "if not 'notebookDir' in globals():\n",
    "    notebookDir = os.getcwd()\n",
    "print('notebookDir: ' + notebookDir)\n",
    "os.chdir(notebookDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Start tensorboard\n",
    "! pkill tensorboard\n",
    "! rm -r /tmp/.tensorboard-info\n",
    "%tensorboard --logdir ../logs/lightning_logs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get folder where logs are stored\n",
    "save_folder = \"../logs/StancePrediction_SemEval/lightning_logs/\"\n",
    "save_folder = os.path.join(notebookDir, save_folder)"
   ]
  },
  {
   "source": [
    "# Load model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Dropdown(description='Select a checkpoint:', options=('/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NL…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f99eb37cdb6044c2845d3a81a67df91c"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Select a model\n",
    "w = widgets.Dropdown(\n",
    "    options=glob(os.path.join(save_folder, '*/checkpoints/*.ckpt')),\n",
    "    description='Select a checkpoint:'\n",
    ")\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'dataset_path': '../../data/raw/SemEval/',\n",
       "  'learning_rate': 0.0010739076860714453,\n",
       "  'batch_size': 32,\n",
       "  'epochs': 20,\n",
       "  'num_trials': 50,\n",
       "  'vocab_size': 30522,\n",
       "  'target_encoding': {0: 'Atheism',\n",
       "   1: 'Climate Change is a Real Concern',\n",
       "   2: 'Feminist Movement',\n",
       "   3: 'Hillary Clinton',\n",
       "   4: 'Legalization of Abortion'},\n",
       "  'stance_encoding': {0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'}},\n",
       " 'version_22')"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "model_version = re.findall(\"version_[0-9]+\", w.value)[0]\n",
    "model = CustomDistilBertModel.load_from_checkpoint(w.value)\n",
    "data_module = SemEvalDataModule(num_workers=4, config=model.config)\n",
    "\n",
    "model.config, model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Testing:  98%|█████████▊| 39/40 [00:36<00:00,  1.24it/s]\n",
      "\n",
      "============\n",
      "Results\t\t\t\t \n",
      "============\n",
      "FAVOR     precision: 0.5906 recall: 0.4934 f-score: 0.5376\n",
      "AGAINST   precision: 0.7110 recall: 0.7776 f-score: 0.7428\n",
      "------------\n",
      "Macro F: 0.6402\n",
      "\n",
      "Testing: 100%|██████████| 40/40 [00:36<00:00,  1.10it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_epoch_F1': 0.7349879741668701,\n",
      " 'test_epoch_target_F1': 0.7349879741668701,\n",
      " 'test_loss': 0.7136555314064026}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7136555314064026,\n",
       "  'test_epoch_target_F1': 0.7349879741668701,\n",
       "  'test_epoch_F1': 0.7349879741668701}]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# check performance\n",
    "trainer = pl.Trainer(deterministic=True)\n",
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best config\n",
    "def get_best_config(path):\n",
    "    scores = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if \".ckpt\" in name:\n",
    "                score = name.split(\"val\")\n",
    "                score = [re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)[-1] for s in score]\n",
    "                score.append(os.path.join(root, name))\n",
    "\n",
    "                scores.append(score)\n",
    "\n",
    "    # filter scores for best version\n",
    "    df = pd.DataFrame(scores, columns=[\"epoch\", \"loss\", \"F1\", \"path\"])\n",
    "    ckpt = df.sort_values(by=[\"F1\", \"loss\"], ascending=[False,True]).head(1).path.values[0]\n",
    "    return ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'dataset_path': '../../data/raw/SemEval/',\n",
       "  'learning_rate': 0.0013774978663536918,\n",
       "  'batch_size': 32,\n",
       "  'epochs': 20,\n",
       "  'num_trials': 50,\n",
       "  'vocab_size': 30522,\n",
       "  'target_encoding': {0: 'Atheism',\n",
       "   1: 'Climate Change is a Real Concern',\n",
       "   2: 'Feminist Movement',\n",
       "   3: 'Hillary Clinton',\n",
       "   4: 'Legalization of Abortion'},\n",
       "  'stance_encoding': {0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'}},\n",
       " 'version_22')"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "# take best model\n",
    "best_model_path = get_best_config(save_folder)\n",
    "best_model = CustomDistilBertModel.load_from_checkpoint(best_model_path)\n",
    "best_data_module = SemEvalDataModule(num_workers=4, config=model.config)\n",
    "\n",
    "best_model.config, best_model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Testing:  98%|█████████▊| 39/40 [00:35<00:00,  1.21it/s]\n",
      "\n",
      "============\n",
      "Results\t\t\t\t \n",
      "============\n",
      "FAVOR     precision: 0.6468 recall: 0.5000 f-score: 0.5640\n",
      "AGAINST   precision: 0.7426 recall: 0.7385 f-score: 0.7405\n",
      "------------\n",
      "Macro F: 0.6523\n",
      "\n",
      "Testing: 100%|██████████| 40/40 [00:35<00:00,  1.12it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_epoch_F1': 0.7502001523971558,\n",
      " 'test_epoch_target_F1': 0.7502001523971558,\n",
      " 'test_loss': 0.7219581007957458}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_loss': 0.7219581007957458,\n",
       "  'test_epoch_target_F1': 0.7502001523971558,\n",
       "  'test_epoch_F1': 0.7502001523971558}]"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "# check performance\n",
    "trainer = pl.Trainer(deterministic=True)\n",
    "trainer.test(best_model, datamodule=best_data_module)"
   ]
  },
  {
   "source": [
    "### Decide for model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model\n",
    "model = best_model"
   ]
  },
  {
   "source": [
    "# Explain model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Sage"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import spacy\n",
    "import pickle\n",
    "import json\n",
    "import sage\n",
    "import nltk\n",
    "import string\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from transformers import DistilBertTokenizer\n",
    "from numpy.random import default_rng\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"git+https://github.com/iancovert/sage.git\"  &> /dev/null\n",
    "!python -m spacy download en &> /dev/null\n",
    "!python -m spacy download en_core_web_lg &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({0: 'AGAINST', 1: 'FAVOR', 2: 'NONE', 3: 'UNKNOWN'},\n",
       " {0: 'Atheism',\n",
       "  1: 'Climate Change is a Real Concern',\n",
       "  2: 'Feminist Movement',\n",
       "  3: 'Hillary Clinton',\n",
       "  4: 'Legalization of Abortion'})"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "data_module.stance_encoding, data_module.target_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text  Stance  Target\n",
       "0  Don't get it twisted. A major presidential can...       0       3\n",
       "1  The Dukes of Hazzard has been on tv for 36 yea...       0       3\n",
       "2  #BlackLivesMatter unless they are pre born bla...       0       4\n",
       "3  Of mothers advising their daughter's to abort ...       0       4\n",
       "4  If you want to empower women, you need to dise...       1       2"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Stance</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Don't get it twisted. A major presidential can...</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The Dukes of Hazzard has been on tv for 36 yea...</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#BlackLivesMatter unless they are pre born bla...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Of mothers advising their daughter's to abort ...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>If you want to empower women, you need to dise...</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "text, label = data_module.trainset.texts, data_module.trainset.labels\n",
    "df = pd.DataFrame(data=(text), columns=[\"Text\"])\n",
    "df[\"Stance\"] = label[0]\n",
    "df[\"Target\"] = label[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text   Stance  \\\n",
       "0  Don't get it twisted. A major presidential can...  AGAINST   \n",
       "1  The Dukes of Hazzard has been on tv for 36 yea...  AGAINST   \n",
       "2  #BlackLivesMatter unless they are pre born bla...  AGAINST   \n",
       "3  Of mothers advising their daughter's to abort ...  AGAINST   \n",
       "4  If you want to empower women, you need to dise...    FAVOR   \n",
       "\n",
       "                     Target  \n",
       "0           Hillary Clinton  \n",
       "1           Hillary Clinton  \n",
       "2  Legalization of Abortion  \n",
       "3  Legalization of Abortion  \n",
       "4         Feminist Movement  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Stance</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Don't get it twisted. A major presidential can...</td>\n      <td>AGAINST</td>\n      <td>Hillary Clinton</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The Dukes of Hazzard has been on tv for 36 yea...</td>\n      <td>AGAINST</td>\n      <td>Hillary Clinton</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#BlackLivesMatter unless they are pre born bla...</td>\n      <td>AGAINST</td>\n      <td>Legalization of Abortion</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Of mothers advising their daughter's to abort ...</td>\n      <td>AGAINST</td>\n      <td>Legalization of Abortion</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>If you want to empower women, you need to dise...</td>\n      <td>FAVOR</td>\n      <td>Feminist Movement</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "df1 = df.copy()\n",
    "df1[\"Stance\"] = df1[\"Stance\"].transform(lambda x: data_module.stance_encoding[x])\n",
    "df1[\"Target\"] = df1[\"Target\"].transform(lambda x: data_module.target_encoding[x])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_bigger = spacy.load('en_core_web_lg')\n",
    "list_of_list = [tokenizer.tokenize(x) for x in df[\"Text\"].values]\n",
    "flat_list = [item for sublist in list_of_list for item in sublist]\n",
    "small_vocab = list(set(flat_list))\n",
    "spacy_vocab = [nlp_bigger(x) for x in small_vocab] \n",
    "vocab_2d = [[x] for x in spacy_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_vocab = [w for w in spacy_vocab if not str(w) in stop_words]\n",
    "filtered_stopwords = [str(w) for w in spacy_vocab if str(w) in stop_words]\n",
    "\n",
    "#removing punctuation from vocab\n",
    "print(string.punctuation)\n",
    "stripped_vocab = [w for w in filtered_vocab if not str(w) in string.punctuation] \n",
    "stripped_punctuation = [str(w) for w in filtered_vocab if str(w) in string.punctuation]\n",
    "\n",
    "#removing digits from vocab\n",
    "stripped_of_digits_vocab = [w for w in stripped_vocab if not str(w).isdigit()] \n",
    "stripped_digits = [str(w) for w in stripped_vocab if str(w).isdigit()]\n",
    "\n",
    "#removing syllables from vocab\n",
    "stripped_of_syllables_vocab = [w for w in stripped_of_digits_vocab if not \"#\" in str(w)] \n",
    "stripped_syllables = [str(w) for w in stripped_of_digits_vocab if \"#\" in str(w)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5201\n"
     ]
    }
   ],
   "source": [
    "stripped_of_syllables_vocab_2d = [[x] for x in stripped_of_syllables_vocab]\n",
    "print(len(stripped_of_syllables_vocab_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_func(u, v):\n",
    "\n",
    "    if u[0].text == \"#\":\n",
    "        token_one = u[len(u)-1]\n",
    "    else:\n",
    "        token_one = u[0]\n",
    "    if v[0].text == \"#\":\n",
    "        token_two = v[len(v)-1]\n",
    "    else:\n",
    "        token_two = v[0]\n",
    "    return token_one.similarity(token_two)"
   ]
  },
  {
   "source": [
    "dists = pdist(np.array(stripped_of_syllables_vocab_2d), similarity_func) \n",
    "similarity_matrix = squareform(dists)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 151,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/user/miniconda3/envs/nlp2/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLUSTERS = 13\n",
    "#important to plug in the \"1-similarity_matrix\" !!!!\n",
    "cluster_model = AgglomerativeClustering(affinity='precomputed', n_clusters=NUMBER_OF_CLUSTERS, linkage='complete').fit(1-similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_vocab = [[] for x in range(NUMBER_OF_CLUSTERS)]\n",
    "_ = [clustered_vocab[cluster_model.labels_[index]].append(str(x)) for index, x in enumerate(stripped_of_syllables_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_groupings = {str(index) : element for index, element in enumerate(clustered_vocab)} \n",
    "\n",
    "token_groupings[\"stop_words\"] = filtered_stopwords\n",
    "token_groupings[\"punctuation\"] = stripped_punctuation\n",
    "token_groupings[\"digits\"] = stripped_digits\n",
    "token_groupings[\"syllables\"] = stripped_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_of_vocab = {\n",
    "    \"description\": \"\",\n",
    "    \"text\": (df[\"Text\"].values,df[\"Stance\"].values),\n",
    "    \"NUMBER_OF_CLUSTERS\": NUMBER_OF_CLUSTERS,\n",
    "    \"similarity_matrix\": similarity_matrix,\n",
    "    \"cluster_model\": cluster_model,\n",
    "    \"clustered_vocab\": clustered_vocab,\n",
    "    \"token_groupings\": token_groupings\n",
    "}\n",
    "\n",
    "# !!!!!!!give a new name always to never overwrite existing data!!!!!!!!!\n",
    "unique_named_file = 'dictionary_of_vocab_' + '13cluster_cleaned_test_samples32' + '.pickle'\n",
    "\n",
    "file = open(os.path.join(notebookDir, \"vocab/\" + unique_named_file), 'wb')\n",
    "pickle.dump(dictionary_of_vocab, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is', 'we', 'other', 'own', 'weren', 'if', 'over', 'up', 'in', 'these', 'same', 'do', 'here', 'few', 'hers', 'don', 'ourselves', 'shouldn', 'then', 'wasn', 'just', 'because', 'myself', 'll', 'yours', 'was', 'be', 'wouldn', 'has', 'that', 'nor', 'can', 'so', 'with', 'now', 'yourself', 'her', 'your', 'any', 'my', 'for', 'such', 'himself', 'their', 'and', 'where', 'while', 'you', 'd', 'ma', 'until', 'me', 'above', 'o', 'why', 'herself', 'been', 't', 'a', 'doesn', 'hasn', 'more', 'm', 'about', 'who', 'isn', 'very'], 'punctuation': ['%', '$', '~', '!', '@', '.', '=', \"'\", '/', '\"', '^', '*', ')', '>', '#', '+', ';', '?', ']', ':', '-', '_', '[', '&', '<', '(', ','], 'digits': ['322', '82', '92', '28', '35', '9', '2010', '68', '42', '26', '143', '205', '2', '300', '0', '4', '130', '45', '58', '25', '37', '1791', '52', '86', '67', '89', '3000', '324', '47', '99', '51', '160', '41', '500', '6', '40', '19', '12', '91', '1500', '140', '44', '61', '14', '16', '32', '297', '36', '57', '100', '30', '75', '10', '8', '390', '270', '193', '1937', '1', '38', '107', '475', '60', '27', '72', '49', '20', '000', '00', '2015', '230', '84', '400', '96', '24', '2014', '46', '5', '80', '2013', '700', '7', '23', '59', '15', '2016', '1989', '17', '3', '90', '730', '77', '29', '2001', '50', '55', '22', '33', '11', '18', '70', '900', '21', '121', '118', '74', '13', '66'], 'syllables': ['##ial', '##kell', '##cat', '##pipe', '##eral', '##gins', '##sche', '##izing', '##bber', '##8', '##aba', '##ort', '##ecure', '##sport', '##ary', '##ye', '##ppy', '##oman', '##mian', '##chin', '##tness', '##falls', '##thal', '##rod', '##eli', '##ssion', '##aga', '##fa', '##itt', '##ain', '##enburg', '##log', '##mine', '##say', '##ew', '##au', '##we', '##cus', '##bai', '##ices', '##ee', '##person', '##uche', '##w', '##bra', '##gree', '##bro', '##tem', '##ci', '##zza', '##ito', '##dance', '##harat', '##day', '##had', '##game', '##thes', '##36', '##see', '##her', '##il', '##wan', '##hers', '##orted', '##ific', '##lle', '##bes', '##dog', '##ation', '##eth', '##good', '##efe', '##ia', '##va', '##c', '##ano', '##rim', '##ned', '##gs', '##vers', '##65', '##ple', '##tok', '##nz', '##saurus', '##chang', '##gee', '##zer', '##ez', '##pin', '##tri', '##cal', '##ir', '##llen', '##ray', '##tain', '##uta', '##ha', '##bs', '##lter', '##hil', '##vi', '##bis', '##row', '##oga', '##stra', '##ok', '##gara', '##ify', '##pped', '##rk', '##baum', '##up', '##get', '##eal', '##gent', '##arian', '##vert', '##ier', '##hom', '##as', '##static', '##kee', '##pace', '##un', '##rage', '##gles', '##ty', '##tad', '##power', '##ms', '##nity', '##low', '##ru', '##ded', '##27', '##dict', '##elo', '##uth', '##ima', '##udeau', '##dle', '##sy', '##lat', '##cast', '##pc', '##no', '##21', '##tious', '##0', '##ell', '##3', '##bau', '##ten', '##pile', '##uro', '##jo', '##smith', '##iness', '##g', '##cards', '##zd', '##oat', '##um', '##tar', '##crats', '##iling', '##n', '##vid', '##tro', '##yu', '##ets', '##15', '##rom', '##kill', '##omy', '##hm', '##qu', '##men', '##icia', '##can', '##cies', '##uk', '##zzo', '##min', '##ake', '##lini', '##gu', '##zio', '##llary', '##lr', '##hy', '##off', '##istic', '##week', '##s', '##oa', '##gar', '##hun', '##ik', '##dm', '##li', '##sius', '##ft', '##yi', '##ob', '##bon', '##ailed', '##vil', '##dan', '##dy', '##tf', '##lene', '##ra', '##uc', '##go', '##lb', '##ast', '##mpt', '##guard', '##rard', '##igate', '##dos', '##sing', '##ht', '##ho', '##fat', '##coll', '##vr', '##sc', '##turn', '##wy', '##fl', '##nda', '##l', '##ren', '##lva', '##49', '##yn', '##rail', '##an', '##tch', '##k', '##10', '##imate', '##sch', '##igo', '##lot', '##gh', '##ou', '##nd', '##gus', '##yon', '##insky', '##ers', '##inton', '##cable', '##u', '##iciencies', '##rated', '##tes', '##ss', '##aly', '##ue', '##ink', '##sum', '##gy', '##rock', '##pment', '##rogen', '##9', '##aha', '##a', '##sas', '##vis', '##ler', '##rite', '##aris', '##cea', '##ages', '##isa', '##ery', '##yp', '##nad', '##fied', '##dad', '##di', '##lish', '##tting', '##ball', '##uts', '##nic', '##sis', '##ider', '##ani', '##eau', '##wg', '##illy', '##rou', '##ower', '##dl', '##fide', '##logies', '##bahn', '##iy', '##oz', '##acy', '##sw', '##guide', '##ree', '##mble', '##fur', '##fm', '##int', '##bia', '##53', '##vale', '##gui', '##building', '##act', '##ili', '##rgan', '##ism', '##mat', '##tc', '##and', '##tua', '##ula', '##urities', '##hh', '##lok', '##bush', '##lp', '##rne', '##oll', '##mind', '##pton', '##stor', '##yana', '##ital', '##eit', '##ance', '##tower', '##alia', '##oris', '##rds', '##rase', '##aa', '##lee', '##pro', '##ben', '##bu', '##aire', '##ifying', '##media', '##olis', '##par', '##hog', '##12', '##2', '##sto', '##bies', '##nga', '##ista', '##su', '##wart', '##fusion', '##fi', '##cas', '##hab', '##istan', '##vin', '##nh', '##won', '##jean', '##kins', '##ndi', '##rill', '##rra', '##eta', '##olin', '##bach', '##rr', '##kken', '##ruff', '##jk', '##tera', '##woman', '##selle', '##rse', '##eck', '##table', '##sma', '##gi', '##33', '##udge', '##ady', '##ues', '##ons', '##ries', '##use', '##trom', '##rie', '##66', '##meral', '##nst', '##umen', '##ath', '##eration', '##leader', '##ring', '##tr', '##vate', '##out', '##ens', '##unce', '##wai', '##mist', '##d', '##anne', '##ffe', '##rp', '##bey', '##lie', '##elia', '##uki', '##gina', '##port', '##dev', '##75', '##away', '##ard', '##17', '##cott', '##mot', '##nne', '##ame', '##ah', '##kar', '##tia', '##us', '##ople', '##ised', '##por', '##oun', '##rh', '##of', '##hit', '##ist', '##me', '##van', '##osi', '##ical', '##rites', '##wn', '##uba', '##sed', '##ml', '##45', '##lea', '##mb', '##av', '##co', '##uring', '##omi', '##sam', '##logic', '##bor', '##lves', '##pon', '##eria', '##ein', '##hana', '##fire', '##ech', '##nate', '##form', '##ze', '##nis', '##bf', '##the', '##ated', '##nce', '##egan', '##lls', '##mu', '##hara', '##tens', '##ssing', '##alle', '##fo', '##mac', '##girl', '##itzer', '##47', '##oit', '##im', '##osta', '##ep', '##dra', '##ada', '##ocene', '##ka', '##wal', '##de', '##hei', '##las', '##with', '##sh', '##rri', '##rel', '##kel', '##ong', '##re', '##net', '##outh', '##var', '##ash', '##trum', '##weather', '##met', '##pl', '##liga', '##ying', '##ap', '##gut', '##gai', '##ics', '##list', '##care', '##chel', '##bling', '##flict', '##ware', '##kind', '##ba', '##tive', '##i', '##te', '##lth', '##cans', '##champ', '##orro', '##words', '##rca', '##ouse', '##aud', '##26', '##pot', '##rst', '##hema', '##hn', '##uses', '##ac', '##alic', '##tech', '##isto', '##write', '##ents', '##j', '##bit', '##lom', '##hine', '##hony', '##drum', '##hler', '##stick', '##may', '##rai', '##vio', '##iers', '##iest', '##ua', '##xx', '##od', '##bl', '##bt', '##stle', '##uck', '##ering', '##nism', '##tta', '##rum', '##oi', '##ffi', '##ping', '##ova', '##tics', '##zzy', '##vot', '##very', '##ode', '##urs', '##ark', '##one', '##ough', '##ina', '##dt', '##ess', '##ono', '##factory', '##gb', '##itude', '##ici', '##cing', '##cton', '##inger', '##het', '##cta', '##ect', '##ach', '##sti', '##tha', '##eg', '##culture', '##vant', '##imi', '##ini', '##ber', '##drop', '##die', '##ous', '##ella', '##sd', '##itic', '##kus', '##5', '##bi', '##urity', '##tist', '##ese', '##gil', '##oy', '##vati', '##18', '##sts', '##jan', '##gate', '##lia', '##hell', '##ing', '##ulo', '##ush', '##elling', '##xi', '##lord', '##ists', '##boro', '##ulate', '##own', '##nder', '##load', '##avia', '##eng', '##powering', '##rit', '##se', '##tman', '##top', '##ki', '##yas', '##vy', '##iva', '##ther', '##don', '##enia', '##books', '##ease', '##ish', '##ference', '##ote', '##lda', '##tian', '##bate', '##ave', '##roup', '##ico', '##rid', '##tron', '##kk', '##ek', '##sit', '##que', '##at', '##tter', '##nology', '##gas', '##front', '##ieg', '##ne', '##azi', '##lay', '##oted', '##gon', '##igan', '##nya', '##ils', '##nna', '##lli', '##col', '##chen', '##gra', '##vs', '##wer', '##rant', '##now', '##gt', '##vie', '##pa', '##och', '##fra', '##ith', '##und', '##vo', '##sant', '##tree', '##race', '##ater', '##ace', '##oh', '##tow', '##gam', '##vable', '##uni', '##o', '##amen', '##ase', '##oli', '##rly', '##wire', '##xon', '##wash', '##ane', '##edge', '##gen', '##rice', '##rts', '##jon', '##pan', '##ipe', '##athi', '##tor', '##01', '##nies', '##hole', '##ction', '##pt', '##ifer', '##ment', '##ris', '##ions', '##hip', '##ado', '##ga', '##sett', '##cy', '##kir', '##cars', '##tal', '##ver', '##tions', '##oke', '##id', '##56', '##tl', '##ib', '##mise', '##wa', '##ctor', '##lley', '##hes', '##hl', '##tine', '##lt', '##aurus', '##ally', '##sse', '##sell', '##rm', '##phobic', '##ro', '##ors', '##ari', '##sie', '##beat', '##vre', '##fest', '##bus', '##dent', '##pas', '##block', '##mc', '##aw', '##tched', '##icide', '##el', '##rl', '##ass', '##ene', '##han', '##oo', '##rates', '##bot', '##ore', '##ians', '##ab', '##eed', '##heart', '##sp', '##ivation', '##nta', '##anda', '##ret', '##ire', '##tory', '##bert', '##ck', '##hal', '##due', '##rke', '##ited', '##zi', '##ified', '##ien', '##uit', '##be', '##ogan', '##press', '##pour', '##kur', '##ta', '##just', '##ility', '##ars', '##zers', '##vent', '##leg', '##tton', '##oic', '##ste', '##phi', '##ayton', '##mania', '##ch', '##q', '##gli', '##meric', '##take', '##mi', '##tern', '##qua', '##inate', '##zen', '##lau', '##100', '##emon', '##house', '##ma', '##qa', '##ai', '##via', '##cked', '##ize', '##nes', '##mart', '##ffed', '##lov', '##veda', '##zar', '##mare', '##ond', '##rea', '##ug', '##wick', '##op', '##hd', '##tee', '##bird', '##lez', '##udy', '##claiming', '##ning', '##shi', '##tea', '##hra', '##ivity', '##dia', '##gal', '##back', '##ade', '##san', '##ion', '##jah', '##db', '##sky', '##je', '##tein', '##lan', '##forth', '##ji', '##7', '##usion', '##arth', '##onate', '##he', '##ich', '##wil', '##rad', '##jak', '##sf', '##andra', '##ure', '##ops', '##kle', '##gg', '##cu', '##38', '##gor', '##raz', '##eca', '##lit', '##cs', '##nee', '##tagram', '##mm', '##hwa', '##bib', '##like', '##onis', '##a1', '##igen', '##nn', '##p', '##ede', '##ll', '##hu', '##si', '##ut', '##ke', '##aro', '##aman', '##ny', '##nostic', '##bow', '##z', '##loe', '##rup', '##ground', '##eis', '##fu', '##mur', '##mad', '##orin', '##sb', '##ness', '##zz', '##ello', '##olf', '##dot', '##hr', '##age', '##amy', '##gn', '##cky', '##ger', '##am', '##ves', '##ddle', '##is', '##wski', '##lon', '##ij', '##nea', '##ata', '##rch', '##bba', '##ott', '##sus', '##pass', '##tra', '##tara', '##lic', '##ais', '##ones', '##48', '##night', '##ant', '##ular', '##et', '##iate', '##it', '##llo', '##yr', '##fan', '##ppel', '##pres', '##mond', '##uma', '##nas', '##ssi', '##mir', '##art', '##cent', '##lies', '##90', '##rus', '##ria', '##mer', '##mt', '##ge', '##bil', '##esses', '##hp', '##8th', '##nick', '##ky', '##pe', '##nni', '##ties', '##eem', '##ache', '##nts', '##hid', '##hed', '##ays', '##put', '##lla', '##bee', '##nca', '##omp', '##nig', '##lim', '##ptic', '##cr', '##ied', '##ins', '##horse', '##gan', '##ege', '##tre', '##hini', '##kra', '##ak', '##ography', '##nus', '##lone', '##pu', '##smo', '##os', '##yd', '##lc', '##rno', '##rin', '##dies', '##rle', '##haus', '##itate', '##ws', '##mon', '##outs', '##oss', '##lock', '##ken', '##oth', '##sal', '##walk', '##eve', '##sco', '##ght', '##far', '##ceae', '##ii', '##tick', '##est', '##bc', '##fc', '##wei', '##bri', '##color', '##fr', '##fy', '##20', '##rio', '##ert', '##thic', '##hak', '##post', '##inn', '##oni', '##ey', '##sten', '##yre', '##4', '##hood', '##rich', '##taro', '##rem', '##ctions', '##wall', '##zo', '##ds', '##ump', '##rc', '##ker', '##hen', '##80', '##park', '##box', '##np', '##gum', '##slow', '##ming', '##ind', '##eric', '##iri', '##uh', '##oor', '##world', '##vel', '##hum', '##chet', '##do', '##m', '##sta', '##nb', '##beck', '##works', '##ent', '##ig', '##win', '##bbs', '##mun', '##icated', '##isch', '##pur', '##itated', '##gin', '##stic', '##ome', '##ym', '##if', '##mori', '##band', '##pal', '##made', '##belt', '##sho', '##ville', '##ditional', '##lz', '##x', '##ice', '##cana', '##ange', '##inated', '##nl', '##ulation', '##ann', '##ogy', '##ema', '##mar', '##tp', '##cer', '##ive', '##cute', '##type', '##ddy', '##sko', '##dav', '##rove', '##uppe', '##ks', '##ses', '##tus', '##pol', '##nt', '##not', '##ca', '##by', '##right', '##etta', '##rily', '##oting', '##wr', '##gl', '##ving', '##nbc', '##lor', '##erty', '##sat', '##lyn', '##dis', '##en', '##mie', '##llet', '##ful', '##stein', '##gging', '##nik', '##hon', '##34', '##vey', '##ot', '##ay', '##tti', '##grate', '##psy', '##st', '##ow', '##ran', '##ali', '##tan', '##46', '##wl', '##ian', '##bat', '##usions', '##vir', '##iere', '##tling', '##cot', '##raf', '##ino', '##dron', '##ora', '##kh', '##para', '##entes', '##dal', '##force', '##lco', '##ffer', '##gly', '##wi', '##less', '##rama', '##ts', '##eu', '##kers', '##case', '##abe', '##br', '##rina', '##nc', '##hg', '##eo', '##com', '##yo', '##tec', '##mate', '##ched', '##bal', '##hill', '##ool', '##bel', '##ton', '##ife', '##ec', '##to', '##ates', '##cian', '##ras', '##zing', '##anto', '##ku', '##wig', '##mas', '##ns', '##lib', '##mba', '##lion', '##all', '##sea', '##cks', '##thed', '##oma', '##tute', '##pop', '##wave', '##mark', '##ies', '##moral', '##nie', '##sher', '##gement', '##rand', '##ometer', '##rt', '##inal', '##bella', '##icle', '##talk', '##rier', '##gol', '##child', '##ele', '##nock', '##ggle', '##heard', '##uli', '##abi', '##ulf', '##cha', '##cial', '##down', '##ans', '##ology', '##eman', '##af', '##borough', '##sol', '##otto', '##bolic', '##ff', '##40', '##ng', '##es', '##mana', '##wo', '##lub', '##ami', '##era', '##cise', '##ly', '##del', '##fb', '##walker', '##tical', '##camp', '##hand', '##tv', '##vere', '##ick', '##in', '##ilde', '##igh', '##ume', '##dock', '##po', '##nzo', '##gnan', '##zine', '##11', '##ely', '##em', '##rica', '##more', '##on', '##ota', '##le', '##weight', '##music', '##well', '##nor', '##arm', '##borg', '##res', '##beau', '##ide', '##partisan', '##14', '##tag', '##rf', '##ar', '##atic', '##lett', '##nen', '##mp', '##bola', '##da', '##19', '##lma', '##cle', '##cci', '##bill', '##way', '##kin', '##plate', '##fed', '##iel', '##pm', '##ry', '##fly', '##gist', '##gur', '##nds', '##tte', '##ic', '##ate', '##ston', '##hire', '##ting', '##nier', '##cho', '##sel', '##hs', '##mg', '##ress', '##rao', '##nist', '##lova', '##cts', '##rd', '##70', '##ality', '##60', '##lam', '##bius', '##ivist', '##ed', '##elt', '##rah', '##unt', '##rth', '##f', '##cz', '##ams', '##our', '##tt', '##lly', '##aja', '##ere', '##mini', '##ear', '##gat', '##ope', '##cup', '##gun', '##lk', '##water', '##ience', '##dd', '##bone', '##bby', '##57', '##ily', '##ao', '##kr', '##rted', '##wt', '##wani', '##kes', '##zone', '##rae', '##ce', '##ped', '##riz', '##ration', '##lta', '##qui', '##ont', '##23', '##eto', '##rre', '##west', '##uz', '##ute', '##64', '##hing', '##rist', '##line', '##200', '##ex', '##idi', '##pis', '##plane', '##church', '##som', '##logue', '##unda', '##13', '##nge', '##mes', '##og', '##lad', '##rat', '##nding', '##umb', '##ept', '##chu', '##wd', '##uti', '##ose', '##eh', '##ust', '##pen', '##ount', '##my', '##hea', '##formed', '##hot', '##nation', '##ika', '##lence', '##nell', '##eld', '##dun', '##lines', '##bla', '##max', '##df', '##hia', '##ands', '##wice', '##22', '##wad', '##text', '##bar', '##long', '##sl', '##backs', '##ttle', '##ad', '##cc', '##orum', '##lich', '##oint', '##t', '##uf', '##bol', '##rys', '##ilo', '##torm', '##oe', '##mal', '##or', '##print', '##app', '##che', '##rb', '##boy', '##cting', '##86', '##ove', '##tour', '##ification', '##nr', '##sburg', '##rn', '##thi', '##tila', '##unk', '##hof', '##mis', '##sar', '##ama', '##nto', '##uls', '##tion', '##fin', '##head', '##lab', '##tura', '##pers', '##mith', '##ys', '##makers', '##chi', '##sphere', '##bank', '##ging', '##ending', '##elli', '##mina', '##ception', '##heads', '##con', '##wara', '##cor', '##ching', '##jou', '##lev', '##fight', '##th', '##rdi', '##hin', '##suit', '##isation', '##ques', '##obe', '##ncy', '##ats', '##oms', '##der', '##io', '##ike', '##nan', '##car', '##b', '##kha', '##cca', '##rac', '##yst', '##ters', '##iot', '##pi', '##les', '##ls', '##berg', '##usa', '##cut', '##bin', '##hee', '##vat', '##ero', '##kat', '##rew', '##ae', '##alla', '##ution', '##gno', '##ald', '##ram', '##gating', '##land', '##ental', '##ots', '##des', '##30', '##aus', '##wk', '##qual', '##ln', '##cratic', '##core', '##used', '##ica', '##fk', '##ois', '##light', '##ivision', '##hesis', '##pha', '##man', '##ox', '##aldi', '##als', '##ney', '##du', '##carriage', '##sa', '##tsa', '##gic', '##har', '##ril', '##cking', '##thing', '##rya', '##mo', '##dding', '##ub', '##lin', '##ale', '##old', '##29', '##lf', '##ter', '##lo', '##listic', '##fish', '##quest', '##rting', '##red', '##opa', '##berger', '##lou', '##over', '##ttering', '##pher', '##ign', '##rce', '##sun', '##fe', '##tors', '##eye', '##h', '##ine', '##la', '##son', '##creen', '##tate', '##med', '##bags', '##ui', '##ability', '##you', '##wich', '##ence', '##cco', '##ham', '##hi', '##pac', '##cre', '##laise', '##tle', '##wheel', '##wee', '##iga', '##ete', '##fully', '##making', '##gne', '##yl', '##uy', '##er', '##eptive', '##ines', '##rata', '##mit', '##moor', '##ros', '##yes', '##pies', '##rta', '##los', '##ph', '##ser', '##ook', '##word', '##hel', '##ury', '##v', '##fer', '##tom', '##dur', '##lmer', '##iam', '##kis', '##nk', '##roll', '##aur', '##ld', '##y', '##him', '##hall', '##vor', '##jn', '##tic', '##lu', '##ura', '##aki', '##ating', '##32', '##court', '##ul', '##esa', '##ur', '##dom', '##ve', '##ol', '##ders', '##cher', '##ling', '##pre', '##bation', '##ner', '##39', '##are', '##valent', '##hol', '##lip', '##org', '##ror', '##uel', '##ceptive', '##born', '##hosh', '##ches', '##zie', '##vc', '##ford', '##43', '##for', '##lars', '##nco', '##bell', '##41', '##sen', '##ni', '##truct', '##using', '##tur', '##mann', '##entation', '##nal', '##his', '##dc', '##wc', '##om', '##6', '##ja', '##vocation', '##ological', '##nat', '##dre', '##body', '##carbon', '##sser', '##ted', '##sai', '##ti', '##sin', '##cko', '##ishment', '##51', '##bn', '##pr', '##bre', '##icides', '##eyer', '##na', '##pet', '##vic', '##shu', '##bid', '##washed', '##emi', '##dic', '##hay', '##pes', '##cock', '##was', '##nath', '##dr', '##oc', '##eb', '##rlin', '##gre', '##grade', '##vet', '##lar', '##hawks', '##so', '##hale', '##cats', '##ives', '##al', '##mara', '##rot', '##jin', '##ana', '##ite', '##athing', '##tral', '##ef', '##cm', '##pressed', '##dar', '##gis', '##ail', '##aj', '##raj', '##ight', '##e', '##ju', '##yer', '##break', '##tlement', '##rley', '##olo', '##tum', '##iss', '##hora', '##ct', '##king', '##ured', '##acies', '##mos', '##ional', '##ude', '##ron', '##rita', '##verance', '##kor', '##roids', '##ente', '##dp', '##lary', '##note', '##mers', '##tz', '##real', '##thor', '##oj', '##ea', '##tos', '##tly', '##caster', '##sca', '##erus', '##buch', '##iti', '##oom', '##rney', '##xa', '##owed', '##pod', '##test', '##gible', '##gga', '##uca', '##ill', '##len', '##sr', '##ined', '##sian', '##hai', '##ise', '##ture', '##able', '##odes', '##per', '##sper', '##christ', '##att', '##sion', '##burn', '##worthy', '##bio', '##ip', '##rey', '##work', '##mma', '##ang', '##py', '##free', '##than', '##den', '##wat', '##paper', '##pid', '##city', '##tm', '##mic', '##r', '##tu', '##ral', '##lein', '##culus', '##ela', '##wyn', '##bo', '##ag', '##worth', '##hardt', '##ri', '##bury', '##oft', '##24', '##oto', '##gr', '##nton', '##ments', '##bb', '##42', '##ble', '##ster', '##ergy', '##war', '##val', '##ity', '##rial', '##hat', '##fires', '##31', '##1', '##uter', '##kam', '##ie', '##za', '##cide', '##raction', '##sil', '##ony', '##firm', '##ized', '##pf', '##ult', '##end', '##00', '##vas', '##sha', '##oid', '##cl', '##ko', '##alo', '##ual', '##foot', '##uga', '##placed', '##ix', '##cos', '##16', '##hri', '##life', '##eda', '##hc', '##lster', '##rs', '##time', '##pper', '##iente', '##enko', '##sm', '##cript', '##pp']}\n"
     ]
    }
   ],
   "source": [
    "dictionary_of_vocab_cluster_cleaned_test_samples = pickle.load(open(os.path.join(notebookDir, \"vocab/\" + unique_named_file), 'rb'))\n",
    "\n",
    "_ = [print(key,':',value) for key, value in dictionary_of_vocab_cluster_cleaned_test_samples.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_groups = len(dictionary_of_vocab_cluster_cleaned_test_samples[\"token_groupings\"])\n",
    "\n",
    "# Set up imputer object\n",
    "class Imputer_groupings:\n",
    "    def __init__(self, model, number_of_groups):\n",
    "        self.model = model\n",
    "        self.num_groups = number_of_groups\n",
    "    \n",
    "    def __call__(self, input_array, S):\n",
    "        max_length = 0\n",
    "        reconstructed_array = []\n",
    "        for index, sentence in enumerate(input_array):\n",
    "            length_of_sentence = max(x[0] for group in sentence for x in group)\n",
    "            if length_of_sentence > max_length:\n",
    "                max_length = length_of_sentence\n",
    "\n",
    "            original_input_ids = [None] * (length_of_sentence+1)\n",
    "            for sub_index, group in enumerate(sentence):\n",
    "                \n",
    "                if S[index][sub_index]: #put in '[MASK]' elements if needed\n",
    "                    for x in group:\n",
    "                        original_input_ids[x[0]]=x[1]\n",
    "                else: \n",
    "                    for x in group:\n",
    "                        original_input_ids[x[0]]=103 #id of '[MASK]'\n",
    "\n",
    "            original_input_ids.append(102)\n",
    "            original_input_ids.insert(0, 101)\n",
    "            reconstructed_array.append(original_input_ids)\n",
    "\n",
    "        max_length+=3\n",
    "\n",
    "        for index, sentence in enumerate(reconstructed_array):\n",
    "            if len(sentence) < max_length:\n",
    "                reconstructed_array[index].extend([0 for i in range(max_length-len(sentence))])\n",
    "\n",
    "        input_ids = np.array(reconstructed_array)\n",
    "\n",
    "        # \"handmade\" attention mask -> basically just set everything to one, except '[PAD]'s which are zero\n",
    "        am = np.ones(input_ids.shape)\n",
    "        am[input_ids == 0] = 0 #id of '[PAD]'\n",
    "        tensor_attention_mask = torch.tensor(am)\n",
    "        tensor_attention_mask = tensor_attention_mask.to(model.device)\n",
    "\n",
    "\n",
    "        tensor_input_ids = torch.tensor(input_ids)\n",
    "        tensor_input_ids = tensor_input_ids.to(model.device)\n",
    "\n",
    "\n",
    "        #predict with model\n",
    "        outputs = self.model(tensor_input_ids, tensor_attention_mask)\n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        \n",
    "        score_most_prob = [max(x) for x in outputs]\n",
    "\n",
    "        return np.array(score_most_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "es--syllables--0--stop_words--stop_words--8--stop_words--7--stop_words--stop_words--0--stop_words--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Politics, Religion, Race; we are so busy killing & hurting each other, Earth dries up & we will all die anyway. #drought #SemST\n",
      "4--punctuation--5--punctuation--1--punctuation--stop_words--stop_words--stop_words--0--6--punctuation--7--stop_words--stop_words--punctuation--5--2--syllables--stop_words\n",
      "--punctuation--stop_words--stop_words--stop_words--10--0--punctuation--punctuation--6--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "@sassy_gramma Good point but  Our heart starts beating 22 days after conception we are alive at conception. #SemST\n",
      "punctuation--2--syllables--punctuation--8--syllables--1--1--stop_words--stop_words--0--1--0--digits--0--stop_words--4--stop_words--stop_words--10--stop_words--4--\n",
      "punctuation--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Greater is He who is in you than he who is in the world. - 1John 4:4 #god #bible #SemST\n",
      "1--stop_words--stop_words--stop_words--stop_words--stop_words--stop_words--stop_words--stop_words--stop_words--stop_words--stop_words--stop_words--9--punctuation--\n",
      "punctuation--digits--syllables--syllables--digits--punctuation--digits--punctuation--10--punctuation--10--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "RT @mcirelli92: Hillary for the win #SemST\n",
      "3--punctuation--2--syllables--syllables--syllables--syllables--punctuation--2--stop_words--stop_words--0--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "You want to hear something really ugly, 1 in 5 women will be sexually assaulted in their time in college #MisogynyIsUgly #SemST\n",
      "stop_words--7--stop_words--11--0--0--11--punctuation--digits--stop_words--digits--0--stop_words--stop_words--7--6--stop_words--stop_words--0--stop_words--12--punctuation\n",
      "--3--syllables--syllables--syllables--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Murdering an unborn child is the crudest form of contraception! #Catholic #Christian #Conservative #feminist #SemST\n",
      "10--stop_words--3--syllables--6--stop_words--stop_words--4--syllables--1--stop_words--2--syllables--punctuation--punctuation--2--punctuation--2--punctuation--4--\n",
      "punctuation--5--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Thank you for your kind RTs and FAVs @pamnsc !!! #WakeUpAmerica #todosmarchamos #Cuba #SemST\n",
      "0--stop_words--stop_words--stop_words--0--3--syllables--stop_words--3--syllables--punctuation--2--syllables--syllables--punctuation--punctuation--punctuation--punctuation\n",
      "--0--syllables--syllables--syllables--punctuation--2--syllables--syllables--syllables--syllables--punctuation--5--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "@ErikLoomis You're a historian? Of what , malice? Your hatred of her child is astonishing. #palin #SemST\n",
      "punctuation--2--syllables--syllables--syllables--stop_words--punctuation--stop_words--stop_words--10--punctuation--stop_words--stop_words--punctuation--6--punctuation--\n",
      "stop_words--6--stop_words--stop_words--6--stop_words--1--punctuation--punctuation--2--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Full of #narcissism and self-loathing, a lot of women out there like that. Its got sod all to do with #WrightStuff #WolfWhistling #SemST\n",
      "1--stop_words--punctuation--3--syllables--syllables--syllables--stop_words--10--punctuation--3--syllables--punctuation--stop_words--0--stop_words--0--stop_words--\n",
      "stop_words--1--stop_words--punctuation--stop_words--1--stop_words--syllables--stop_words--stop_words--stop_words--stop_words--punctuation--2--syllables--syllables--\n",
      "syllables--punctuation--4--syllables--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      ":) May Your Heart Be Firmly Fixed, Trusting The LORD...  He Shall Not Falter, He Cannot Fail... May His Breath Guide Your Sail...  #SemST\n",
      "punctuation--punctuation--1--stop_words--0--stop_words--1--1--punctuation--6--stop_words--10--punctuation--punctuation--punctuation--stop_words--9--stop_words--3--\n",
      "syllables--punctuation--stop_words--1--1--punctuation--punctuation--punctuation--1--stop_words--0--1--stop_words--10--punctuation--punctuation--punctuation--punctuation--\n",
      "3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "#Mission:#Climate @ home > Defrost old fridges and freezers regularly #Tip #LoveOurPlanet #SemST\n",
      "punctuation--9--punctuation--punctuation--1--punctuation--7--punctuation--3--syllables--syllables--11--11--syllables--stop_words--11--syllables--1--punctuation--11--\n",
      "punctuation--0--syllables--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "The people who never spoke up against the Nazis during World War II were likely \"live and let live \" people. #fb #SemST\n",
      "stop_words--9--stop_words--0--0--stop_words--stop_words--stop_words--2--stop_words--9--5--3--stop_words--1--punctuation--7--stop_words--0--7--punctuation--9--punctuation\n",
      "--punctuation--3--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Could all those who believe in a god please leave. The meeting will now continue for the grown ups only. #SemST\n",
      "7--stop_words--stop_words--stop_words--0--stop_words--stop_words--10--1--1--punctuation--stop_words--12--stop_words--stop_words--1--stop_words--stop_words--4--1--\n",
      "stop_words--punctuation--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Interestingly burning your wife alive suspecting her chastity is a great justice Ram style. @RamaY_BRF @madhukishwar @authoramish #SemST\n",
      "1--syllables--7--stop_words--6--10--10--syllables--stop_words--3--syllables--syllables--stop_words--stop_words--1--6--3--1--punctuation--punctuation--2--syllables--\n",
      "punctuation--3--syllables--punctuation--0--syllables--syllables--syllables--syllables--punctuation--0--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "If the Olympics were 100% feminist, the men's division would be erradicated for \"equality\"  #GamerGate #SemST\n",
      "stop_words--stop_words--2--stop_words--digits--punctuation--5--punctuation--stop_words--0--punctuation--stop_words--0--7--stop_words--3--syllables--syllables--stop_words\n",
      "--punctuation--5--punctuation--punctuation--1--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Joining us in studio is Amos and Josh on Music and Career with Kobi Kihara #Bands #SemST\n",
      "12--9--stop_words--1--stop_words--2--stop_words--2--stop_words--0--stop_words--0--stop_words--3--syllables--3--syllables--punctuation--0--punctuation--3--syllables--\n",
      "syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "when they say men look at women like a piece of meat what do they even mean, they want to cook & eat her? #YesAllWomen #SemST\n",
      "stop_words--stop_words--0--0--0--stop_words--0--1--stop_words--7--stop_words--11--stop_words--stop_words--stop_words--0--0--punctuation--stop_words--7--stop_words--11--\n",
      "punctuation--11--stop_words--punctuation--punctuation--11--syllables--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "IF FEMINISTS WERE HONEST  \"I don't want to learn skills, I want others to work under my name.\"  #GamerGate #SemST\n",
      "stop_words--5--syllables--stop_words--0--punctuation--stop_words--stop_words--punctuation--stop_words--7--stop_words--9--9--punctuation--stop_words--7--9--stop_words--1--\n",
      "stop_words--stop_words--9--punctuation--punctuation--punctuation--1--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "'Amen, I say to you, whatever you did for one of these least brothers of mine, you did for Me.'  Mt 25:40  #ProLifeYouth #SemST\n",
      "punctuation--stop_words--syllables--punctuation--stop_words--0--stop_words--stop_words--punctuation--0--stop_words--stop_words--stop_words--1--stop_words--stop_words--1--\n",
      "6--stop_words--7--punctuation--stop_words--stop_words--stop_words--stop_words--punctuation--punctuation--2--digits--punctuation--digits--punctuation--0--syllables--\n",
      "syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "The fact that Chris Brown is somehow still famous is why I need #SemST\n",
      "stop_words--7--stop_words--2--11--stop_words--1--0--1--stop_words--stop_words--stop_words--7--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "@hillaryclinton - I guess this means you're happy for me to be pro-life as I never aborted any of my own children? #hypocrisy #SemST\n",
      "punctuation--2--syllables--syllables--punctuation--stop_words--7--stop_words--9--stop_words--punctuation--stop_words--0--stop_words--stop_words--stop_words--stop_words--0\n",
      "--punctuation--6--stop_words--stop_words--0--3--syllables--stop_words--stop_words--stop_words--stop_words--6--punctuation--punctuation--3--syllables--syllables--syllables\n",
      "--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "If you do not repent your emission of CO2 you will end up in hell #bible #SemST\n",
      "stop_words--stop_words--stop_words--stop_words--1--syllables--stop_words--4--stop_words--3--syllables--stop_words--stop_words--1--stop_words--stop_words--0--punctuation--\n",
      "10--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "@BarackObama #Love_peace and peaceful coexistence of all no to violence and #terrorism #SemST\n",
      "punctuation--2--syllables--syllables--punctuation--0--punctuation--4--stop_words--11--2--syllables--syllables--syllables--stop_words--stop_words--stop_words--stop_words--\n",
      "6--stop_words--punctuation--6--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Faithful God, we #pray that we may learn to trust the uncertainty & mystery of walking on water toward you #SemST\n",
      "0--10--punctuation--stop_words--punctuation--10--stop_words--stop_words--1--9--stop_words--9--stop_words--4--punctuation--5--stop_words--1--stop_words--7--1--stop_words--\n",
      "punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "@oreillyfactor \"@HillaryClinton can't even manage @billclinton and she wants to be @POTUS\" #lol #WakeUpAmerica #StopHillary2016 #SemST\n",
      "punctuation--7--syllables--syllables--syllables--punctuation--punctuation--2--syllables--syllables--stop_words--punctuation--stop_words--0--1--punctuation--1--syllables--\n",
      "syllables--stop_words--stop_words--0--stop_words--stop_words--punctuation--11--syllables--punctuation--punctuation--3--syllables--punctuation--0--syllables--syllables--\n",
      "syllables--punctuation--7--syllables--syllables--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "feminist response people wearing meninist clothes-  mock them or call them derogatory names? NOT A HATE GROUP ??? #SemST\n",
      "5--1--9--11--0--syllables--syllables--1--punctuation--6--stop_words--stop_words--1--stop_words--3--syllables--syllables--9--punctuation--stop_words--stop_words--6--9--\n",
      "punctuation--punctuation--punctuation--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Before going to church, African Americans should realize that their make believe god supported slavery.  #confed2015 #SemST\n",
      "stop_words--0--stop_words--10--punctuation--5--5--stop_words--7--stop_words--stop_words--1--0--10--1--6--punctuation--punctuation--3--syllables--syllables--syllables--\n",
      "punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "@camerongreen22 Young man, stop wearing your sister's clothes and makeup. You look ridiculous. #SpankAFeminist #SemST\n",
      "punctuation--2--syllables--syllables--syllables--6--11--punctuation--7--11--stop_words--6--punctuation--stop_words--1--stop_words--11--punctuation--stop_words--0--0--\n",
      "punctuation--punctuation--1--syllables--syllables--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "If you are in need of prayer or encouragement, please call my 24/7 prayerline at 772-324-9040. #God bless you! #PrayerChangesThings #SemST\n",
      "stop_words--stop_words--stop_words--stop_words--7--stop_words--10--stop_words--4--punctuation--1--1--stop_words--digits--punctuation--digits--10--syllables--stop_words--\n",
      "digits--syllables--punctuation--digits--punctuation--digits--syllables--punctuation--punctuation--10--10--stop_words--punctuation--punctuation--10--syllables--syllables--\n",
      "syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "At least the few mistakes made by @GOP candidates on the campaign aren't felonies like the ones@HillaryClinton makes.#OhHillNo #SemST\n",
      "stop_words--1--stop_words--stop_words--6--1--stop_words--punctuation--0--syllables--6--stop_words--stop_words--6--stop_words--punctuation--stop_words--3--syllables--\n",
      "syllables--1--stop_words--1--punctuation--2--syllables--syllables--0--punctuation--punctuation--11--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "@HillaryClinton Stop raising taxes and let people keep more of their wages! #LibertyNotHillary #StandWithRand #SemST\n",
      "punctuation--2--syllables--syllables--7--6--1--stop_words--0--9--0--stop_words--stop_words--stop_words--6--punctuation--punctuation--4--syllables--syllables--syllables--\n",
      "punctuation--1--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "I refuse to accept that there is a unbreakable glass ceiling, after all if you hit glass hard enough it will shatter #glassceiling #SemST\n",
      "stop_words--9--stop_words--1--stop_words--stop_words--stop_words--stop_words--3--syllables--syllables--11--7--punctuation--stop_words--stop_words--stop_words--stop_words\n",
      "--1--11--0--0--stop_words--stop_words--5--punctuation--11--syllables--syllables--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "I take it personally when people call my dog a he. Toni is a girls name too #SemST\n",
      "stop_words--0--stop_words--0--stop_words--9--1--stop_words--4--stop_words--stop_words--punctuation--2--stop_words--stop_words--0--9--stop_words--punctuation--3--syllables\n",
      "--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "I do not wish for #Women to have power over #Men but over themselves. #Milan #KnockKnock #PAK_Army #PTIFamily #SemST\n",
      "stop_words--stop_words--stop_words--0--stop_words--punctuation--0--stop_words--stop_words--9--stop_words--punctuation--0--stop_words--stop_words--stop_words--punctuation\n",
      "--punctuation--2--punctuation--5--syllables--syllables--punctuation--2--punctuation--5--punctuation--3--syllables--syllables--syllables--punctuation--3--syllables--\n",
      "syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Great meeting @balboalice and @CTwigg1 today to discuss Future Cities and how UK expertise can be shared & applied in #China #SemST\n",
      "1--12--punctuation--3--syllables--syllables--syllables--stop_words--punctuation--8--syllables--syllables--syllables--0--stop_words--12--9--7--stop_words--stop_words--5--1\n",
      "--stop_words--stop_words--9--punctuation--1--stop_words--punctuation--5--punctuation--3--syllables--syllables\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "current_groupings = dictionary_of_vocab_cluster_cleaned_test_samples[\"token_groupings\"]\n",
    "feature_names = list(current_groupings.keys())\n",
    "sage_input_all_instances = []\n",
    "for text_element in dictionary_of_vocab_cluster_cleaned_test_samples[\"text\"][0]:\n",
    "    sage_input_groupings = [[] for x in range(number_of_groups)]\n",
    "    input_text = tokenizer.tokenize(text_element)\n",
    "    input_ids = tokenizer(text_element, add_special_tokens=False)[\"input_ids\"]\n",
    "    groups_to_text = [next((key for key, value in current_groupings.items() if x in value)) for x in input_text]\n",
    "    print(\"\\n\".join(textwrap.wrap(text_element, 170)))\n",
    "    print(\"\\n\".join(textwrap.wrap(\"--\".join(groups_to_text), 170)))\n",
    "    print(\"-\"*170)\n",
    "    positioned_ids = [(i, x) for i, x in enumerate(input_ids)]\n",
    "    _ = [sage_input_groupings[list(current_groupings.keys()).index(groups_to_text[index])].append(x)  for index, x in enumerate(positioned_ids)]\n",
    "    sage_input_all_instances.append(sage_input_groupings)\n",
    "\n",
    "sage_input_all_instances_array = np.array(sage_input_all_instances, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer_groupings(model, number_of_groups)\n",
    "groupings_estimator = sage.PermutationEstimator(imputer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_348983/605050170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msage_values_groupings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroupings_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msage_input_all_instances_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_of_vocab_cluster_cleaned_test_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nlp2/lib/python3.7/site-packages/sage/permutation_estimator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, batch_size, detect_convergence, thresh, n_permutations, min_coalition, max_coalition, verbose, bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         X, Y = utils.verify_model_data(self.imputer, X, Y, self.loss_fn,\n\u001b[0;32m---> 64\u001b[0;31m                                        batch_size)\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Determine min/max coalition sizes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp2/lib/python3.7/site-packages/sage/utils.py\u001b[0m in \u001b[0;36mverify_model_data\u001b[0;34m(imputer, X, Y, loss, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Check labels shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_348983/218671702.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_array, S)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#predict with model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# sage_values = estimator(x, y)\n",
    "sage_values_groupings = groupings_estimator(sage_input_all_instances_array, np.array(dictionary_of_vocab_cluster_cleaned_test_samples[\"text\"][1]), batch_size=64, verbose=True,thresh=0.10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "metadata": {},
     "execution_count": 210
    }
   ],
   "source": [
    "np.ones((64, imputer.num_groups), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[list([(13, 2034)]), list([(3, 2131), (8, 2350), (11, 2478)]),\n",
       "        list([]), ...,\n",
       "        list([(1, 1005), (6, 1012), (24, 1012), (25, 1001)]), list([]),\n",
       "        list([(21, 19170), (22, 13320), (23, 3258), (27, 5244), (28, 2102)])],\n",
       "       [list([(24, 2409)]), list([(9, 2694), (12, 2086)]),\n",
       "        list([(1, 16606)]), ...,\n",
       "        list([(13, 1010), (17, 1005), (22, 1001), (28, 1012), (29, 1001), (33, 1001)]),\n",
       "        list([(11, 4029)]),\n",
       "        list([(4, 20715), (5, 4103), (31, 16558), (32, 5657), (35, 5244), (36, 2102)])],\n",
       "       [list([]), list([]), list([]), ...,\n",
       "        list([(0, 1001), (16, 1005), (19, 1001), (24, 1001), (27, 1001)]),\n",
       "        list([]),\n",
       "        list([(2, 3669), (3, 6961), (4, 18900), (5, 3334), (21, 2964), (22, 3126), (23, 4063), (26, 27179), (29, 5244), (30, 2102)])],\n",
       "       ...,\n",
       "       [list([(10, 2467), (16, 2409), (27, 2879)]), list([]), list([]),\n",
       "        ...,\n",
       "        list([(3, 1006), (7, 1011), (14, 1007), (20, 1005), (28, 1013), (31, 1012), (32, 1001)]),\n",
       "        list([]), list([(34, 5244), (35, 2102)])],\n",
       "       [list([(0, 2308), (2, 2272)]),\n",
       "        list([(7, 2067), (8, 11302), (15, 2147), (26, 2131), (29, 2067)]),\n",
       "        list([]), ...,\n",
       "        list([(6, 1011), (9, 1012), (13, 1005), (19, 1005), (30, 1012), (31, 1001)]),\n",
       "        list([]), list([(33, 5244), (34, 2102)])],\n",
       "       [list([]), list([]), list([]), ...,\n",
       "        list([(3, 1011), (6, 1010), (9, 1012), (10, 1001), (13, 1001), (17, 1001)]),\n",
       "        list([]),\n",
       "        list([(8, 26332), (12, 3085), (15, 3540), (16, 13901), (19, 5244), (20, 2102)])]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 205
    }
   ],
   "source": [
    "sage_input_all_instances_array[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_estimator = sage.SignEstimator(Imputer_groupings())\n",
    "sage_sign_groupings = sign_estimator(sage_input_all_instances_array, np.array(dictionary_of_vocab_cluster_cleaned_test_samples[\"text\"][1]), batch_size=64,sign_confidence=0.95,narrow_thresh=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_sign_groupings.plot_sign(feature_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_values_groupings.plot(feature_names) "
   ]
  }
 ]
}