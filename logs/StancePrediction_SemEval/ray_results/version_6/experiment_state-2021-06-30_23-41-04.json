{
  "checkpoints": [
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00004\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0018945899596614023,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0018945899596614023,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"4_batch_size=64,learning_rate=0.0018946\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.603275179862976,\n    \"mean_F1\": 0.6252158880233765,\n    \"time_this_iter_s\": 803.4535365104675,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 8,\n    \"experiment_id\": \"857fea385ff741fc9c0515160a022ad9\",\n    \"date\": \"2021-07-01_01-30-32\",\n    \"timestamp\": 1625095832,\n    \"time_total_s\": 6564.7099850177765,\n    \"pid\": 155845,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0018945899596614023,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 6564.7099850177765,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 8,\n    \"trial_id\": \"deb7c_00004\",\n    \"experiment_tag\": \"4_batch_size=64,learning_rate=0.0018946\"\n  },\n  \"last_update_time\": 1625095832.468852,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9489690065383911,\n      \"min\": 1.603275179862976,\n      \"avg\": 1.7202920019626617,\n      \"last\": 1.603275179862976,\n      \"last-5-avg\": 1.6646923065185546,\n      \"last-10-avg\": 1.7202920019626617\n    },\n    \"mean_F1\": {\n      \"max\": 0.6252158880233765,\n      \"min\": 0.5332850813865662,\n      \"avg\": 0.5994311571121216,\n      \"last\": 0.6252158880233765,\n      \"last-5-avg\": 0.6171247601509094,\n      \"last-10-avg\": 0.5994311571121216\n    },\n    \"time_this_iter_s\": {\n      \"max\": 868.8258180618286,\n      \"min\": 803.4535365104675,\n      \"avg\": 820.5887481272221,\n      \"last\": 803.4535365104675,\n      \"last-5-avg\": 811.6418281078338,\n      \"last-10-avg\": 820.5887481272221\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.125,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.125\n    },\n    \"training_iteration\": {\n      \"max\": 8,\n      \"min\": 1,\n      \"avg\": 4.5,\n      \"last\": 8,\n      \"last-5-avg\": 6.0,\n      \"last-10-avg\": 4.5\n    },\n    \"timestamp\": {\n      \"max\": 1625095832,\n      \"min\": 1625090136,\n      \"avg\": 1625092991.2499998,\n      \"last\": 1625095832,\n      \"last-5-avg\": 1625094210.8,\n      \"last-10-avg\": 1625092991.25\n    },\n    \"time_total_s\": {\n      \"max\": 6564.7099850177765,\n      \"min\": 868.8258180618286,\n      \"avg\": 3724.065867364406,\n      \"last\": 6564.7099850177765,\n      \"last-5-avg\": 4943.763135337829,\n      \"last-10-avg\": 3724.0658673644066\n    },\n    \"pid\": {\n      \"max\": 155845,\n      \"min\": 155845,\n      \"avg\": 155845.0,\n      \"last\": 155845,\n      \"last-5-avg\": 155845.0,\n      \"last-10-avg\": 155845.0\n    },\n    \"time_since_restore\": {\n      \"max\": 6564.7099850177765,\n      \"min\": 868.8258180618286,\n      \"avg\": 3724.065867364406,\n      \"last\": 6564.7099850177765,\n      \"last-5-avg\": 4943.763135337829,\n      \"last-10-avg\": 3724.0658673644066\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 8,\n      \"min\": 1,\n      \"avg\": 4.5,\n      \"last\": 8,\n      \"last-5-avg\": 6.0,\n      \"last-10-avg\": 4.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0018945899596614023,\n      \"min\": 0.0018945899596614023,\n      \"avg\": 0.0018945899596614021,\n      \"last\": 0.0018945899596614023,\n      \"last-5-avg\": 0.0018945899596614023,\n      \"last-10-avg\": 0.0018945899596614026\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64.0,\n      \"last\": 64,\n      \"last-5-avg\": 64.0,\n      \"last-10-avg\": 64.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffa7fa640000000473ffa8c76a0000000473ffc521d80000000473ffa27a7c0000000473ff9a703e0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fff2efa20000000473ffcbca120000000473ffb1a0600000000473ffa7fa640000000473ffa8c76a0000000473ffc521d80000000473ffa27a7c0000000473ff9a703e0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe38af8c0000000473fe3c48060000000473fe3a0c900000000473fe3cb6740000000473fe401c4c0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe110abe0000000473fe27cf000000000473fe3294800000000473fe38af8c0000000473fe3c48060000000473fe3a0c900000000473fe3cb6740000000473fe401c4c0000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740891d25e7600000474089bba01d0000004740897c141fa000004740896131562000004740891ba0d7c00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b269b46800000474089ec786280000047408940ee11c000004740891d25e7600000474089bba01d0000004740897c141fa000004740896131562000004740891ba0d7c00000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b044b054b064b074b08652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b044b054b064b074b08652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ae1f1dc604a19f5dc604a48f8dc604a74fbdc604a98fedc60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a58e8dc604a96ebdc604abeeedc604ae1f1dc604a19f5dc604a48f8dc604a74fbdc604a98fedc60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740a9dc49e88800004740b02598f7e400004740b3551b7bd800004740b68141a69c00004740b9a4b5c1940000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b269b4680000047409a8989d48000004740a395006eb000004740a9dc49e88800004740b02598f7e400004740b3551b7bd800004740b68141a69c00004740b9a4b5c1940000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ac56002004ac56002004ac56002004ac56002004ac5600200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ac56002004ac56002004ac56002004ac56002004ac56002004ac56002004ac56002004ac5600200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740a9dc49e88800004740b02598f7e400004740b3551b7bd800004740b68141a69c00004740b9a4b5c1940000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b269b4680000047409a8989d48000004740a395006eb000004740a9dc49e88800004740b02598f7e400004740b3551b7bd800004740b68141a69c00004740b9a4b5c1940000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b044b054b064b074b08652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b044b054b064b074b08652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6473f5f0a7c7aa0fbe6652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b404b404b404b404b40652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b404b404b404b404b404b404b404b40652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625089264.59083,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00004_4_batch_size=64,learning_rate=0.0018946_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00007\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.001130746407766284,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.001130746407766284,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"7_batch_size=16,learning_rate=0.0011307\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.558098316192627,\n    \"mean_F1\": 0.6568366885185242,\n    \"time_this_iter_s\": 756.218278169632,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 20,\n    \"experiment_id\": \"4cd480f362be45a190453d908aa73f2c\",\n    \"date\": \"2021-07-01_03-55-53\",\n    \"timestamp\": 1625104553,\n    \"time_total_s\": 15285.236867427826,\n    \"pid\": 155849,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.001130746407766284,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 15285.236867427826,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 20,\n    \"trial_id\": \"deb7c_00007\",\n    \"experiment_tag\": \"7_batch_size=16,learning_rate=0.0011307\"\n  },\n  \"last_update_time\": 1625104553.227029,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9709546566009521,\n      \"min\": 1.5520122051239014,\n      \"avg\": 1.649991500377655,\n      \"last\": 1.558098316192627,\n      \"last-5-avg\": 1.6060095071792602,\n      \"last-10-avg\": 1.5978833198547364\n    },\n    \"mean_F1\": {\n      \"max\": 0.6568366885185242,\n      \"min\": 0.5436974763870239,\n      \"avg\": 0.631282901763916,\n      \"last\": 0.6568366885185242,\n      \"last-5-avg\": 0.6534963846206665,\n      \"last-10-avg\": 0.6499769747257232\n    },\n    \"time_this_iter_s\": {\n      \"max\": 778.8149156570435,\n      \"min\": 750.9997749328613,\n      \"avg\": 764.2618433713911,\n      \"last\": 756.218278169632,\n      \"last-5-avg\": 765.4833488464355,\n      \"last-10-avg\": 766.0629875898361\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.05,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.1\n    },\n    \"training_iteration\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"timestamp\": {\n      \"max\": 1625104553,\n      \"min\": 1625090040,\n      \"avg\": 1625097289.7999995,\n      \"last\": 1625104553,\n      \"last-5-avg\": 1625103027.4,\n      \"last-10-avg\": 1625101111.2\n    },\n    \"time_total_s\": {\n      \"max\": 15285.236867427826,\n      \"min\": 772.9747030735016,\n      \"avg\": 8022.487846839428,\n      \"last\": 15285.236867427826,\n      \"last-5-avg\": 13760.049547147752,\n      \"last-10-avg\": 11843.851861000061\n    },\n    \"pid\": {\n      \"max\": 155849,\n      \"min\": 155849,\n      \"avg\": 155849.0,\n      \"last\": 155849,\n      \"last-5-avg\": 155849.0,\n      \"last-10-avg\": 155849.0\n    },\n    \"time_since_restore\": {\n      \"max\": 15285.236867427826,\n      \"min\": 772.9747030735016,\n      \"avg\": 8022.487846839428,\n      \"last\": 15285.236867427826,\n      \"last-5-avg\": 13760.049547147752,\n      \"last-10-avg\": 11843.851861000061\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.001130746407766284,\n      \"min\": 0.001130746407766284,\n      \"avg\": 0.0011307464077662842,\n      \"last\": 0.001130746407766284,\n      \"last-5-avg\": 0.001130746407766284,\n      \"last-10-avg\": 0.0011307464077662842\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16.0,\n      \"last\": 16,\n      \"last-5-avg\": 16.0,\n      \"last-10-avg\": 16.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffaefa040000000473ff9bd3bc0000000473ff9f51c20000000473ff8eb2280000000473ff8edf880000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffa7d37c0000000473ff9622840000000473ff8d72520000000473ff8d50ac0000000473ff9a2aa00000000473ffaefa040000000473ff9bd3bc0000000473ff9f51c20000000473ff8eb2280000000473ff8edf880000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe4d324c0000000473fe4da7de0000000473fe4e5dc80000000473fe4f6e8c0000000473fe504ce60000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe48df640000000473fe498aa40000000473fe4b30060000000473fe4c809e0000000473fe4cd3c20000000473fe4d324c0000000473fe4da7de0000000473fe4e5dc80000000473fe4f6e8c0000000473fe504ce60000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474087f507fca0000047408829298c200000474087f2d64ba00000474087e88ea1000000474087a1bf08a00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740881f60bec00000474087f33017400000474087fb12e1200000474087c2f5a6e00000474087f91b20600000474087f507fca0000047408829298c200000474087f2d64ba00000474087e88ea1000000474087a1bf08a00000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ab414dd604ab917dd604ab71add604ab41ddd604aa920dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ac005dd604abe08dd604abe0bdd604ab60edd604ab511dd604ab414dd604ab917dd604ab71add604ab41ddd604aa920dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c7e039799600004740c962cc125800004740cae1f9771200004740cc6082612200004740cdda9e51ac0000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c06643bdd200004740c1e576bf4600004740c36527ed5800004740c4e15747c600004740c660e8f9cc00004740c7e039799600004740c962cc125800004740cae1f9771200004740cc6082612200004740cdda9e51ac0000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ac96002004ac96002004ac96002004ac96002004ac9600200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ac96002004ac96002004ac96002004ac96002004ac96002004ac96002004ac96002004ac96002004ac96002004ac9600200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c7e039799600004740c962cc125800004740cae1f9771200004740cc6082612200004740cdda9e51ac0000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c06643bdd200004740c1e576bf4600004740c36527ed5800004740c4e15747c600004740c660e8f9cc00004740c7e039799600004740c962cc125800004740cae1f9771200004740cc6082612200004740cdda9e51ac0000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e473f5286b1b5d9eb0e652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b104b104b104b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b104b104b104b104b104b104b104b104b104b10652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059540000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625089264.6711528,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00007_7_batch_size=16,learning_rate=0.0011307_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00015\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 3.1446145910362104e-05,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 3.1446145910362104e-05,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"15_batch_size=128,learning_rate=3.1446e-05\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.6362667083740234,\n    \"mean_F1\": 0.3449328541755676,\n    \"time_this_iter_s\": 954.0982780456543,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"391971d88c6f4dd9a10e1030a352760f\",\n    \"date\": \"2021-07-01_00-52-16\",\n    \"timestamp\": 1625093536,\n    \"time_total_s\": 954.0982780456543,\n    \"pid\": 170739,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 3.1446145910362104e-05,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 954.0982780456543,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00015\",\n    \"experiment_tag\": \"15_batch_size=128,learning_rate=3.1446e-05\"\n  },\n  \"last_update_time\": 1625093536.8941777,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.6362667083740234,\n      \"min\": 2.6362667083740234,\n      \"avg\": 2.6362667083740234,\n      \"last\": 2.6362667083740234,\n      \"last-5-avg\": 2.6362667083740234,\n      \"last-10-avg\": 2.6362667083740234\n    },\n    \"mean_F1\": {\n      \"max\": 0.3449328541755676,\n      \"min\": 0.3449328541755676,\n      \"avg\": 0.3449328541755676,\n      \"last\": 0.3449328541755676,\n      \"last-5-avg\": 0.3449328541755676,\n      \"last-10-avg\": 0.3449328541755676\n    },\n    \"time_this_iter_s\": {\n      \"max\": 954.0982780456543,\n      \"min\": 954.0982780456543,\n      \"avg\": 954.0982780456543,\n      \"last\": 954.0982780456543,\n      \"last-5-avg\": 954.0982780456543,\n      \"last-10-avg\": 954.0982780456543\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625093536,\n      \"min\": 1625093536,\n      \"avg\": 1625093536,\n      \"last\": 1625093536,\n      \"last-5-avg\": 1625093536,\n      \"last-10-avg\": 1625093536\n    },\n    \"time_total_s\": {\n      \"max\": 954.0982780456543,\n      \"min\": 954.0982780456543,\n      \"avg\": 954.0982780456543,\n      \"last\": 954.0982780456543,\n      \"last-5-avg\": 954.0982780456543,\n      \"last-10-avg\": 954.0982780456543\n    },\n    \"pid\": {\n      \"max\": 170739,\n      \"min\": 170739,\n      \"avg\": 170739,\n      \"last\": 170739,\n      \"last-5-avg\": 170739,\n      \"last-10-avg\": 170739\n    },\n    \"time_since_restore\": {\n      \"max\": 954.0982780456543,\n      \"min\": 954.0982780456543,\n      \"avg\": 954.0982780456543,\n      \"last\": 954.0982780456543,\n      \"last-5-avg\": 954.0982780456543,\n      \"last-10-avg\": 954.0982780456543\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 3.1446145910362104e-05,\n      \"min\": 3.1446145910362104e-05,\n      \"avg\": 3.1446145910362104e-05,\n      \"last\": 3.1446145910362104e-05,\n      \"last-5-avg\": 3.1446145910362104e-05,\n      \"last-10-avg\": 3.1446145910362104e-05\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474005171300000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474005171300000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fd6136140000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fd6136140000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408dd0c946000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408dd0c946000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944aa0f5dc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944aa0f5dc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408dd0c946000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408dd0c946000000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944af39a0200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944af39a0200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408dd0c946000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408dd0c946000000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f007ca1589e1290612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f007ca1589e1290612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625092567.65086,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00015_15_batch_size=128,learning_rate=3.1446e-05_2021-07-01_00-26-08\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00006\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0016901729991646017,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0016901729991646017,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"6_batch_size=128,learning_rate=0.0016902\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.012173652648926,\n    \"mean_F1\": 0.45787546038627625,\n    \"time_this_iter_s\": 942.0430061817169,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"a8e6c23402a340ad9cac7e548693f18f\",\n    \"date\": \"2021-06-30_23-56-50\",\n    \"timestamp\": 1625090210,\n    \"time_total_s\": 942.0430061817169,\n    \"pid\": 155850,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0016901729991646017,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 942.0430061817169,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00006\",\n    \"experiment_tag\": \"6_batch_size=128,learning_rate=0.0016902\"\n  },\n  \"last_update_time\": 1625090210.1749294,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.012173652648926,\n      \"min\": 2.012173652648926,\n      \"avg\": 2.012173652648926,\n      \"last\": 2.012173652648926,\n      \"last-5-avg\": 2.012173652648926,\n      \"last-10-avg\": 2.012173652648926\n    },\n    \"mean_F1\": {\n      \"max\": 0.45787546038627625,\n      \"min\": 0.45787546038627625,\n      \"avg\": 0.45787546038627625,\n      \"last\": 0.45787546038627625,\n      \"last-5-avg\": 0.45787546038627625,\n      \"last-10-avg\": 0.45787546038627625\n    },\n    \"time_this_iter_s\": {\n      \"max\": 942.0430061817169,\n      \"min\": 942.0430061817169,\n      \"avg\": 942.0430061817169,\n      \"last\": 942.0430061817169,\n      \"last-5-avg\": 942.0430061817169,\n      \"last-10-avg\": 942.0430061817169\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625090210,\n      \"min\": 1625090210,\n      \"avg\": 1625090210,\n      \"last\": 1625090210,\n      \"last-5-avg\": 1625090210,\n      \"last-10-avg\": 1625090210\n    },\n    \"time_total_s\": {\n      \"max\": 942.0430061817169,\n      \"min\": 942.0430061817169,\n      \"avg\": 942.0430061817169,\n      \"last\": 942.0430061817169,\n      \"last-5-avg\": 942.0430061817169,\n      \"last-10-avg\": 942.0430061817169\n    },\n    \"pid\": {\n      \"max\": 155850,\n      \"min\": 155850,\n      \"avg\": 155850,\n      \"last\": 155850,\n      \"last-5-avg\": 155850,\n      \"last-10-avg\": 155850\n    },\n    \"time_since_restore\": {\n      \"max\": 942.0430061817169,\n      \"min\": 942.0430061817169,\n      \"avg\": 942.0430061817169,\n      \"last\": 942.0430061817169,\n      \"last-5-avg\": 942.0430061817169,\n      \"last-10-avg\": 942.0430061817169\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0016901729991646017,\n      \"min\": 0.0016901729991646017,\n      \"avg\": 0.0016901729991646017,\n      \"last\": 0.0016901729991646017,\n      \"last-5-avg\": 0.0016901729991646017,\n      \"last-10-avg\": 0.0016901729991646017\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447400018ee80000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447400018ee80000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fdd4dd4e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fdd4dd4e0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d705813a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d705813a00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944aa2e8dc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944aa2e8dc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d705813a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d705813a00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944aca600200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944aca600200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d705813a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d705813a00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f5bb1197062357f612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f5bb1197062357f612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625089264.633389,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00006_6_batch_size=128,learning_rate=0.0016902_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00008\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0007682071391619788,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0007682071391619788,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"8_batch_size=128,learning_rate=0.00076821\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.1874916553497314,\n    \"mean_F1\": 0.49267399311065674,\n    \"time_this_iter_s\": 961.2029156684875,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"a609109fcaec4e6fb6ac427bc3e603d1\",\n    \"date\": \"2021-07-01_00-10-17\",\n    \"timestamp\": 1625091017,\n    \"time_total_s\": 961.2029156684875,\n    \"pid\": 159694,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0007682071391619788,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 961.2029156684875,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00008\",\n    \"experiment_tag\": \"8_batch_size=128,learning_rate=0.00076821\"\n  },\n  \"last_update_time\": 1625091017.919247,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.1874916553497314,\n      \"min\": 2.1874916553497314,\n      \"avg\": 2.1874916553497314,\n      \"last\": 2.1874916553497314,\n      \"last-5-avg\": 2.1874916553497314,\n      \"last-10-avg\": 2.1874916553497314\n    },\n    \"mean_F1\": {\n      \"max\": 0.49267399311065674,\n      \"min\": 0.49267399311065674,\n      \"avg\": 0.49267399311065674,\n      \"last\": 0.49267399311065674,\n      \"last-5-avg\": 0.49267399311065674,\n      \"last-10-avg\": 0.49267399311065674\n    },\n    \"time_this_iter_s\": {\n      \"max\": 961.2029156684875,\n      \"min\": 961.2029156684875,\n      \"avg\": 961.2029156684875,\n      \"last\": 961.2029156684875,\n      \"last-5-avg\": 961.2029156684875,\n      \"last-10-avg\": 961.2029156684875\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625091017,\n      \"min\": 1625091017,\n      \"avg\": 1625091017,\n      \"last\": 1625091017,\n      \"last-5-avg\": 1625091017,\n      \"last-10-avg\": 1625091017\n    },\n    \"time_total_s\": {\n      \"max\": 961.2029156684875,\n      \"min\": 961.2029156684875,\n      \"avg\": 961.2029156684875,\n      \"last\": 961.2029156684875,\n      \"last-5-avg\": 961.2029156684875,\n      \"last-10-avg\": 961.2029156684875\n    },\n    \"pid\": {\n      \"max\": 159694,\n      \"min\": 159694,\n      \"avg\": 159694,\n      \"last\": 159694,\n      \"last-5-avg\": 159694,\n      \"last-10-avg\": 159694\n    },\n    \"time_since_restore\": {\n      \"max\": 961.2029156684875,\n      \"min\": 961.2029156684875,\n      \"avg\": 961.2029156684875,\n      \"last\": 961.2029156684875,\n      \"last-5-avg\": 961.2029156684875,\n      \"last-10-avg\": 961.2029156684875\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0007682071391619788,\n      \"min\": 0.0007682071391619788,\n      \"avg\": 0.0007682071391619788,\n      \"last\": 0.0007682071391619788,\n      \"last-5-avg\": 0.0007682071391619788,\n      \"last-10-avg\": 0.0007682071391619788\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740017ffba0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740017ffba0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fdf87f880000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fdf87f880000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e099f92400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e099f92400000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ac9ebdc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ac9ebdc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e099f92400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e099f92400000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ace6f0200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ace6f0200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e099f92400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e099f92400000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f492c30450649f1612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f492c30450649f1612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625090041.2549136,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00008_8_batch_size=128,learning_rate=0.00076821_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00000\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.001273599006558383,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.001273599006558383,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"0_batch_size=64,learning_rate=0.0012736\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.6745284795761108,\n    \"mean_F1\": 0.6182773113250732,\n    \"time_this_iter_s\": 816.1350009441376,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 4,\n    \"experiment_id\": \"8dd2c8a068ee4ba58a2910f4bd257ab1\",\n    \"date\": \"2021-07-01_00-37-00\",\n    \"timestamp\": 1625092620,\n    \"time_total_s\": 3353.3609750270844,\n    \"pid\": 155848,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.001273599006558383,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 3353.3609750270844,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 4,\n    \"trial_id\": \"deb7c_00000\",\n    \"experiment_tag\": \"0_batch_size=64,learning_rate=0.0012736\"\n  },\n  \"last_update_time\": 1625092620.7347476,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.913841724395752,\n      \"min\": 1.6745284795761108,\n      \"avg\": 1.7734070420265198,\n      \"last\": 1.6745284795761108,\n      \"last-5-avg\": 1.7734070420265198,\n      \"last-10-avg\": 1.7734070420265198\n    },\n    \"mean_F1\": {\n      \"max\": 0.6182773113250732,\n      \"min\": 0.568740963935852,\n      \"avg\": 0.5974223166704178,\n      \"last\": 0.6182773113250732,\n      \"last-5-avg\": 0.5974223166704178,\n      \"last-10-avg\": 0.5974223166704178\n    },\n    \"time_this_iter_s\": {\n      \"max\": 873.9036822319031,\n      \"min\": 816.1350009441376,\n      \"avg\": 838.3402437567711,\n      \"last\": 816.1350009441376,\n      \"last-5-avg\": 838.3402437567711,\n      \"last-10-avg\": 838.3402437567711\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.25,\n      \"last\": true,\n      \"last-5-avg\": 0.25,\n      \"last-10-avg\": 0.25\n    },\n    \"training_iteration\": {\n      \"max\": 4,\n      \"min\": 1,\n      \"avg\": 2.5,\n      \"last\": 4,\n      \"last-5-avg\": 2.5,\n      \"last-10-avg\": 2.5\n    },\n    \"timestamp\": {\n      \"max\": 1625092620,\n      \"min\": 1625090141,\n      \"avg\": 1625091387.25,\n      \"last\": 1625092620,\n      \"last-5-avg\": 1625091387.25,\n      \"last-10-avg\": 1625091387.25\n    },\n    \"time_total_s\": {\n      \"max\": 3353.3609750270844,\n      \"min\": 873.9036822319031,\n      \"avg\": 2120.44291472435,\n      \"last\": 3353.3609750270844,\n      \"last-5-avg\": 2120.44291472435,\n      \"last-10-avg\": 2120.44291472435\n    },\n    \"pid\": {\n      \"max\": 155848,\n      \"min\": 155848,\n      \"avg\": 155848.0,\n      \"last\": 155848,\n      \"last-5-avg\": 155848.0,\n      \"last-10-avg\": 155848.0\n    },\n    \"time_since_restore\": {\n      \"max\": 3353.3609750270844,\n      \"min\": 873.9036822319031,\n      \"avg\": 2120.44291472435,\n      \"last\": 3353.3609750270844,\n      \"last-5-avg\": 2120.44291472435,\n      \"last-10-avg\": 2120.44291472435\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 4,\n      \"min\": 1,\n      \"avg\": 2.5,\n      \"last\": 4,\n      \"last-5-avg\": 2.5,\n      \"last-10-avg\": 2.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.001273599006558383,\n      \"min\": 0.001273599006558383,\n      \"avg\": 0.001273599006558383,\n      \"last\": 0.001273599006558383,\n      \"last-5-avg\": 0.001273599006558383,\n      \"last-10-avg\": 0.001273599006558383\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64.0,\n      \"last\": 64,\n      \"last-5-avg\": 64.0,\n      \"last-10-avg\": 64.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffe9f1880000000473ffc94c400000000473ffb80c560000000473ffacade60000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffe9f1880000000473ffc94c400000000473ffb80c560000000473ffacade60000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe2332040000000473fe2ff9780000000473fe37cb060000000473fe3c8ed80000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe2332040000000473fe2ff9780000000473fe37cb060000000473fe3c8ed80000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b4f3abdc0000047408a5b04cda000004740899f8f4020000047408981147b600000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b4f3abdc0000047408a5b04cda000004740899f8f4020000047408981147b600000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942889898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b024b034b04652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b04652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a5de8dc604aa8ebdc604adceedc604a0cf2dc60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a5de8dc604aa8ebdc604adceedc604a0cf2dc60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b4f3abdc0000047409ad51fc5b000004740a3d273b2e000004740aa32b8d1b80000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b4f3abdc0000047409ad51fc5b000004740a3d273b2e000004740aa32b8d1b80000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ac86002004ac86002004ac86002004ac8600200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ac86002004ac86002004ac86002004ac8600200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b4f3abdc0000047409ad51fc5b000004740a3d273b2e000004740aa32b8d1b80000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b4f3abdc0000047409ad51fc5b000004740a3d273b2e000004740aa32b8d1b80000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b024b034b04652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b04652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f54dddc85356f12473f54dddc85356f12473f54dddc85356f12473f54dddc85356f12652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f54dddc85356f12473f54dddc85356f12473f54dddc85356f12473f54dddc85356f12652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b404b404b404b40652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b404b404b404b40652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952e000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952e000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625089264.5186515,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00000_0_batch_size=64,learning_rate=0.0012736_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00010\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0014577507986803457,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0014577507986803457,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"10_batch_size=16,learning_rate=0.0014578\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.9420467615127563,\n    \"mean_F1\": 0.5712435245513916,\n    \"time_this_iter_s\": 751.4156069755554,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"641bddc20fcc4a129ba70143da13313e\",\n    \"date\": \"2021-07-01_00-21-32\",\n    \"timestamp\": 1625091692,\n    \"time_total_s\": 1541.2172157764435,\n    \"pid\": 160324,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0014577507986803457,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1541.2172157764435,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00010\",\n    \"experiment_tag\": \"10_batch_size=16,learning_rate=0.0014578\"\n  },\n  \"last_update_time\": 1625091692.7550807,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9687937498092651,\n      \"min\": 1.9420467615127563,\n      \"avg\": 1.9554202556610107,\n      \"last\": 1.9420467615127563,\n      \"last-5-avg\": 1.9554202556610107,\n      \"last-10-avg\": 1.9554202556610107\n    },\n    \"mean_F1\": {\n      \"max\": 0.5712435245513916,\n      \"min\": 0.5428571701049805,\n      \"avg\": 0.557050347328186,\n      \"last\": 0.5712435245513916,\n      \"last-5-avg\": 0.557050347328186,\n      \"last-10-avg\": 0.557050347328186\n    },\n    \"time_this_iter_s\": {\n      \"max\": 789.8016088008881,\n      \"min\": 751.4156069755554,\n      \"avg\": 770.6086078882217,\n      \"last\": 751.4156069755554,\n      \"last-5-avg\": 770.6086078882217,\n      \"last-10-avg\": 770.6086078882217\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625091692,\n      \"min\": 1625090941,\n      \"avg\": 1625091316.5,\n      \"last\": 1625091692,\n      \"last-5-avg\": 1625091316.5,\n      \"last-10-avg\": 1625091316.5\n    },\n    \"time_total_s\": {\n      \"max\": 1541.2172157764435,\n      \"min\": 789.8016088008881,\n      \"avg\": 1165.5094122886658,\n      \"last\": 1541.2172157764435,\n      \"last-5-avg\": 1165.5094122886658,\n      \"last-10-avg\": 1165.5094122886658\n    },\n    \"pid\": {\n      \"max\": 160324,\n      \"min\": 160324,\n      \"avg\": 160324.0,\n      \"last\": 160324,\n      \"last-5-avg\": 160324.0,\n      \"last-10-avg\": 160324.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1541.2172157764435,\n      \"min\": 789.8016088008881,\n      \"avg\": 1165.5094122886658,\n      \"last\": 1541.2172157764435,\n      \"last-5-avg\": 1165.5094122886658,\n      \"last-10-avg\": 1165.5094122886658\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0014577507986803457,\n      \"min\": 0.0014577507986803457,\n      \"avg\": 0.0014577507986803457,\n      \"last\": 0.0014577507986803457,\n      \"last-5-avg\": 0.0014577507986803457,\n      \"last-10-avg\": 0.0014577507986803457\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16.0,\n      \"last\": 16,\n      \"last-5-avg\": 16.0,\n      \"last-10-avg\": 16.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fff802de0000000473fff129fa0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fff802de0000000473fff129fa0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe15f1600000000473fe247a080000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe15f1600000000473fe247a080000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474088ae69b1e000004740877b5329c00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088ae69b1e000004740877b5329c00000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a7debdc604a6ceedc60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a7debdc604a6ceedc60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474088ae69b1e0000047409814de6dd00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088ae69b1e0000047409814de6dd00000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a447202004a44720200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a447202004a44720200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474088ae69b1e0000047409814de6dd00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088ae69b1e0000047409814de6dd00000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f57e24000631f64473f57e24000631f64652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f57e24000631f64473f57e24000631f64652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b104b10652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625090137.9328218,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00010_10_batch_size=16,learning_rate=0.0014578_2021-06-30_23-54-09\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00002\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 6.1948778566766e-05,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 6.1948778566766e-05,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"2_batch_size=16,learning_rate=6.1949e-05\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.4351353645324707,\n    \"mean_F1\": 0.46554622054100037,\n    \"time_this_iter_s\": 781.9929780960083,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"405aed8952a647488fcfb24b4a2da7b7\",\n    \"date\": \"2021-06-30_23-54-09\",\n    \"timestamp\": 1625090049,\n    \"time_total_s\": 781.9929780960083,\n    \"pid\": 155846,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 6.1948778566766e-05,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 781.9929780960083,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00002\",\n    \"experiment_tag\": \"2_batch_size=16,learning_rate=6.1949e-05\"\n  },\n  \"last_update_time\": 1625090049.6138427,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.4351353645324707,\n      \"min\": 2.4351353645324707,\n      \"avg\": 2.4351353645324707,\n      \"last\": 2.4351353645324707,\n      \"last-5-avg\": 2.4351353645324707,\n      \"last-10-avg\": 2.4351353645324707\n    },\n    \"mean_F1\": {\n      \"max\": 0.46554622054100037,\n      \"min\": 0.46554622054100037,\n      \"avg\": 0.46554622054100037,\n      \"last\": 0.46554622054100037,\n      \"last-5-avg\": 0.46554622054100037,\n      \"last-10-avg\": 0.46554622054100037\n    },\n    \"time_this_iter_s\": {\n      \"max\": 781.9929780960083,\n      \"min\": 781.9929780960083,\n      \"avg\": 781.9929780960083,\n      \"last\": 781.9929780960083,\n      \"last-5-avg\": 781.9929780960083,\n      \"last-10-avg\": 781.9929780960083\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625090049,\n      \"min\": 1625090049,\n      \"avg\": 1625090049,\n      \"last\": 1625090049,\n      \"last-5-avg\": 1625090049,\n      \"last-10-avg\": 1625090049\n    },\n    \"time_total_s\": {\n      \"max\": 781.9929780960083,\n      \"min\": 781.9929780960083,\n      \"avg\": 781.9929780960083,\n      \"last\": 781.9929780960083,\n      \"last-5-avg\": 781.9929780960083,\n      \"last-10-avg\": 781.9929780960083\n    },\n    \"pid\": {\n      \"max\": 155846,\n      \"min\": 155846,\n      \"avg\": 155846,\n      \"last\": 155846,\n      \"last-5-avg\": 155846,\n      \"last-10-avg\": 155846\n    },\n    \"time_since_restore\": {\n      \"max\": 781.9929780960083,\n      \"min\": 781.9929780960083,\n      \"avg\": 781.9929780960083,\n      \"last\": 781.9929780960083,\n      \"last-5-avg\": 781.9929780960083,\n      \"last-10-avg\": 781.9929780960083\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 6.1948778566766e-05,\n      \"min\": 6.1948778566766e-05,\n      \"avg\": 6.1948778566766e-05,\n      \"last\": 6.1948778566766e-05,\n      \"last-5-avg\": 6.1948778566766e-05,\n      \"last-10-avg\": 6.1948778566766e-05\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16,\n      \"last\": 16,\n      \"last-5-avg\": 16,\n      \"last-10-avg\": 16\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740037b2840000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740037b2840000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fddcb8260000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fddcb8260000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740886ff19e800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740886ff19e800000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a01e8dc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a01e8dc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740886ff19e800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740886ff19e800000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ac6600200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ac6600200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740886ff19e800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740886ff19e800000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f103d4fe9715610612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f103d4fe9715610612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b10612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b10612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625089264.5644999,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00002_2_batch_size=16,learning_rate=6.1949e-05_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00012\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.00032222094543897615,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.00032222094543897615,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"12_batch_size=128,learning_rate=0.00032222\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.404020309448242,\n    \"mean_F1\": 0.42307692766189575,\n    \"time_this_iter_s\": 936.238175868988,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"ac4fe37870f343b6bb59d34fc72ef7b9\",\n    \"date\": \"2021-07-01_00-26-08\",\n    \"timestamp\": 1625091968,\n    \"time_total_s\": 936.238175868988,\n    \"pid\": 164233,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.00032222094543897615,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 936.238175868988,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00012\",\n    \"experiment_tag\": \"12_batch_size=128,learning_rate=0.00032222\"\n  },\n  \"last_update_time\": 1625091968.5792644,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.404020309448242,\n      \"min\": 2.404020309448242,\n      \"avg\": 2.404020309448242,\n      \"last\": 2.404020309448242,\n      \"last-5-avg\": 2.404020309448242,\n      \"last-10-avg\": 2.404020309448242\n    },\n    \"mean_F1\": {\n      \"max\": 0.42307692766189575,\n      \"min\": 0.42307692766189575,\n      \"avg\": 0.42307692766189575,\n      \"last\": 0.42307692766189575,\n      \"last-5-avg\": 0.42307692766189575,\n      \"last-10-avg\": 0.42307692766189575\n    },\n    \"time_this_iter_s\": {\n      \"max\": 936.238175868988,\n      \"min\": 936.238175868988,\n      \"avg\": 936.238175868988,\n      \"last\": 936.238175868988,\n      \"last-5-avg\": 936.238175868988,\n      \"last-10-avg\": 936.238175868988\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625091968,\n      \"min\": 1625091968,\n      \"avg\": 1625091968,\n      \"last\": 1625091968,\n      \"last-5-avg\": 1625091968,\n      \"last-10-avg\": 1625091968\n    },\n    \"time_total_s\": {\n      \"max\": 936.238175868988,\n      \"min\": 936.238175868988,\n      \"avg\": 936.238175868988,\n      \"last\": 936.238175868988,\n      \"last-5-avg\": 936.238175868988,\n      \"last-10-avg\": 936.238175868988\n    },\n    \"pid\": {\n      \"max\": 164233,\n      \"min\": 164233,\n      \"avg\": 164233,\n      \"last\": 164233,\n      \"last-5-avg\": 164233,\n      \"last-10-avg\": 164233\n    },\n    \"time_since_restore\": {\n      \"max\": 936.238175868988,\n      \"min\": 936.238175868988,\n      \"avg\": 936.238175868988,\n      \"last\": 936.238175868988,\n      \"last-5-avg\": 936.238175868988,\n      \"last-10-avg\": 936.238175868988\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.00032222094543897615,\n      \"min\": 0.00032222094543897615,\n      \"avg\": 0.00032222094543897615,\n      \"last\": 0.00032222094543897615,\n      \"last-5-avg\": 0.00032222094543897615,\n      \"last-10-avg\": 0.00032222094543897615\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740033b6f00000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740033b6f00000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fdb13b140000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fdb13b140000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d41e7c8c00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d41e7c8c00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a80efdc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a80efdc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d41e7c8c00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d41e7c8c00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a89810200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a89810200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d41e7c8c00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d41e7c8c00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f351df86c391f12612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f351df86c391f12612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625091018.143984,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00012_12_batch_size=128,learning_rate=0.00032222_2021-06-30_23-56-50\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00013\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.000803357199135051,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.000803357199135051,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"13_batch_size=64,learning_rate=0.00080336\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.0132715702056885,\n    \"mean_F1\": 0.5296671390533447,\n    \"time_this_iter_s\": 859.9410557746887,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"b2e2013850e1480c961368b001b24695\",\n    \"date\": \"2021-07-01_00-36-07\",\n    \"timestamp\": 1625092567,\n    \"time_total_s\": 859.9410557746887,\n    \"pid\": 167046,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.000803357199135051,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 859.9410557746887,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00013\",\n    \"experiment_tag\": \"13_batch_size=64,learning_rate=0.00080336\"\n  },\n  \"last_update_time\": 1625092567.53834,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.0132715702056885,\n      \"min\": 2.0132715702056885,\n      \"avg\": 2.0132715702056885,\n      \"last\": 2.0132715702056885,\n      \"last-5-avg\": 2.0132715702056885,\n      \"last-10-avg\": 2.0132715702056885\n    },\n    \"mean_F1\": {\n      \"max\": 0.5296671390533447,\n      \"min\": 0.5296671390533447,\n      \"avg\": 0.5296671390533447,\n      \"last\": 0.5296671390533447,\n      \"last-5-avg\": 0.5296671390533447,\n      \"last-10-avg\": 0.5296671390533447\n    },\n    \"time_this_iter_s\": {\n      \"max\": 859.9410557746887,\n      \"min\": 859.9410557746887,\n      \"avg\": 859.9410557746887,\n      \"last\": 859.9410557746887,\n      \"last-5-avg\": 859.9410557746887,\n      \"last-10-avg\": 859.9410557746887\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625092567,\n      \"min\": 1625092567,\n      \"avg\": 1625092567,\n      \"last\": 1625092567,\n      \"last-5-avg\": 1625092567,\n      \"last-10-avg\": 1625092567\n    },\n    \"time_total_s\": {\n      \"max\": 859.9410557746887,\n      \"min\": 859.9410557746887,\n      \"avg\": 859.9410557746887,\n      \"last\": 859.9410557746887,\n      \"last-5-avg\": 859.9410557746887,\n      \"last-10-avg\": 859.9410557746887\n    },\n    \"pid\": {\n      \"max\": 167046,\n      \"min\": 167046,\n      \"avg\": 167046,\n      \"last\": 167046,\n      \"last-5-avg\": 167046,\n      \"last-10-avg\": 167046\n    },\n    \"time_since_restore\": {\n      \"max\": 859.9410557746887,\n      \"min\": 859.9410557746887,\n      \"avg\": 859.9410557746887,\n      \"last\": 859.9410557746887,\n      \"last-5-avg\": 859.9410557746887,\n      \"last-10-avg\": 859.9410557746887\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.000803357199135051,\n      \"min\": 0.000803357199135051,\n      \"avg\": 0.000803357199135051,\n      \"last\": 0.000803357199135051,\n      \"last-5-avg\": 0.000803357199135051,\n      \"last-10-avg\": 0.000803357199135051\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64,\n      \"last\": 64,\n      \"last-5-avg\": 64,\n      \"last-10-avg\": 64\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740001b2e20000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740001b2e20000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe0f30880000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe0f30880000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408adf8748400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408adf8748400000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ad7f1dc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ad7f1dc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408adf8748400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408adf8748400000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a868c0200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a868c0200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408adf8748400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408adf8748400000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f4a530c72da702b612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f4a530c72da702b612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b40612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b40612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625091692.9981165,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00013_13_batch_size=64,learning_rate=0.00080336_2021-07-01_00-10-18\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00014\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0005418753581837613,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0005418753581837613,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"14_batch_size=32,learning_rate=0.00054188\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.8086698055267334,\n    \"mean_F1\": 0.5882352590560913,\n    \"time_this_iter_s\": 776.6806032657623,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"d77715c6db71498e9132ad92cd90638f\",\n    \"date\": \"2021-07-01_00-52-44\",\n    \"timestamp\": 1625093564,\n    \"time_total_s\": 1581.644783258438,\n    \"pid\": 168273,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0005418753581837613,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1581.644783258438,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00014\",\n    \"experiment_tag\": \"14_batch_size=32,learning_rate=0.00054188\"\n  },\n  \"last_update_time\": 1625093564.9995518,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9615306854248047,\n      \"min\": 1.8086698055267334,\n      \"avg\": 1.885100245475769,\n      \"last\": 1.8086698055267334,\n      \"last-5-avg\": 1.885100245475769,\n      \"last-10-avg\": 1.885100245475769\n    },\n    \"mean_F1\": {\n      \"max\": 0.5882352590560913,\n      \"min\": 0.5606060028076172,\n      \"avg\": 0.5744206309318542,\n      \"last\": 0.5882352590560913,\n      \"last-5-avg\": 0.5744206309318542,\n      \"last-10-avg\": 0.5744206309318542\n    },\n    \"time_this_iter_s\": {\n      \"max\": 804.9641799926758,\n      \"min\": 776.6806032657623,\n      \"avg\": 790.822391629219,\n      \"last\": 776.6806032657623,\n      \"last-5-avg\": 790.822391629219,\n      \"last-10-avg\": 790.822391629219\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625093564,\n      \"min\": 1625092788,\n      \"avg\": 1625093176.0,\n      \"last\": 1625093564,\n      \"last-5-avg\": 1625093176.0,\n      \"last-10-avg\": 1625093176.0\n    },\n    \"time_total_s\": {\n      \"max\": 1581.644783258438,\n      \"min\": 804.9641799926758,\n      \"avg\": 1193.304481625557,\n      \"last\": 1581.644783258438,\n      \"last-5-avg\": 1193.304481625557,\n      \"last-10-avg\": 1193.304481625557\n    },\n    \"pid\": {\n      \"max\": 168273,\n      \"min\": 168273,\n      \"avg\": 168273.0,\n      \"last\": 168273,\n      \"last-5-avg\": 168273.0,\n      \"last-10-avg\": 168273.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1581.644783258438,\n      \"min\": 804.9641799926758,\n      \"avg\": 1193.304481625557,\n      \"last\": 1581.644783258438,\n      \"last-5-avg\": 1193.304481625557,\n      \"last-10-avg\": 1193.304481625557\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0005418753581837613,\n      \"min\": 0.0005418753581837613,\n      \"avg\": 0.0005418753581837613,\n      \"last\": 0.0005418753581837613,\n      \"last-5-avg\": 0.0005418753581837613,\n      \"last-10-avg\": 0.0005418753581837613\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fff626e00000000473ffcf04fc0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fff626e00000000473ffcf04fc0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe1f07c00000000473fe2d2d2c0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe1f07c00000000473fe2d2d2c0000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408927b6a40000004740884571e0200000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408927b6a40000004740884571e0200000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ab4f2dc604abcf5dc60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ab4f2dc604abcf5dc60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408927b6a4000000474098b69442100000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408927b6a4000000474098b69442100000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a519102004a51910200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a519102004a51910200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408927b6a4000000474098b69442100000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408927b6a4000000474098b69442100000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f41c19478906d22473f41c19478906d22652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f41c19478906d22473f41c19478906d22652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625091968.7375154,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00014_14_batch_size=32,learning_rate=0.00054188_2021-07-01_00-21-33\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00003\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0008906708082239961,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0008906708082239961,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"3_batch_size=32,learning_rate=0.00089067\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.6001352071762085,\n    \"mean_F1\": 0.6430604457855225,\n    \"time_this_iter_s\": 779.5617206096649,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 8,\n    \"experiment_id\": \"402f8966d32e46b1967e9b191f24fc0a\",\n    \"date\": \"2021-07-01_01-25-20\",\n    \"timestamp\": 1625095520,\n    \"time_total_s\": 6252.794419527054,\n    \"pid\": 155843,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0008906708082239961,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 6252.794419527054,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 8,\n    \"trial_id\": \"deb7c_00003\",\n    \"experiment_tag\": \"3_batch_size=32,learning_rate=0.00089067\"\n  },\n  \"last_update_time\": 1625095520.8157835,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9329307079315186,\n      \"min\": 1.6001352071762085,\n      \"avg\": 1.6911626160144806,\n      \"last\": 1.6001352071762085,\n      \"last-5-avg\": 1.6300825834274293,\n      \"last-10-avg\": 1.6911626160144806\n    },\n    \"mean_F1\": {\n      \"max\": 0.6430604457855225,\n      \"min\": 0.5749601125717163,\n      \"avg\": 0.6208823621273041,\n      \"last\": 0.6430604457855225,\n      \"last-5-avg\": 0.6353103756904602,\n      \"last-10-avg\": 0.6208823621273041\n    },\n    \"time_this_iter_s\": {\n      \"max\": 809.1942768096924,\n      \"min\": 767.1961803436279,\n      \"avg\": 781.5993024408816,\n      \"last\": 779.5617206096649,\n      \"last-5-avg\": 777.3714671134949,\n      \"last-10-avg\": 781.5993024408817\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.125,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.125\n    },\n    \"training_iteration\": {\n      \"max\": 8,\n      \"min\": 1,\n      \"avg\": 4.5,\n      \"last\": 8,\n      \"last-5-avg\": 6.0,\n      \"last-10-avg\": 4.5\n    },\n    \"timestamp\": {\n      \"max\": 1625095520,\n      \"min\": 1625090077,\n      \"avg\": 1625092798.0,\n      \"last\": 1625095520,\n      \"last-5-avg\": 1625093961.6,\n      \"last-10-avg\": 1625092798.0\n    },\n    \"time_total_s\": {\n      \"max\": 6252.794419527054,\n      \"min\": 809.1942768096924,\n      \"avg\": 3530.6116644740105,\n      \"last\": 6252.794419527054,\n      \"last-5-avg\": 4694.204210281372,\n      \"last-10-avg\": 3530.6116644740105\n    },\n    \"pid\": {\n      \"max\": 155843,\n      \"min\": 155843,\n      \"avg\": 155843.0,\n      \"last\": 155843,\n      \"last-5-avg\": 155843.0,\n      \"last-10-avg\": 155843.0\n    },\n    \"time_since_restore\": {\n      \"max\": 6252.794419527054,\n      \"min\": 809.1942768096924,\n      \"avg\": 3530.6116644740105,\n      \"last\": 6252.794419527054,\n      \"last-5-avg\": 4694.204210281372,\n      \"last-10-avg\": 3530.6116644740105\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 8,\n      \"min\": 1,\n      \"avg\": 4.5,\n      \"last\": 8,\n      \"last-5-avg\": 6.0,\n      \"last-10-avg\": 4.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0008906708082239961,\n      \"min\": 0.0008906708082239961,\n      \"avg\": 0.0008906708082239961,\n      \"last\": 0.0008906708082239961,\n      \"last-5-avg\": 0.0008906708082239961,\n      \"last-10-avg\": 0.0008906708082239961\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffa5d9f20000000473ffa2ae640000000473ffa99c0a0000000473ff9abaa00000000473ff99a2760000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffeed48c0000000473ffc43aee0000000473ffadef540000000473ffa5d9f20000000473ffa2ae640000000473ffa99c0a0000000473ff9abaa00000000473ff99a2760000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe40feac0000000473fe4477dc0000000473fe44cc160000000473fe46e32c0000000473fe493f380000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe26612c0000000473fe3256e80000000473fe3c05420000000473fe40feac0000000473fe4477dc0000000473fe44cc160000000473fe46e32c0000000473fe493f380000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740883ee2bca00000474088201ae46000004740883c9822e000004740887ec7a78000004740885c7e67600000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089498de1000000474088ac5f7de00000474087f991c70000004740883ee2bca00000474088201ae46000004740883c9822e000004740887ec7a78000004740885c7e67600000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b044b054b064b074b08652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b044b054b064b074b08652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a39f1dc604a3df4dc604a45f7dc604a55fadc604a60fddc60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a1de8dc604a32ebdc604a31eedc604a39f1dc604a3df4dc604a45f7dc604a55fadc604a60fddc60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740a88b9878a000004740ae939f31b800004740b251629d3800004740b5613b922800004740b86ccb5f140000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089498de1000000474098faf6af7000004740a27bdfc97800004740a88b9878a000004740ae939f31b800004740b251629d3800004740b5613b922800004740b86ccb5f140000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ac36002004ac36002004ac36002004ac36002004ac3600200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ac36002004ac36002004ac36002004ac36002004ac36002004ac36002004ac36002004ac3600200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740a88b9878a000004740ae939f31b800004740b251629d3800004740b5613b922800004740b86ccb5f140000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089498de1000000474098faf6af7000004740a27bdfc97800004740a88b9878a000004740ae939f31b800004740b251629d3800004740b5613b922800004740b86ccb5f140000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b044b054b064b074b08652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b044b054b064b074b08652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0473f4d2f7cff14daf0652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b204b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625089264.5768783,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00003_3_batch_size=32,learning_rate=0.00089067_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00005\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.00154387601791768,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.00154387601791768,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"5_batch_size=16,learning_rate=0.0015439\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.9955512285232544,\n    \"mean_F1\": 0.5277311205863953,\n    \"time_this_iter_s\": 773.0863978862762,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"9ac48e9304d34d608a9cdcaa864b3a98\",\n    \"date\": \"2021-06-30_23-54-00\",\n    \"timestamp\": 1625090040,\n    \"time_total_s\": 773.0863978862762,\n    \"pid\": 155847,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.00154387601791768,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 773.0863978862762,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00005\",\n    \"experiment_tag\": \"5_batch_size=16,learning_rate=0.0015439\"\n  },\n  \"last_update_time\": 1625090041.2165632,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9955512285232544,\n      \"min\": 1.9955512285232544,\n      \"avg\": 1.9955512285232544,\n      \"last\": 1.9955512285232544,\n      \"last-5-avg\": 1.9955512285232544,\n      \"last-10-avg\": 1.9955512285232544\n    },\n    \"mean_F1\": {\n      \"max\": 0.5277311205863953,\n      \"min\": 0.5277311205863953,\n      \"avg\": 0.5277311205863953,\n      \"last\": 0.5277311205863953,\n      \"last-5-avg\": 0.5277311205863953,\n      \"last-10-avg\": 0.5277311205863953\n    },\n    \"time_this_iter_s\": {\n      \"max\": 773.0863978862762,\n      \"min\": 773.0863978862762,\n      \"avg\": 773.0863978862762,\n      \"last\": 773.0863978862762,\n      \"last-5-avg\": 773.0863978862762,\n      \"last-10-avg\": 773.0863978862762\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625090040,\n      \"min\": 1625090040,\n      \"avg\": 1625090040,\n      \"last\": 1625090040,\n      \"last-5-avg\": 1625090040,\n      \"last-10-avg\": 1625090040\n    },\n    \"time_total_s\": {\n      \"max\": 773.0863978862762,\n      \"min\": 773.0863978862762,\n      \"avg\": 773.0863978862762,\n      \"last\": 773.0863978862762,\n      \"last-5-avg\": 773.0863978862762,\n      \"last-10-avg\": 773.0863978862762\n    },\n    \"pid\": {\n      \"max\": 155847,\n      \"min\": 155847,\n      \"avg\": 155847,\n      \"last\": 155847,\n      \"last-5-avg\": 155847,\n      \"last-10-avg\": 155847\n    },\n    \"time_since_restore\": {\n      \"max\": 773.0863978862762,\n      \"min\": 773.0863978862762,\n      \"avg\": 773.0863978862762,\n      \"last\": 773.0863978862762,\n      \"last-5-avg\": 773.0863978862762,\n      \"last-10-avg\": 773.0863978862762\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.00154387601791768,\n      \"min\": 0.00154387601791768,\n      \"avg\": 0.00154387601791768,\n      \"last\": 0.00154387601791768,\n      \"last-5-avg\": 0.00154387601791768,\n      \"last-10-avg\": 0.00154387601791768\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16,\n      \"last\": 16,\n      \"last-5-avg\": 16,\n      \"last-10-avg\": 16\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fffedc720000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fffedc720000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe0e32c60000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe0e32c60000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408828b0f1600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408828b0f1600000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944af8e7dc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944af8e7dc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408828b0f1600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408828b0f1600000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ac7600200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ac7600200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408828b0f1600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408828b0f1600000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f594b7c4062e13c612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f594b7c4062e13c612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b10612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b10612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625089264.610514,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00005_5_batch_size=16,learning_rate=0.0015439_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00011\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.002374572009300586,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.002374572009300586,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"11_batch_size=32,learning_rate=0.0023746\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.5956366062164307,\n    \"mean_F1\": 0.6604114770889282,\n    \"time_this_iter_s\": 792.673168182373,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 20,\n    \"experiment_id\": \"b362707af16046c0986c1cedf0d68163\",\n    \"date\": \"2021-07-01_04-17-24\",\n    \"timestamp\": 1625105844,\n    \"time_total_s\": 15621.06105875969,\n    \"pid\": 160792,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.002374572009300586,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 15621.06105875969,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 20,\n    \"trial_id\": \"deb7c_00011\",\n    \"experiment_tag\": \"11_batch_size=32,learning_rate=0.0023746\"\n  },\n  \"last_update_time\": 1625105844.8524525,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9569967985153198,\n      \"min\": 1.5442039966583252,\n      \"avg\": 1.6372535049915315,\n      \"last\": 1.5956366062164307,\n      \"last-5-avg\": 1.5898672342300415,\n      \"last-10-avg\": 1.5941033244132996\n    },\n    \"mean_F1\": {\n      \"max\": 0.6604114770889282,\n      \"min\": 0.5255182981491089,\n      \"avg\": 0.6320414513349534,\n      \"last\": 0.6604114770889282,\n      \"last-5-avg\": 0.657861340045929,\n      \"last-10-avg\": 0.6533413231372833\n    },\n    \"time_this_iter_s\": {\n      \"max\": 813.6951410770416,\n      \"min\": 762.7919232845306,\n      \"avg\": 781.0530529379844,\n      \"last\": 792.673168182373,\n      \"last-5-avg\": 784.7267407894135,\n      \"last-10-avg\": 782.7729466199875\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.05,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.1\n    },\n    \"training_iteration\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"timestamp\": {\n      \"max\": 1625105844,\n      \"min\": 1625091037,\n      \"avg\": 1625098413.8,\n      \"last\": 1625105844,\n      \"last-5-avg\": 1625104272.8,\n      \"last-10-avg\": 1625102312.5\n    },\n    \"time_total_s\": {\n      \"max\": 15621.06105875969,\n      \"min\": 813.6951410770416,\n      \"avg\": 8190.470753896237,\n      \"last\": 15621.06105875969,\n      \"last-5-avg\": 14049.674268770217,\n      \"last-10-avg\": 12089.183855509758\n    },\n    \"pid\": {\n      \"max\": 160792,\n      \"min\": 160792,\n      \"avg\": 160792.0,\n      \"last\": 160792,\n      \"last-5-avg\": 160792.0,\n      \"last-10-avg\": 160792.0\n    },\n    \"time_since_restore\": {\n      \"max\": 15621.06105875969,\n      \"min\": 813.6951410770416,\n      \"avg\": 8190.470753896237,\n      \"last\": 15621.06105875969,\n      \"last-5-avg\": 14049.674268770217,\n      \"last-10-avg\": 12089.183855509758\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.002374572009300586,\n      \"min\": 0.002374572009300586,\n      \"avg\": 0.0023745720093005865,\n      \"last\": 0.002374572009300586,\n      \"last-5-avg\": 0.002374572009300586,\n      \"last-10-avg\": 0.002374572009300586\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ff8f868c0000000473ffa36c6c0000000473ff9c48220000000473ff8b50f40000000473ff987ba40000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffa39bbc0000000473ff9246e80000000473ff9b72860000000473ff933c2e0000000473ff994e840000000473ff8f868c0000000473ffa36c6c0000000473ff9c48220000000473ff8b50f40000000473ff987ba40000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe4f76c00000000473fe4ff9a00000000473fe50f5000000000473fe51992e0000000473fe5221740000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe4a0dfe0000000473fe4b42d00000000473fe4c3fa80000000473fe4d39ac0000000473fe4e31660000000473fe4f76c00000000473fe4ff9a00000000473fe50f5000000000473fe51992e0000000473fe5221740000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474088c56cd3e000004740883d887700000047408849c7fbc000004740888af1e6c00000474088c562a6000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088402474e0000047408854bd470000004740885276ae60000047408841fe33000000474088d76d81c00000474088c56cd3e000004740883d887700000047408849c7fbc000004740888af1e6c00000474088c562a6000000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a7919dd604a811cdd604a8a1fdd604a9c22dd604ab425dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a290add604a330ddd604a3e10dd604a4613dd604a6116dd604a7919dd604a811cdd604a8a1fdd604a9c22dd604ab425dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c8650d80ce00004740c9e8e6083e00004740cb6d8287fa00004740ccf631a66600004740ce8287d0c60000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c0bcacb8ee00004740c241f88d5e00004740c3c71ff84400004740c54b3fdb7400004740c6d8b6b39000004740c8650d80ce00004740c9e8e6083e00004740cb6d8287fa00004740ccf631a66600004740ce8287d0c60000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a187402004a187402004a187402004a187402004a18740200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a187402004a187402004a187402004a187402004a187402004a187402004a187402004a187402004a187402004a18740200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c8650d80ce00004740c9e8e6083e00004740cb6d8287fa00004740ccf631a66600004740ce8287d0c60000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c0bcacb8ee00004740c241f88d5e00004740c3c71ff84400004740c54b3fdb7400004740c6d8b6b39000004740c8650d80ce00004740c9e8e6083e00004740cb6d8287fa00004740ccf631a66600004740ce8287d0c60000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f473f6373d6a3e6f50f652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b204b204b204b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059540000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625090210.4494286,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00011_11_batch_size=32,learning_rate=0.0023746_2021-06-30_23-55-37\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00009\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0009921976269544916,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0009921976269544916,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"9_batch_size=32,learning_rate=0.0009922\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.5386419296264648,\n    \"mean_F1\": 0.6620452404022217,\n    \"time_this_iter_s\": 783.3783705234528,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 20,\n    \"experiment_id\": \"74b2942a4da043689453e7971fd1157d\",\n    \"date\": \"2021-07-01_04-15-21\",\n    \"timestamp\": 1625105721,\n    \"time_total_s\": 15661.127942323685,\n    \"pid\": 159800,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0009921976269544916,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 15661.127942323685,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 20,\n    \"trial_id\": \"deb7c_00009\",\n    \"experiment_tag\": \"9_batch_size=32,learning_rate=0.0009922\"\n  },\n  \"last_update_time\": 1625105721.3277512,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9095728397369385,\n      \"min\": 1.5386419296264648,\n      \"avg\": 1.6301863968372345,\n      \"last\": 1.5386419296264648,\n      \"last-5-avg\": 1.57311053276062,\n      \"last-10-avg\": 1.5862205862998962\n    },\n    \"mean_F1\": {\n      \"max\": 0.6620452404022217,\n      \"min\": 0.5677831172943115,\n      \"avg\": 0.641308081150055,\n      \"last\": 0.6620452404022217,\n      \"last-5-avg\": 0.65935777425766,\n      \"last-10-avg\": 0.6558059394359589\n    },\n    \"time_this_iter_s\": {\n      \"max\": 820.3321416378021,\n      \"min\": 764.8733494281769,\n      \"avg\": 783.0563971161841,\n      \"last\": 783.3783705234528,\n      \"last-5-avg\": 785.2086092948914,\n      \"last-10-avg\": 784.8327149152756\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.05,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.1\n    },\n    \"training_iteration\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"timestamp\": {\n      \"max\": 1625105721,\n      \"min\": 1625090880,\n      \"avg\": 1625098277.0000002,\n      \"last\": 1625105721,\n      \"last-5-avg\": 1625104156.4,\n      \"last-10-avg\": 1625102189.9\n    },\n    \"time_total_s\": {\n      \"max\": 15661.127942323685,\n      \"min\": 820.3321416378021,\n      \"avg\": 8217.348869192601,\n      \"last\": 15661.127942323685,\n      \"last-5-avg\": 14096.873426961898,\n      \"last-10-avg\": 12130.281869220733\n    },\n    \"pid\": {\n      \"max\": 159800,\n      \"min\": 159800,\n      \"avg\": 159800.0,\n      \"last\": 159800,\n      \"last-5-avg\": 159800.0,\n      \"last-10-avg\": 159800.0\n    },\n    \"time_since_restore\": {\n      \"max\": 15661.127942323685,\n      \"min\": 820.3321416378021,\n      \"avg\": 8217.348869192601,\n      \"last\": 15661.127942323685,\n      \"last-5-avg\": 14096.873426961898,\n      \"last-10-avg\": 12130.281869220733\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0009921976269544916,\n      \"min\": 0.0009921976269544916,\n      \"avg\": 0.0009921976269544918,\n      \"last\": 0.0009921976269544916,\n      \"last-5-avg\": 0.0009921976269544916,\n      \"last-10-avg\": 0.0009921976269544918\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ff8fe9e00000000473ffa498240000000473ff909b7e0000000473ff8e92ea0000000473ff89e4700000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffa2933e0000000473ff9660040000000473ff998cc60000000473ff9095700000000473ff9c0f320000000473ff8fe9e00000000473ffa498240000000473ff909b7e0000000473ff8e92ea0000000473ff89e4700000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe504f760000000473fe50d3480000000473fe51a8f00000000473fe5231700000000473fe52f7980000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe4c4e100000000473fe4d10100000000473fe4e16200000000473fe4ebd500000000473fe4f93b00000000473fe504f760000000473fe50d3480000000473fe51a8f00000000473fe5231700000000473fe52f7980000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474088e1c92ee0000047408894472860000047408853f7474000004740886b49a32000004740887b06e7200000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088bc199ec00000474088426d1ae000004740884c2fd7a00000474088714158600000474088d64dee000000474088e1c92ee0000047408894472860000047408853f7474000004740886b49a32000004740887b06e7200000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284aff18dd604a111cdd604a1c1fdd604a2922dd604a3925dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284aa809dd604ab00cdd604aba0fdd604ac812dd604ae315dd604aff18dd604a111cdd604a1c1fdd604a2922dd604a3925dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c879a770cc00004740ca02ebe35200004740cb882b57c600004740cd0edff1f800004740ce9690606a0000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c0ce281a5000004740c2524eebfe00004740c3d711e97800004740c55e25fefe00004740c6eb8addde00004740c879a770cc00004740ca02ebe35200004740cb882b57c600004740cd0edff1f800004740ce9690606a0000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a387002004a387002004a387002004a387002004a38700200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a387002004a387002004a387002004a387002004a387002004a387002004a387002004a387002004a387002004a38700200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c879a770cc00004740ca02ebe35200004740cb882b57c600004740cd0edff1f800004740ce9690606a0000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c0ce281a5000004740c2524eebfe00004740c3d711e97800004740c55e25fefe00004740c6eb8addde00004740c879a770cc00004740ca02ebe35200004740cb882b57c600004740cd0edff1f800004740ce9690606a0000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e473f50419416f8d88e652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b204b204b204b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059540000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625090049.6841023,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00009_9_batch_size=32,learning_rate=0.0009922_2021-06-30_23-54-01\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00001\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0026100710944633567,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0026100710944633567,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"1_batch_size=64,learning_rate=0.0026101\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.976347804069519,\n    \"mean_F1\": 0.5173661708831787,\n    \"time_this_iter_s\": 869.9753589630127,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"e5e475913a004ad4a5616b7c9fea12fb\",\n    \"date\": \"2021-06-30_23-55-37\",\n    \"timestamp\": 1625090137,\n    \"time_total_s\": 869.9753589630127,\n    \"pid\": 155844,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0026100710944633567,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 869.9753589630127,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00001\",\n    \"experiment_tag\": \"1_batch_size=64,learning_rate=0.0026101\"\n  },\n  \"last_update_time\": 1625090137.8895202,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.976347804069519,\n      \"min\": 1.976347804069519,\n      \"avg\": 1.976347804069519,\n      \"last\": 1.976347804069519,\n      \"last-5-avg\": 1.976347804069519,\n      \"last-10-avg\": 1.976347804069519\n    },\n    \"mean_F1\": {\n      \"max\": 0.5173661708831787,\n      \"min\": 0.5173661708831787,\n      \"avg\": 0.5173661708831787,\n      \"last\": 0.5173661708831787,\n      \"last-5-avg\": 0.5173661708831787,\n      \"last-10-avg\": 0.5173661708831787\n    },\n    \"time_this_iter_s\": {\n      \"max\": 869.9753589630127,\n      \"min\": 869.9753589630127,\n      \"avg\": 869.9753589630127,\n      \"last\": 869.9753589630127,\n      \"last-5-avg\": 869.9753589630127,\n      \"last-10-avg\": 869.9753589630127\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625090137,\n      \"min\": 1625090137,\n      \"avg\": 1625090137,\n      \"last\": 1625090137,\n      \"last-5-avg\": 1625090137,\n      \"last-10-avg\": 1625090137\n    },\n    \"time_total_s\": {\n      \"max\": 869.9753589630127,\n      \"min\": 869.9753589630127,\n      \"avg\": 869.9753589630127,\n      \"last\": 869.9753589630127,\n      \"last-5-avg\": 869.9753589630127,\n      \"last-10-avg\": 869.9753589630127\n    },\n    \"pid\": {\n      \"max\": 155844,\n      \"min\": 155844,\n      \"avg\": 155844,\n      \"last\": 155844,\n      \"last-5-avg\": 155844,\n      \"last-10-avg\": 155844\n    },\n    \"time_since_restore\": {\n      \"max\": 869.9753589630127,\n      \"min\": 869.9753589630127,\n      \"avg\": 869.9753589630127,\n      \"last\": 869.9753589630127,\n      \"last-5-avg\": 869.9753589630127,\n      \"last-10-avg\": 869.9753589630127\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0026100710944633567,\n      \"min\": 0.0026100710944633567,\n      \"avg\": 0.0026100710944633567,\n      \"last\": 0.0026100710944633567,\n      \"last-5-avg\": 0.0026100710944633567,\n      \"last-10-avg\": 0.0026100710944633567\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64,\n      \"last\": 64,\n      \"last-5-avg\": 64,\n      \"last-10-avg\": 64\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fff9f1ee0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fff9f1ee0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe08e4380000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe08e4380000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b2fcd89000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b2fcd89000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a59e8dc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a59e8dc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b2fcd89000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b2fcd89000000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ac4600200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ac4600200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b2fcd89000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b2fcd89000000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f6561b73fb5e75a612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f6561b73fb5e75a612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b40612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b40612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625089264.5536325,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00001_1_batch_size=64,learning_rate=0.0026101_2021-06-30_23-41-04\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00018\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0007080479506499317,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0007080479506499317,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"18_batch_size=64,learning_rate=0.00070805\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.0525383949279785,\n    \"mean_F1\": 0.5441389083862305,\n    \"time_this_iter_s\": 880.6788372993469,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"5264e20c1130438abd592621e720c647\",\n    \"date\": \"2021-07-01_01-07-36\",\n    \"timestamp\": 1625094456,\n    \"time_total_s\": 880.6788372993469,\n    \"pid\": 175090,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0007080479506499317,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 880.6788372993469,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00018\",\n    \"experiment_tag\": \"18_batch_size=64,learning_rate=0.00070805\"\n  },\n  \"last_update_time\": 1625094456.6052508,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.0525383949279785,\n      \"min\": 2.0525383949279785,\n      \"avg\": 2.0525383949279785,\n      \"last\": 2.0525383949279785,\n      \"last-5-avg\": 2.0525383949279785,\n      \"last-10-avg\": 2.0525383949279785\n    },\n    \"mean_F1\": {\n      \"max\": 0.5441389083862305,\n      \"min\": 0.5441389083862305,\n      \"avg\": 0.5441389083862305,\n      \"last\": 0.5441389083862305,\n      \"last-5-avg\": 0.5441389083862305,\n      \"last-10-avg\": 0.5441389083862305\n    },\n    \"time_this_iter_s\": {\n      \"max\": 880.6788372993469,\n      \"min\": 880.6788372993469,\n      \"avg\": 880.6788372993469,\n      \"last\": 880.6788372993469,\n      \"last-5-avg\": 880.6788372993469,\n      \"last-10-avg\": 880.6788372993469\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625094456,\n      \"min\": 1625094456,\n      \"avg\": 1625094456,\n      \"last\": 1625094456,\n      \"last-5-avg\": 1625094456,\n      \"last-10-avg\": 1625094456\n    },\n    \"time_total_s\": {\n      \"max\": 880.6788372993469,\n      \"min\": 880.6788372993469,\n      \"avg\": 880.6788372993469,\n      \"last\": 880.6788372993469,\n      \"last-5-avg\": 880.6788372993469,\n      \"last-10-avg\": 880.6788372993469\n    },\n    \"pid\": {\n      \"max\": 175090,\n      \"min\": 175090,\n      \"avg\": 175090,\n      \"last\": 175090,\n      \"last-5-avg\": 175090,\n      \"last-10-avg\": 175090\n    },\n    \"time_since_restore\": {\n      \"max\": 880.6788372993469,\n      \"min\": 880.6788372993469,\n      \"avg\": 880.6788372993469,\n      \"last\": 880.6788372993469,\n      \"last-5-avg\": 880.6788372993469,\n      \"last-10-avg\": 880.6788372993469\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0007080479506499317,\n      \"min\": 0.0007080479506499317,\n      \"avg\": 0.0007080479506499317,\n      \"last\": 0.0007080479506499317,\n      \"last-5-avg\": 0.0007080479506499317,\n      \"last-10-avg\": 0.0007080479506499317\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64,\n      \"last\": 64,\n      \"last-5-avg\": 64,\n      \"last-10-avg\": 64\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740006b9940000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740006b9940000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe1699600000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe1699600000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b856e42400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b856e42400000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a38f9dc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a38f9dc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b856e42400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b856e42400000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944af2ab0200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944af2ab0200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b856e42400000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b856e42400000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f47338965619bce612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f47338965619bce612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b40612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b40612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625093565.159791,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00018_18_batch_size=64,learning_rate=0.00070805_2021-07-01_00-52-17\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00023\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0013774978663536918,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0013774978663536918,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"23_batch_size=32,learning_rate=0.0013775\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.5696605443954468,\n    \"mean_F1\": 0.6642971038818359,\n    \"time_this_iter_s\": 231.16743302345276,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 20,\n    \"experiment_id\": \"1e6893a202ec46569a01979f2b44a8d8\",\n    \"date\": \"2021-07-01_05-20-06\",\n    \"timestamp\": 1625109606,\n    \"time_total_s\": 13456.433845996857,\n    \"pid\": 186058,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0013774978663536918,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 13456.433845996857,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 20,\n    \"trial_id\": \"deb7c_00023\",\n    \"experiment_tag\": \"23_batch_size=32,learning_rate=0.0013775\"\n  },\n  \"last_update_time\": 1625109606.629517,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.8939863443374634,\n      \"min\": 1.546707272529602,\n      \"avg\": 1.6250338375568392,\n      \"last\": 1.5696605443954468,\n      \"last-5-avg\": 1.5833891868591308,\n      \"last-10-avg\": 1.5868264794349671\n    },\n    \"mean_F1\": {\n      \"max\": 0.6642971038818359,\n      \"min\": 0.5645933151245117,\n      \"avg\": 0.6413855820894241,\n      \"last\": 0.6642971038818359,\n      \"last-5-avg\": 0.6622281193733215,\n      \"last-10-avg\": 0.6580495595932007\n    },\n    \"time_this_iter_s\": {\n      \"max\": 798.6137444972992,\n      \"min\": 231.07324624061584,\n      \"avg\": 672.8216922998428,\n      \"last\": 231.16743302345276,\n      \"last-5-avg\": 339.11824054718016,\n      \"last-10-avg\": 561.8785581588745\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.05,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.1\n    },\n    \"training_iteration\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"timestamp\": {\n      \"max\": 1625109606,\n      \"min\": 1625096948,\n      \"avg\": 1625104097.3999996,\n      \"last\": 1625109606,\n      \"last-5-avg\": 1625109120.0,\n      \"last-10-avg\": 1625107729.1\n    },\n    \"time_total_s\": {\n      \"max\": 13456.433845996857,\n      \"min\": 798.6137444972992,\n      \"avg\": 7947.786399078368,\n      \"last\": 13456.433845996857,\n      \"last-5-avg\": 12970.382609462738,\n      \"last-10-avg\": 11579.39016981125\n    },\n    \"pid\": {\n      \"max\": 186058,\n      \"min\": 186058,\n      \"avg\": 186058.0,\n      \"last\": 186058,\n      \"last-5-avg\": 186058.0,\n      \"last-10-avg\": 186058.0\n    },\n    \"time_since_restore\": {\n      \"max\": 13456.433845996857,\n      \"min\": 798.6137444972992,\n      \"avg\": 7947.786399078368,\n      \"last\": 13456.433845996857,\n      \"last-5-avg\": 12970.382609462738,\n      \"last-10-avg\": 11579.39016981125\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0013774978663536918,\n      \"min\": 0.0013774978663536918,\n      \"avg\": 0.0013774978663536914,\n      \"last\": 0.0013774978663536918,\n      \"last-5-avg\": 0.0013774978663536918,\n      \"last-10-avg\": 0.0013774978663536918\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ff8bf5020000000473ffab750c0000000473ff9520c40000000473ff8c5ce00000000473ff91d5460000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ff9d9a840000000473ff95644a0000000473ff9da9f20000000473ff8defbe0000000473ff94f1240000000473ff8bf5020000000473ffab750c0000000473ff9520c40000000473ff8c5ce00000000473ff91d5460000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe520f580000000473fe525dcc0000000473fe5310d20000000473fe53b11c0000000473fe541ec00000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe4ce0b40000000473fe4dd9dc0000000473fe4e9b440000000473fe4fac7e0000000473fe50e6940000000473fe520f580000000473fe525dcc0000000473fe5310d20000000473fe53b11c0000000473fe541ec00000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474084bb971760000047407476fd1240000047406e4ee0fb00000047406ce2580880000047406ce55b9c800000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088533c856000004740884a88dbc00000474088ed0930e00000474088adcd6680000047408860f21e000000474084bb971760000047407476fd1240000047406e4ee0fb00000047406ce2580880000047406ce55b9c800000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a5e30dd604aa531dd604a9832dd604a7f33dd604a6634dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a7e21dd604a8724dd604aa527dd604aba2add604ac72ddd604a5e30dd604aa531dd604a9832dd604a7f33dd604a6634dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c844254d3200004740c8e7dd35c400004740c96118b9b000004740c9d4a219d200004740ca483788440000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c0d406c2aa00004740c258af506600004740c3e77fe37400004740c5725cb9dc00004740c6f86bdbbc00004740c844254d3200004740c8e7dd35c400004740c96118b9b000004740c9d4a219d200004740ca483788440000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284acad602004acad602004acad602004acad602004acad60200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284acad602004acad602004acad602004acad602004acad602004acad602004acad602004acad602004acad602004acad60200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c844254d3200004740c8e7dd35c400004740c96118b9b000004740c9d4a219d200004740ca483788440000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740c0d406c2aa00004740c258af506600004740c3e77fe37400004740c5725cb9dc00004740c6f86bdbbc00004740c844254d3200004740c8e7dd35c400004740c96118b9b000004740c9d4a219d200004740ca483788440000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f473f5691a51252b88f652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b204b204b204b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059540000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625096135.7714365,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00023_23_batch_size=32,learning_rate=0.0013775_2021-07-01_01-30-32\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00019\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0001359663352175119,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0001359663352175119,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"19_batch_size=16,learning_rate=0.00013597\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.243975877761841,\n    \"mean_F1\": 0.5521008968353271,\n    \"time_this_iter_s\": 774.1953160762787,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"8f03380eeebb44569edf6837a5c1f1a7\",\n    \"date\": \"2021-07-01_01-20-46\",\n    \"timestamp\": 1625095246,\n    \"time_total_s\": 774.1953160762787,\n    \"pid\": 178774,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0001359663352175119,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 774.1953160762787,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00019\",\n    \"experiment_tag\": \"19_batch_size=16,learning_rate=0.00013597\"\n  },\n  \"last_update_time\": 1625095246.5049617,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.243975877761841,\n      \"min\": 2.243975877761841,\n      \"avg\": 2.243975877761841,\n      \"last\": 2.243975877761841,\n      \"last-5-avg\": 2.243975877761841,\n      \"last-10-avg\": 2.243975877761841\n    },\n    \"mean_F1\": {\n      \"max\": 0.5521008968353271,\n      \"min\": 0.5521008968353271,\n      \"avg\": 0.5521008968353271,\n      \"last\": 0.5521008968353271,\n      \"last-5-avg\": 0.5521008968353271,\n      \"last-10-avg\": 0.5521008968353271\n    },\n    \"time_this_iter_s\": {\n      \"max\": 774.1953160762787,\n      \"min\": 774.1953160762787,\n      \"avg\": 774.1953160762787,\n      \"last\": 774.1953160762787,\n      \"last-5-avg\": 774.1953160762787,\n      \"last-10-avg\": 774.1953160762787\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625095246,\n      \"min\": 1625095246,\n      \"avg\": 1625095246,\n      \"last\": 1625095246,\n      \"last-5-avg\": 1625095246,\n      \"last-10-avg\": 1625095246\n    },\n    \"time_total_s\": {\n      \"max\": 774.1953160762787,\n      \"min\": 774.1953160762787,\n      \"avg\": 774.1953160762787,\n      \"last\": 774.1953160762787,\n      \"last-5-avg\": 774.1953160762787,\n      \"last-10-avg\": 774.1953160762787\n    },\n    \"pid\": {\n      \"max\": 178774,\n      \"min\": 178774,\n      \"avg\": 178774,\n      \"last\": 178774,\n      \"last-5-avg\": 178774,\n      \"last-10-avg\": 178774\n    },\n    \"time_since_restore\": {\n      \"max\": 774.1953160762787,\n      \"min\": 774.1953160762787,\n      \"avg\": 774.1953160762787,\n      \"last\": 774.1953160762787,\n      \"last-5-avg\": 774.1953160762787,\n      \"last-10-avg\": 774.1953160762787\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0001359663352175119,\n      \"min\": 0.0001359663352175119,\n      \"avg\": 0.0001359663352175119,\n      \"last\": 0.0001359663352175119,\n      \"last-5-avg\": 0.0001359663352175119,\n      \"last-10-avg\": 0.0001359663352175119\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16,\n      \"last\": 16,\n      \"last-5-avg\": 16,\n      \"last-10-avg\": 16\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474001f3a9a0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474001f3a9a0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe1aacf80000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe1aacf80000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474088319001e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474088319001e00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a4efcdc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a4efcdc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474088319001e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474088319001e00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a56ba0200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a56ba0200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474088319001e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474088319001e00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f21d245ed1d9098612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f21d245ed1d9098612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b10612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b10612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625094456.7827837,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00019_19_batch_size=16,learning_rate=0.00013597_2021-07-01_00-52-45\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00020\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.002404896703968453,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.002404896703968453,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"20_batch_size=64,learning_rate=0.0024049\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.9945259094238281,\n    \"mean_F1\": 0.5361794233322144,\n    \"time_this_iter_s\": 873.8978204727173,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"0406728ddaea40dbb72360173064afd9\",\n    \"date\": \"2021-07-01_01-35-35\",\n    \"timestamp\": 1625096135,\n    \"time_total_s\": 873.8978204727173,\n    \"pid\": 182112,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.002404896703968453,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 873.8978204727173,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00020\",\n    \"experiment_tag\": \"20_batch_size=64,learning_rate=0.0024049\"\n  },\n  \"last_update_time\": 1625096135.7167535,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9945259094238281,\n      \"min\": 1.9945259094238281,\n      \"avg\": 1.9945259094238281,\n      \"last\": 1.9945259094238281,\n      \"last-5-avg\": 1.9945259094238281,\n      \"last-10-avg\": 1.9945259094238281\n    },\n    \"mean_F1\": {\n      \"max\": 0.5361794233322144,\n      \"min\": 0.5361794233322144,\n      \"avg\": 0.5361794233322144,\n      \"last\": 0.5361794233322144,\n      \"last-5-avg\": 0.5361794233322144,\n      \"last-10-avg\": 0.5361794233322144\n    },\n    \"time_this_iter_s\": {\n      \"max\": 873.8978204727173,\n      \"min\": 873.8978204727173,\n      \"avg\": 873.8978204727173,\n      \"last\": 873.8978204727173,\n      \"last-5-avg\": 873.8978204727173,\n      \"last-10-avg\": 873.8978204727173\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625096135,\n      \"min\": 1625096135,\n      \"avg\": 1625096135,\n      \"last\": 1625096135,\n      \"last-5-avg\": 1625096135,\n      \"last-10-avg\": 1625096135\n    },\n    \"time_total_s\": {\n      \"max\": 873.8978204727173,\n      \"min\": 873.8978204727173,\n      \"avg\": 873.8978204727173,\n      \"last\": 873.8978204727173,\n      \"last-5-avg\": 873.8978204727173,\n      \"last-10-avg\": 873.8978204727173\n    },\n    \"pid\": {\n      \"max\": 182112,\n      \"min\": 182112,\n      \"avg\": 182112,\n      \"last\": 182112,\n      \"last-5-avg\": 182112,\n      \"last-10-avg\": 182112\n    },\n    \"time_since_restore\": {\n      \"max\": 873.8978204727173,\n      \"min\": 873.8978204727173,\n      \"avg\": 873.8978204727173,\n      \"last\": 873.8978204727173,\n      \"last-5-avg\": 873.8978204727173,\n      \"last-10-avg\": 873.8978204727173\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.002404896703968453,\n      \"min\": 0.002404896703968453,\n      \"avg\": 0.002404896703968453,\n      \"last\": 0.002404896703968453,\n      \"last-5-avg\": 0.002404896703968453,\n      \"last-10-avg\": 0.002404896703968453\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64,\n      \"last\": 64,\n      \"last-5-avg\": 64,\n      \"last-10-avg\": 64\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fffe99400000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fffe99400000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe12861c0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe12861c0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b4f2ebc800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b4f2ebc800000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ac7ffdc60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ac7ffdc60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b4f2ebc800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b4f2ebc800000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a60c70200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a60c70200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b4f2ebc800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b4f2ebc800000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f63b36f1633a1b8612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f63b36f1633a1b8612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b40612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b40612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625095246.7331107,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00020_20_batch_size=64,learning_rate=0.0024049_2021-07-01_01-07-36\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00022\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0007716129084682654,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0007716129084682654,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"22_batch_size=128,learning_rate=0.00077161\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.2099170684814453,\n    \"mean_F1\": 0.43894994258880615,\n    \"time_this_iter_s\": 943.4906587600708,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"01f45d76a590465f9b975c617b926709\",\n    \"date\": \"2021-07-01_01-46-29\",\n    \"timestamp\": 1625096789,\n    \"time_total_s\": 943.4906587600708,\n    \"pid\": 184657,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0007716129084682654,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 943.4906587600708,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00022\",\n    \"experiment_tag\": \"22_batch_size=128,learning_rate=0.00077161\"\n  },\n  \"last_update_time\": 1625096789.8987901,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.2099170684814453,\n      \"min\": 2.2099170684814453,\n      \"avg\": 2.2099170684814453,\n      \"last\": 2.2099170684814453,\n      \"last-5-avg\": 2.2099170684814453,\n      \"last-10-avg\": 2.2099170684814453\n    },\n    \"mean_F1\": {\n      \"max\": 0.43894994258880615,\n      \"min\": 0.43894994258880615,\n      \"avg\": 0.43894994258880615,\n      \"last\": 0.43894994258880615,\n      \"last-5-avg\": 0.43894994258880615,\n      \"last-10-avg\": 0.43894994258880615\n    },\n    \"time_this_iter_s\": {\n      \"max\": 943.4906587600708,\n      \"min\": 943.4906587600708,\n      \"avg\": 943.4906587600708,\n      \"last\": 943.4906587600708,\n      \"last-5-avg\": 943.4906587600708,\n      \"last-10-avg\": 943.4906587600708\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625096789,\n      \"min\": 1625096789,\n      \"avg\": 1625096789,\n      \"last\": 1625096789,\n      \"last-5-avg\": 1625096789,\n      \"last-10-avg\": 1625096789\n    },\n    \"time_total_s\": {\n      \"max\": 943.4906587600708,\n      \"min\": 943.4906587600708,\n      \"avg\": 943.4906587600708,\n      \"last\": 943.4906587600708,\n      \"last-5-avg\": 943.4906587600708,\n      \"last-10-avg\": 943.4906587600708\n    },\n    \"pid\": {\n      \"max\": 184657,\n      \"min\": 184657,\n      \"avg\": 184657,\n      \"last\": 184657,\n      \"last-5-avg\": 184657,\n      \"last-10-avg\": 184657\n    },\n    \"time_since_restore\": {\n      \"max\": 943.4906587600708,\n      \"min\": 943.4906587600708,\n      \"avg\": 943.4906587600708,\n      \"last\": 943.4906587600708,\n      \"last-5-avg\": 943.4906587600708,\n      \"last-10-avg\": 943.4906587600708\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0007716129084682654,\n      \"min\": 0.0007716129084682654,\n      \"avg\": 0.0007716129084682654,\n      \"last\": 0.0007716129084682654,\n      \"last-5-avg\": 0.0007716129084682654,\n      \"last-10-avg\": 0.0007716129084682654\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474001ade900000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474001ade900000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fdc17c180000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fdc17c180000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d7becde800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d7becde800000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a5502dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a5502dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d7becde800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d7becde800000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a51d10200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a51d10200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d7becde800000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d7becde800000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f4948c21a805f8f612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f4948c21a805f8f612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625095832.6086736,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00022_22_batch_size=128,learning_rate=0.00077161_2021-07-01_01-25-21\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00021\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0030806995333433384,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0030806995333433384,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"21_batch_size=16,learning_rate=0.0030807\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.9576860666275024,\n    \"mean_F1\": 0.5578583478927612,\n    \"time_this_iter_s\": 759.7424125671387,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"19174f33808748e5a1f5dce5fa9123b8\",\n    \"date\": \"2021-07-01_01-51-13\",\n    \"timestamp\": 1625097073,\n    \"time_total_s\": 1538.3489310741425,\n    \"pid\": 183315,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0030806995333433384,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1538.3489310741425,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00021\",\n    \"experiment_tag\": \"21_batch_size=16,learning_rate=0.0030807\"\n  },\n  \"last_update_time\": 1625097073.5032866,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9605029821395874,\n      \"min\": 1.9576860666275024,\n      \"avg\": 1.959094524383545,\n      \"last\": 1.9576860666275024,\n      \"last-5-avg\": 1.959094524383545,\n      \"last-10-avg\": 1.959094524383545\n    },\n    \"mean_F1\": {\n      \"max\": 0.5578583478927612,\n      \"min\": 0.5319327712059021,\n      \"avg\": 0.5448955595493317,\n      \"last\": 0.5578583478927612,\n      \"last-5-avg\": 0.5448955595493317,\n      \"last-10-avg\": 0.5448955595493317\n    },\n    \"time_this_iter_s\": {\n      \"max\": 778.6065185070038,\n      \"min\": 759.7424125671387,\n      \"avg\": 769.1744655370712,\n      \"last\": 759.7424125671387,\n      \"last-5-avg\": 769.1744655370712,\n      \"last-10-avg\": 769.1744655370712\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625097073,\n      \"min\": 1625096313,\n      \"avg\": 1625096693.0,\n      \"last\": 1625097073,\n      \"last-5-avg\": 1625096693.0,\n      \"last-10-avg\": 1625096693.0\n    },\n    \"time_total_s\": {\n      \"max\": 1538.3489310741425,\n      \"min\": 778.6065185070038,\n      \"avg\": 1158.4777247905731,\n      \"last\": 1538.3489310741425,\n      \"last-5-avg\": 1158.4777247905731,\n      \"last-10-avg\": 1158.4777247905731\n    },\n    \"pid\": {\n      \"max\": 183315,\n      \"min\": 183315,\n      \"avg\": 183315.0,\n      \"last\": 183315,\n      \"last-5-avg\": 183315.0,\n      \"last-10-avg\": 183315.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1538.3489310741425,\n      \"min\": 778.6065185070038,\n      \"avg\": 1158.4777247905731,\n      \"last\": 1538.3489310741425,\n      \"last-5-avg\": 1158.4777247905731,\n      \"last-10-avg\": 1158.4777247905731\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0030806995333433384,\n      \"min\": 0.0030806995333433384,\n      \"avg\": 0.0030806995333433384,\n      \"last\": 0.0030806995333433384,\n      \"last-5-avg\": 0.0030806995333433384,\n      \"last-10-avg\": 0.0030806995333433384\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16.0,\n      \"last\": 16,\n      \"last-5-avg\": 16.0,\n      \"last-10-avg\": 16.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fff5e3860000000473fff52aea0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fff5e3860000000473fff52aea0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe10597e0000000473fe1d9f9c0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe10597e0000000473fe1d9f9c0000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408854da26600000474087bdf076000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408854da26600000474087bdf076000000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a7900dd604a7103dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a7900dd604a7103dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408854da2660000047409809654e300000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408854da2660000047409809654e300000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a13cc02004a13cc0200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a13cc02004a13cc0200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408854da2660000047409809654e300000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408854da2660000047409809654e300000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f693cb1f7d30b0a473f693cb1f7d30b0a652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f693cb1f7d30b0a473f693cb1f7d30b0a652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b104b10652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625095521.051225,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00021_21_batch_size=16,learning_rate=0.0030807_2021-07-01_01-20-46\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00016\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0028089967877585266,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0028089967877585266,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"16_batch_size=32,learning_rate=0.002809\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.5708913803100586,\n    \"mean_F1\": 0.6552028059959412,\n    \"time_this_iter_s\": 785.4770739078522,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 16,\n    \"experiment_id\": \"9c44d94340c34f659de8fc5c244a0b1f\",\n    \"date\": \"2021-07-01_04-05-50\",\n    \"timestamp\": 1625105150,\n    \"time_total_s\": 12516.66636800766,\n    \"pid\": 171073,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0028089967877585266,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 12516.66636800766,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 16,\n    \"trial_id\": \"deb7c_00016\",\n    \"experiment_tag\": \"16_batch_size=32,learning_rate=0.002809\"\n  },\n  \"last_update_time\": 1625105151.0326235,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9359588623046875,\n      \"min\": 1.5693128108978271,\n      \"avg\": 1.6409852728247643,\n      \"last\": 1.5708913803100586,\n      \"last-5-avg\": 1.5927248001098633,\n      \"last-10-avg\": 1.5958166360855102\n    },\n    \"mean_F1\": {\n      \"max\": 0.6552028059959412,\n      \"min\": 0.5295056104660034,\n      \"avg\": 0.628859907388687,\n      \"last\": 0.6552028059959412,\n      \"last-5-avg\": 0.6515772581100464,\n      \"last-10-avg\": 0.6461584687232971\n    },\n    \"time_this_iter_s\": {\n      \"max\": 809.48211145401,\n      \"min\": 772.0229663848877,\n      \"avg\": 782.2916480004787,\n      \"last\": 785.4770739078522,\n      \"last-5-avg\": 785.3285909175872,\n      \"last-10-avg\": 782.040404677391\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.0625,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.1\n    },\n    \"training_iteration\": {\n      \"max\": 16,\n      \"min\": 1,\n      \"avg\": 8.5,\n      \"last\": 16,\n      \"last-5-avg\": 14.0,\n      \"last-10-avg\": 11.5\n    },\n    \"timestamp\": {\n      \"max\": 1625105150,\n      \"min\": 1625093443,\n      \"avg\": 1625099288.5,\n      \"last\": 1625105150,\n      \"last-5-avg\": 1625103589.0,\n      \"last-10-avg\": 1625101628.8\n    },\n    \"time_total_s\": {\n      \"max\": 12516.66636800766,\n      \"min\": 809.48211145401,\n      \"avg\": 6654.7143523693085,\n      \"last\": 12516.66636800766,\n      \"last-5-avg\": 10955.354439973831,\n      \"last-10-avg\": 8994.97531607151\n    },\n    \"pid\": {\n      \"max\": 171073,\n      \"min\": 171073,\n      \"avg\": 171073.0,\n      \"last\": 171073,\n      \"last-5-avg\": 171073.0,\n      \"last-10-avg\": 171073.0\n    },\n    \"time_since_restore\": {\n      \"max\": 12516.66636800766,\n      \"min\": 809.48211145401,\n      \"avg\": 6654.7143523693085,\n      \"last\": 12516.66636800766,\n      \"last-5-avg\": 10955.354439973831,\n      \"last-10-avg\": 8994.97531607151\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 16,\n      \"min\": 1,\n      \"avg\": 8.5,\n      \"last\": 16,\n      \"last-5-avg\": 14.0,\n      \"last-10-avg\": 11.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0028089967877585266,\n      \"min\": 0.0028089967877585266,\n      \"avg\": 0.002808996787758527,\n      \"last\": 0.0028089967877585266,\n      \"last-5-avg\": 0.0028089967877585266,\n      \"last-10-avg\": 0.0028089967877585266\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ff9686c00000000473ffa6b7340000000473ff94bd5a0000000473ff928ed20000000473ff9225f00000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ff92b17e0000000473ff92226c0000000473ffab5a720000000473ff91be7c0000000473ff9cad7c0000000473ff9686c00000000473ffa6b7340000000473ff94bd5a0000000473ff928ed20000000473ff9225f00000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe4be62c0000000473fe4cc4d00000000473fe4d526a0000000473fe4e95880000000473fe4f76be0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe44c72c0000000473fe4782780000000473fe478bd80000000473fe4976c80000000473fe4afee40000000473fe4be62c0000000473fe4cc4d00000000473fe4d526a0000000473fe4e95880000000473fe4f76be0000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740890057b6e00000474088ba5e45200000474088410188c000004740882d9c348000004740888bd10c200000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408893602dc00000474088202f090000004740885679b86000004740885442dc6000004740884fcaec0000004740890057b6e00000474088ba5e45200000474088410188c000004740882d9c348000004740888bd10c200000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b0c4b0d4b0e4b0f4b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b074b084b094b0a4b0b4b0c4b0d4b0e4b0f4b10652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ac816dd604adf19dd604ae71cdd604aed1fdd604afe22dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a8507dd604a890add604a930ddd604a9e10dd604aa813dd604ac816dd604adf19dd604ae71cdd604aed1fdd604afe22dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c257087aa400004740c3e2ae5ef600004740c566be778200004740c6e9983aca00004740c872554b8c0000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740b56aaf2d3400004740b86eb50e5400004740bb7984456000004740be840ca0ec00004740c0c702ff3600004740c257087aa400004740c3e2ae5ef600004740c566be778200004740c6e9983aca00004740c872554b8c0000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a419c02004a419c02004a419c02004a419c02004a419c0200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a419c02004a419c02004a419c02004a419c02004a419c02004a419c02004a419c02004a419c02004a419c02004a419c0200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c257087aa400004740c3e2ae5ef600004740c566be778200004740c6e9983aca00004740c872554b8c0000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740b56aaf2d3400004740b86eb50e5400004740bb7984456000004740be840ca0ec00004740c0c702ff3600004740c257087aa400004740c3e2ae5ef600004740c566be778200004740c6e9983aca00004740c872554b8c0000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b0c4b0d4b0e4b0f4b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b074b084b094b0a4b0b4b0c4b0d4b0e4b0f4b10652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471473f6702e4aad0d471652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b204b204b204b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059540000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625092620.9343631,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00016_16_batch_size=32,learning_rate=0.002809_2021-07-01_00-36-07\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00017\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0010739076860714453,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0010739076860714453,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"17_batch_size=32,learning_rate=0.0010739\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.5938141345977783,\n    \"mean_F1\": 0.6434983015060425,\n    \"time_this_iter_s\": 779.2019803524017,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 8,\n    \"experiment_id\": \"7e666b488ea5428d8758fd47a66e65e3\",\n    \"date\": \"2021-07-01_02-36-37\",\n    \"timestamp\": 1625099797,\n    \"time_total_s\": 6244.191653728485,\n    \"pid\": 174929,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0010739076860714453,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 6244.191653728485,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 8,\n    \"trial_id\": \"deb7c_00017\",\n    \"experiment_tag\": \"17_batch_size=32,learning_rate=0.0010739\"\n  },\n  \"last_update_time\": 1625099797.6203525,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9172215461730957,\n      \"min\": 1.5886813402175903,\n      \"avg\": 1.6834789067506788,\n      \"last\": 1.5938141345977783,\n      \"last-5-avg\": 1.6243379354476928,\n      \"last-10-avg\": 1.683478906750679\n    },\n    \"mean_F1\": {\n      \"max\": 0.6434983015060425,\n      \"min\": 0.5566188097000122,\n      \"avg\": 0.6167200058698654,\n      \"last\": 0.6434983015060425,\n      \"last-5-avg\": 0.6342399835586547,\n      \"last-10-avg\": 0.6167200058698654\n    },\n    \"time_this_iter_s\": {\n      \"max\": 800.8026461601257,\n      \"min\": 771.5044779777527,\n      \"avg\": 780.5239567160605,\n      \"last\": 779.2019803524017,\n      \"last-5-avg\": 779.9229317188262,\n      \"last-10-avg\": 780.5239567160606\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.125,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.125\n    },\n    \"training_iteration\": {\n      \"max\": 8,\n      \"min\": 1,\n      \"avg\": 4.5,\n      \"last\": 8,\n      \"last-5-avg\": 6.0,\n      \"last-10-avg\": 4.5\n    },\n    \"timestamp\": {\n      \"max\": 1625099797,\n      \"min\": 1625094354,\n      \"avg\": 1625097071.0,\n      \"last\": 1625099797,\n      \"last-5-avg\": 1625098238.4,\n      \"last-10-avg\": 1625097071.0\n    },\n    \"time_total_s\": {\n      \"max\": 6244.191653728485,\n      \"min\": 800.8026461601257,\n      \"avg\": 3518.221670955419,\n      \"last\": 6244.191653728485,\n      \"last-5-avg\": 4685.617320442199,\n      \"last-10-avg\": 3518.2216709554195\n    },\n    \"pid\": {\n      \"max\": 174929,\n      \"min\": 174929,\n      \"avg\": 174929.0,\n      \"last\": 174929,\n      \"last-5-avg\": 174929.0,\n      \"last-10-avg\": 174929.0\n    },\n    \"time_since_restore\": {\n      \"max\": 6244.191653728485,\n      \"min\": 800.8026461601257,\n      \"avg\": 3518.221670955419,\n      \"last\": 6244.191653728485,\n      \"last-5-avg\": 4685.617320442199,\n      \"last-10-avg\": 3518.2216709554195\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 8,\n      \"min\": 1,\n      \"avg\": 4.5,\n      \"last\": 8,\n      \"last-5-avg\": 6.0,\n      \"last-10-avg\": 4.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0010739076860714453,\n      \"min\": 0.0010739076860714453,\n      \"avg\": 0.0010739076860714451,\n      \"last\": 0.0010739076860714453,\n      \"last-5-avg\": 0.0010739076860714453,\n      \"last-10-avg\": 0.0010739076860714453\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffa2e4100000000473ff9f723c0000000473ffae18bc0000000473ff96b3d20000000473ff9804340000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffeacf080000000473ffc2cece0000000473ffaafee60000000473ffa2e4100000000473ff9f723c0000000473ffae18bc0000000473ff96b3d20000000473ff9804340000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe3f56380000000473fe43c1c00000000473fe4433c40000000473fe46e32c0000000473fe49789c0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe1cfd240000000473fe2fc20c0000000473fe39af180000000473fe3f56380000000473fe43c1c00000000473fe4433c40000000473fe46e32c0000000473fe49789c0000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740885beb45e000004740887b8758a000004740885e946f2000004740884d461ca00000474088599da7e00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089066bd1c000004740881c092bc000004740882228b22000004740885beb45e000004740887b8758a000004740885e946f2000004740884d461ca00000474088599da7e00000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b044b054b064b074b08652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b044b054b064b074b08652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ae501dd604af404dd604a0008dd604a0a0bdd604a150edd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ad2f8dc604ad5fbdc604ad9fedc604ae501dd604af404dd604a0008dd604a0a0bdd604a150edd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740a868223d6000004740ae8704138800004740b24f5497a800004740b558fd5b3c00004740b8643110380000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089066bd1c00000474098913a7ec000004740a251276be800004740a868223d6000004740ae8704138800004740b24f5497a800004740b558fd5b3c00004740b8643110380000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a51ab02004a51ab02004a51ab02004a51ab02004a51ab0200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a51ab02004a51ab02004a51ab02004a51ab02004a51ab02004a51ab02004a51ab02004a51ab0200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740a868223d6000004740ae8704138800004740b24f5497a800004740b558fd5b3c00004740b8643110380000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089066bd1c00000474098913a7ec000004740a251276be800004740a868223d6000004740ae8704138800004740b24f5497a800004740b558fd5b3c00004740b8643110380000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b044b054b064b074b08652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b044b054b064b074b08652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005956a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1473f51984b98ff96b1652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b204b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059532000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625093537.0166962,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00017_17_batch_size=32,learning_rate=0.0010739_2021-07-01_00-37-00\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00025\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0018175942409178665,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0018175942409178665,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"25_batch_size=32,learning_rate=0.0018176\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.5681504011154175,\n    \"mean_F1\": 0.6571869850158691,\n    \"time_this_iter_s\": 316.20261335372925,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 16,\n    \"experiment_id\": \"2c774d187eaa4ab7befd1e8558a5e8a1\",\n    \"date\": \"2021-07-01_05-09-14\",\n    \"timestamp\": 1625108954,\n    \"time_total_s\": 11867.54262804985,\n    \"pid\": 190147,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0018175942409178665,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 11867.54262804985,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 16,\n    \"trial_id\": \"deb7c_00025\",\n    \"experiment_tag\": \"25_batch_size=32,learning_rate=0.0018176\"\n  },\n  \"last_update_time\": 1625108955.0104208,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9200799465179443,\n      \"min\": 1.5681504011154175,\n      \"avg\": 1.6398382037878036,\n      \"last\": 1.5681504011154175,\n      \"last-5-avg\": 1.5862212181091309,\n      \"last-10-avg\": 1.595033013820648\n    },\n    \"mean_F1\": {\n      \"max\": 0.6571869850158691,\n      \"min\": 0.5542264580726624,\n      \"avg\": 0.6334770545363426,\n      \"last\": 0.6571869850158691,\n      \"last-5-avg\": 0.6535648822784423,\n      \"last-10-avg\": 0.6481513977050781\n    },\n    \"time_this_iter_s\": {\n      \"max\": 812.3894639015198,\n      \"min\": 316.20261335372925,\n      \"avg\": 741.7214142531155,\n      \"last\": 316.20261335372925,\n      \"last-5-avg\": 652.1836402893066,\n      \"last-10-avg\": 717.4137204647064\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.0625,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.1\n    },\n    \"training_iteration\": {\n      \"max\": 16,\n      \"min\": 1,\n      \"avg\": 8.5,\n      \"last\": 16,\n      \"last-5-avg\": 14.0,\n      \"last-10-avg\": 11.5\n    },\n    \"timestamp\": {\n      \"max\": 1625108954,\n      \"min\": 1625097899,\n      \"avg\": 1625103692.3125,\n      \"last\": 1625108954,\n      \"last-5-avg\": 1625107883.6,\n      \"last-10-avg\": 1625106006.7\n    },\n    \"time_total_s\": {\n      \"max\": 11867.54262804985,\n      \"min\": 812.3894639015198,\n      \"avg\": 6605.52795483172,\n      \"last\": 11867.54262804985,\n      \"last-5-avg\": 10796.955893230439,\n      \"last-10-avg\": 8919.893657803535\n    },\n    \"pid\": {\n      \"max\": 190147,\n      \"min\": 190147,\n      \"avg\": 190147.0,\n      \"last\": 190147,\n      \"last-5-avg\": 190147.0,\n      \"last-10-avg\": 190147.0\n    },\n    \"time_since_restore\": {\n      \"max\": 11867.54262804985,\n      \"min\": 812.3894639015198,\n      \"avg\": 6605.52795483172,\n      \"last\": 11867.54262804985,\n      \"last-5-avg\": 10796.955893230439,\n      \"last-10-avg\": 8919.893657803535\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 16,\n      \"min\": 1,\n      \"avg\": 8.5,\n      \"last\": 16,\n      \"last-5-avg\": 14.0,\n      \"last-10-avg\": 11.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0018175942409178665,\n      \"min\": 0.0018175942409178665,\n      \"avg\": 0.0018175942409178669,\n      \"last\": 0.0018175942409178665,\n      \"last-5-avg\": 0.0018175942409178665,\n      \"last-10-avg\": 0.0018175942409178665\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ff9c98de0000000473ff99586e0000000473ff954dfa0000000473ff91ab640000000473ff91724e0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ff9745e00000000473ff91a7a00000000473ffa2c95a0000000473ff9afab60000000473ff9e3a4e0000000473ff9c98de0000000473ff99586e0000000473ff954dfa0000000473ff91ab640000000473ff91724e0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe4c7fec0000000473fe4dc63c0000000473fe4eacd00000000473fe4fb2800000000473fe507ad00000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe455a720000000473fe484b520000000473fe4924920000000473fe4ab90e0000000473fe4be5540000000473fe4c7fec0000000473fe4dc63c0000000473fe4eacd00000000473fe4fb2800000000473fe507ad00000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740890af5646000004740888b60bf600000474088188e8bc0000047408256d4d6c00000474073c33de7800000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088c45b260000004740887035732000004740883482f60000004740880436d8a00000474088dc761d0000004740890af5646000004740888b60bf600000474088188e8bc0000047408256d4d6c00000474073c33de7800000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b0c4b0d4b0e4b0f4b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b074b084b094b0a4b0b4b0c4b0d4b0e4b0f4b10652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a3f28dd604a502bdd604a532edd604a9e30dd604ada31dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284aed18dd604afb1bdd604a011fdd604a0222dd604a1e25dd604a3f28dd604a502bdd604a532edd604a9e30dd604ada31dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c25fff437c00004740c3e8b54f7200004740c56a3e382e00004740c68fab859a00004740c72dc574d60000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740b56df32e9400004740b87bf9dcf800004740bb828a3bb800004740be831116cc00004740c0cf4fed3600004740c25fff437c00004740c3e8b54f7200004740c56a3e382e00004740c68fab859a00004740c72dc574d60000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ac3e602004ac3e602004ac3e602004ac3e602004ac3e60200652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ac3e602004ac3e602004ac3e602004ac3e602004ac3e602004ac3e602004ac3e602004ac3e602004ac3e602004ac3e60200652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740c25fff437c00004740c3e8b54f7200004740c56a3e382e00004740c68fab859a00004740c72dc574d60000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740b56df32e9400004740b87bf9dcf800004740bb828a3bb800004740be831116cc00004740c0cf4fed3600004740c25fff437c00004740c3e8b54f7200004740c56a3e382e00004740c68fab859a00004740c72dc574d60000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b0c4b0d4b0e4b0f4b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b074b084b094b0a4b0b4b0c4b0d4b0e4b0f4b10652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da473f5dc78af49df1da652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b204b204b204b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059540000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625097073.7118156,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00025_25_batch_size=32,learning_rate=0.0018176_2021-07-01_01-46-30\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00024\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0023704030045911843,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0023704030045911843,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"24_batch_size=16,learning_rate=0.0023704\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.0470752716064453,\n    \"mean_F1\": 0.5117647051811218,\n    \"time_this_iter_s\": 772.400102853775,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"ccfbdbdcc1b6414e95dd1ead91fcd4e3\",\n    \"date\": \"2021-07-01_01-59-37\",\n    \"timestamp\": 1625097577,\n    \"time_total_s\": 772.400102853775,\n    \"pid\": 188907,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0023704030045911843,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 772.400102853775,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00024\",\n    \"experiment_tag\": \"24_batch_size=16,learning_rate=0.0023704\"\n  },\n  \"last_update_time\": 1625097577.9410098,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.0470752716064453,\n      \"min\": 2.0470752716064453,\n      \"avg\": 2.0470752716064453,\n      \"last\": 2.0470752716064453,\n      \"last-5-avg\": 2.0470752716064453,\n      \"last-10-avg\": 2.0470752716064453\n    },\n    \"mean_F1\": {\n      \"max\": 0.5117647051811218,\n      \"min\": 0.5117647051811218,\n      \"avg\": 0.5117647051811218,\n      \"last\": 0.5117647051811218,\n      \"last-5-avg\": 0.5117647051811218,\n      \"last-10-avg\": 0.5117647051811218\n    },\n    \"time_this_iter_s\": {\n      \"max\": 772.400102853775,\n      \"min\": 772.400102853775,\n      \"avg\": 772.400102853775,\n      \"last\": 772.400102853775,\n      \"last-5-avg\": 772.400102853775,\n      \"last-10-avg\": 772.400102853775\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625097577,\n      \"min\": 1625097577,\n      \"avg\": 1625097577,\n      \"last\": 1625097577,\n      \"last-5-avg\": 1625097577,\n      \"last-10-avg\": 1625097577\n    },\n    \"time_total_s\": {\n      \"max\": 772.400102853775,\n      \"min\": 772.400102853775,\n      \"avg\": 772.400102853775,\n      \"last\": 772.400102853775,\n      \"last-5-avg\": 772.400102853775,\n      \"last-10-avg\": 772.400102853775\n    },\n    \"pid\": {\n      \"max\": 188907,\n      \"min\": 188907,\n      \"avg\": 188907,\n      \"last\": 188907,\n      \"last-5-avg\": 188907,\n      \"last-10-avg\": 188907\n    },\n    \"time_since_restore\": {\n      \"max\": 772.400102853775,\n      \"min\": 772.400102853775,\n      \"avg\": 772.400102853775,\n      \"last\": 772.400102853775,\n      \"last-5-avg\": 772.400102853775,\n      \"last-10-avg\": 772.400102853775\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0023704030045911843,\n      \"min\": 0.0023704030045911843,\n      \"avg\": 0.0023704030045911843,\n      \"last\": 0.0023704030045911843,\n      \"last-5-avg\": 0.0023704030045911843,\n      \"last-10-avg\": 0.0023704030045911843\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16,\n      \"last\": 16,\n      \"last-5-avg\": 16,\n      \"last-10-avg\": 16\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474000606900000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474000606900000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe0606060000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe0606060000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474088233369200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474088233369200000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a6905dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a6905dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474088233369200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474088233369200000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944aebe10200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944aebe10200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474088233369200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474088233369200000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f636b186c420586612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f636b186c420586612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b10612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b10612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625096790.1879694,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00024_24_batch_size=16,learning_rate=0.0023704_2021-07-01_01-35-35\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00026\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.00043937294575895807,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.00043937294575895807,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"26_batch_size=64,learning_rate=0.00043937\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.143594264984131,\n    \"mean_F1\": 0.5094066858291626,\n    \"time_this_iter_s\": 879.4203221797943,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"85836171f9634b53bd8265c331dc709f\",\n    \"date\": \"2021-07-01_02-14-32\",\n    \"timestamp\": 1625098472,\n    \"time_total_s\": 879.4203221797943,\n    \"pid\": 192434,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.00043937294575895807,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 879.4203221797943,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00026\",\n    \"experiment_tag\": \"26_batch_size=64,learning_rate=0.00043937\"\n  },\n  \"last_update_time\": 1625098472.1493888,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.143594264984131,\n      \"min\": 2.143594264984131,\n      \"avg\": 2.143594264984131,\n      \"last\": 2.143594264984131,\n      \"last-5-avg\": 2.143594264984131,\n      \"last-10-avg\": 2.143594264984131\n    },\n    \"mean_F1\": {\n      \"max\": 0.5094066858291626,\n      \"min\": 0.5094066858291626,\n      \"avg\": 0.5094066858291626,\n      \"last\": 0.5094066858291626,\n      \"last-5-avg\": 0.5094066858291626,\n      \"last-10-avg\": 0.5094066858291626\n    },\n    \"time_this_iter_s\": {\n      \"max\": 879.4203221797943,\n      \"min\": 879.4203221797943,\n      \"avg\": 879.4203221797943,\n      \"last\": 879.4203221797943,\n      \"last-5-avg\": 879.4203221797943,\n      \"last-10-avg\": 879.4203221797943\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625098472,\n      \"min\": 1625098472,\n      \"avg\": 1625098472,\n      \"last\": 1625098472,\n      \"last-5-avg\": 1625098472,\n      \"last-10-avg\": 1625098472\n    },\n    \"time_total_s\": {\n      \"max\": 879.4203221797943,\n      \"min\": 879.4203221797943,\n      \"avg\": 879.4203221797943,\n      \"last\": 879.4203221797943,\n      \"last-5-avg\": 879.4203221797943,\n      \"last-10-avg\": 879.4203221797943\n    },\n    \"pid\": {\n      \"max\": 192434,\n      \"min\": 192434,\n      \"avg\": 192434,\n      \"last\": 192434,\n      \"last-5-avg\": 192434,\n      \"last-10-avg\": 192434\n    },\n    \"time_since_restore\": {\n      \"max\": 879.4203221797943,\n      \"min\": 879.4203221797943,\n      \"avg\": 879.4203221797943,\n      \"last\": 879.4203221797943,\n      \"last-5-avg\": 879.4203221797943,\n      \"last-10-avg\": 879.4203221797943\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.00043937294575895807,\n      \"min\": 0.00043937294575895807,\n      \"avg\": 0.00043937294575895807,\n      \"last\": 0.00043937294575895807,\n      \"last-5-avg\": 0.00043937294575895807,\n      \"last-10-avg\": 0.00043937294575895807\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64,\n      \"last\": 64,\n      \"last-5-avg\": 64,\n      \"last-10-avg\": 64\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740012614c0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740012614c0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe04d0f40000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe04d0f40000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b7b5cd1e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b7b5cd1e00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ae808dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ae808dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b7b5cd1e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b7b5cd1e00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ab2ef0200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ab2ef0200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b7b5cd1e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b7b5cd1e00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f3ccb746ecacb88612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f3ccb746ecacb88612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b40612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b40612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625097578.2064288,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00026_26_batch_size=64,learning_rate=0.00043937_2021-07-01_01-51-13\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00027\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0011680630305737289,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0011680630305737289,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"27_batch_size=128,learning_rate=0.0011681\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.081984519958496,\n    \"mean_F1\": 0.49572649598121643,\n    \"time_this_iter_s\": 940.901802778244,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"8b92117fc17d4b01a2c707bfdd6011c5\",\n    \"date\": \"2021-07-01_02-30-27\",\n    \"timestamp\": 1625099427,\n    \"time_total_s\": 940.901802778244,\n    \"pid\": 196345,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0011680630305737289,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 940.901802778244,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00027\",\n    \"experiment_tag\": \"27_batch_size=128,learning_rate=0.0011681\"\n  },\n  \"last_update_time\": 1625099427.8476717,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.081984519958496,\n      \"min\": 2.081984519958496,\n      \"avg\": 2.081984519958496,\n      \"last\": 2.081984519958496,\n      \"last-5-avg\": 2.081984519958496,\n      \"last-10-avg\": 2.081984519958496\n    },\n    \"mean_F1\": {\n      \"max\": 0.49572649598121643,\n      \"min\": 0.49572649598121643,\n      \"avg\": 0.49572649598121643,\n      \"last\": 0.49572649598121643,\n      \"last-5-avg\": 0.49572649598121643,\n      \"last-10-avg\": 0.49572649598121643\n    },\n    \"time_this_iter_s\": {\n      \"max\": 940.901802778244,\n      \"min\": 940.901802778244,\n      \"avg\": 940.901802778244,\n      \"last\": 940.901802778244,\n      \"last-5-avg\": 940.901802778244,\n      \"last-10-avg\": 940.901802778244\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625099427,\n      \"min\": 1625099427,\n      \"avg\": 1625099427,\n      \"last\": 1625099427,\n      \"last-5-avg\": 1625099427,\n      \"last-10-avg\": 1625099427\n    },\n    \"time_total_s\": {\n      \"max\": 940.901802778244,\n      \"min\": 940.901802778244,\n      \"avg\": 940.901802778244,\n      \"last\": 940.901802778244,\n      \"last-5-avg\": 940.901802778244,\n      \"last-10-avg\": 940.901802778244\n    },\n    \"pid\": {\n      \"max\": 196345,\n      \"min\": 196345,\n      \"avg\": 196345,\n      \"last\": 196345,\n      \"last-5-avg\": 196345,\n      \"last-10-avg\": 196345\n    },\n    \"time_since_restore\": {\n      \"max\": 940.901802778244,\n      \"min\": 940.901802778244,\n      \"avg\": 940.901802778244,\n      \"last\": 940.901802778244,\n      \"last-5-avg\": 940.901802778244,\n      \"last-10-avg\": 940.901802778244\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0011680630305737289,\n      \"min\": 0.0011680630305737289,\n      \"avg\": 0.0011680630305737289,\n      \"last\": 0.0011680630305737289,\n      \"last-5-avg\": 0.0011680630305737289,\n      \"last-10-avg\": 0.0011680630305737289\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474000a7e780000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474000a7e780000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fdfb9fba0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fdfb9fba0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d6736e4600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d6736e4600000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944aa30cdd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944aa30cdd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d6736e4600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d6736e4600000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944af9fe0200612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944af9fe0200612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408d6736e4600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408d6736e4600000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f5323362105d464612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f5323362105d464612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625098472.3165278,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00027_27_batch_size=128,learning_rate=0.0011681_2021-07-01_01-59-38\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00028\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.00338558016236126,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.00338558016236126,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"28_batch_size=64,learning_rate=0.0033856\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.7818708419799805,\n    \"mean_F1\": 0.595693826675415,\n    \"time_this_iter_s\": 820.2732448577881,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"e61c6fdfa6684f8dad38d8b3be5806c7\",\n    \"date\": \"2021-07-01_02-58-52\",\n    \"timestamp\": 1625101132,\n    \"time_total_s\": 1689.2247788906097,\n    \"pid\": 200297,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.00338558016236126,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1689.2247788906097,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00028\",\n    \"experiment_tag\": \"28_batch_size=64,learning_rate=0.0033856\"\n  },\n  \"last_update_time\": 1625101132.6926293,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9385273456573486,\n      \"min\": 1.7818708419799805,\n      \"avg\": 1.8601990938186646,\n      \"last\": 1.7818708419799805,\n      \"last-5-avg\": 1.8601990938186646,\n      \"last-10-avg\": 1.8601990938186646\n    },\n    \"mean_F1\": {\n      \"max\": 0.595693826675415,\n      \"min\": 0.5665701627731323,\n      \"avg\": 0.5811319947242737,\n      \"last\": 0.595693826675415,\n      \"last-5-avg\": 0.5811319947242737,\n      \"last-10-avg\": 0.5811319947242737\n    },\n    \"time_this_iter_s\": {\n      \"max\": 868.9515340328217,\n      \"min\": 820.2732448577881,\n      \"avg\": 844.6123894453049,\n      \"last\": 820.2732448577881,\n      \"last-5-avg\": 844.6123894453049,\n      \"last-10-avg\": 844.6123894453049\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625101132,\n      \"min\": 1625100312,\n      \"avg\": 1625100722.0,\n      \"last\": 1625101132,\n      \"last-5-avg\": 1625100722.0,\n      \"last-10-avg\": 1625100722.0\n    },\n    \"time_total_s\": {\n      \"max\": 1689.2247788906097,\n      \"min\": 868.9515340328217,\n      \"avg\": 1279.0881564617157,\n      \"last\": 1689.2247788906097,\n      \"last-5-avg\": 1279.0881564617157,\n      \"last-10-avg\": 1279.0881564617157\n    },\n    \"pid\": {\n      \"max\": 200297,\n      \"min\": 200297,\n      \"avg\": 200297.0,\n      \"last\": 200297,\n      \"last-5-avg\": 200297.0,\n      \"last-10-avg\": 200297.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1689.2247788906097,\n      \"min\": 868.9515340328217,\n      \"avg\": 1279.0881564617157,\n      \"last\": 1689.2247788906097,\n      \"last-5-avg\": 1279.0881564617157,\n      \"last-10-avg\": 1279.0881564617157\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.00338558016236126,\n      \"min\": 0.00338558016236126,\n      \"avg\": 0.00338558016236126,\n      \"last\": 0.00338558016236126,\n      \"last-5-avg\": 0.00338558016236126,\n      \"last-10-avg\": 0.00338558016236126\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64.0,\n      \"last\": 64,\n      \"last-5-avg\": 64.0,\n      \"last-10-avg\": 64.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fff043540000000473ffc828b00000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fff043540000000473ffc828b00000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe22157c0000000473fe30fec80000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe22157c0000000473fe30fec80000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b279cbde00000474089a22f9b000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b279cbde00000474089a22f9b000000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a1810dd604a4c13dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a1810dd604a4c13dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b279cbde0000047409a64e62c700000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b279cbde0000047409a64e62c700000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a690e03004a690e0300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a690e03004a690e0300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b279cbde0000047409a64e62c700000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b279cbde0000047409a64e62c700000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f6bbc138269163a473f6bbc138269163a652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f6bbc138269163a473f6bbc138269163a652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b404b40652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b404b40652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625099428.1237338,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00028_28_batch_size=64,learning_rate=0.0033856_2021-07-01_02-14-32\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00029\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0014795176769718268,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0014795176769718268,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"29_batch_size=16,learning_rate=0.0014795\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.9138882160186768,\n    \"mean_F1\": 0.5703799724578857,\n    \"time_this_iter_s\": 764.3454666137695,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"ab0e279871bb429fbc0d55c11e43ca15\",\n    \"date\": \"2021-07-01_03-02-36\",\n    \"timestamp\": 1625101356,\n    \"time_total_s\": 1543.7711718082428,\n    \"pid\": 201907,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0014795176769718268,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1543.7711718082428,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00029\",\n    \"experiment_tag\": \"29_batch_size=16,learning_rate=0.0014795\"\n  },\n  \"last_update_time\": 1625101356.398685,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9556267261505127,\n      \"min\": 1.9138882160186768,\n      \"avg\": 1.9347574710845947,\n      \"last\": 1.9138882160186768,\n      \"last-5-avg\": 1.9347574710845947,\n      \"last-10-avg\": 1.9347574710845947\n    },\n    \"mean_F1\": {\n      \"max\": 0.5703799724578857,\n      \"min\": 0.5420168042182922,\n      \"avg\": 0.556198388338089,\n      \"last\": 0.5703799724578857,\n      \"last-5-avg\": 0.556198388338089,\n      \"last-10-avg\": 0.556198388338089\n    },\n    \"time_this_iter_s\": {\n      \"max\": 779.4257051944733,\n      \"min\": 764.3454666137695,\n      \"avg\": 771.8855859041214,\n      \"last\": 764.3454666137695,\n      \"last-5-avg\": 771.8855859041214,\n      \"last-10-avg\": 771.8855859041214\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625101356,\n      \"min\": 1625100591,\n      \"avg\": 1625100973.5,\n      \"last\": 1625101356,\n      \"last-5-avg\": 1625100973.5,\n      \"last-10-avg\": 1625100973.5\n    },\n    \"time_total_s\": {\n      \"max\": 1543.7711718082428,\n      \"min\": 779.4257051944733,\n      \"avg\": 1161.598438501358,\n      \"last\": 1543.7711718082428,\n      \"last-5-avg\": 1161.598438501358,\n      \"last-10-avg\": 1161.598438501358\n    },\n    \"pid\": {\n      \"max\": 201907,\n      \"min\": 201907,\n      \"avg\": 201907.0,\n      \"last\": 201907,\n      \"last-5-avg\": 201907.0,\n      \"last-10-avg\": 201907.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1543.7711718082428,\n      \"min\": 779.4257051944733,\n      \"avg\": 1161.598438501358,\n      \"last\": 1543.7711718082428,\n      \"last-5-avg\": 1161.598438501358,\n      \"last-10-avg\": 1161.598438501358\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0014795176769718268,\n      \"min\": 0.0014795176769718268,\n      \"avg\": 0.0014795176769718268,\n      \"last\": 0.0014795176769718268,\n      \"last-5-avg\": 0.0014795176769718268,\n      \"last-10-avg\": 0.0014795176769718268\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16.0,\n      \"last\": 16,\n      \"last-5-avg\": 16.0,\n      \"last-10-avg\": 16.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fff4a3f40000000473ffe9f4940000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fff4a3f40000000473ffe9f4940000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe15833a0000000473fe2408d80000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe15833a0000000473fe2408d80000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740885b67d8200000474087e2c384000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740885b67d8200000474087e2c384000000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a2f11dd604a2c14dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a2f11dd604a2c14dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740885b67d82000004740981f15ae100000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740885b67d82000004740981f15ae100000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ab31403004ab3140300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ab31403004ab3140300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740885b67d82000004740981f15ae100000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740885b67d82000004740981f15ae100000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f583d8c02552986473f583d8c02552986652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f583d8c02552986473f583d8c02552986652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b104b10652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625099797.9522977,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00029_29_batch_size=16,learning_rate=0.0014795_2021-07-01_02-30-28\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00030\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.00011556624761474977,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.00011556624761474977,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"30_batch_size=128,learning_rate=0.00011557\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.557321786880493,\n    \"mean_F1\": 0.3797313868999481,\n    \"time_this_iter_s\": 967.7648768424988,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"a9170f5a79ae4446956c71eb20972e67\",\n    \"date\": \"2021-07-01_03-15-15\",\n    \"timestamp\": 1625102115,\n    \"time_total_s\": 967.7648768424988,\n    \"pid\": 207528,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.00011556624761474977,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 967.7648768424988,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00030\",\n    \"experiment_tag\": \"30_batch_size=128,learning_rate=0.00011557\"\n  },\n  \"last_update_time\": 1625102115.1638172,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.557321786880493,\n      \"min\": 2.557321786880493,\n      \"avg\": 2.557321786880493,\n      \"last\": 2.557321786880493,\n      \"last-5-avg\": 2.557321786880493,\n      \"last-10-avg\": 2.557321786880493\n    },\n    \"mean_F1\": {\n      \"max\": 0.3797313868999481,\n      \"min\": 0.3797313868999481,\n      \"avg\": 0.3797313868999481,\n      \"last\": 0.3797313868999481,\n      \"last-5-avg\": 0.3797313868999481,\n      \"last-10-avg\": 0.3797313868999481\n    },\n    \"time_this_iter_s\": {\n      \"max\": 967.7648768424988,\n      \"min\": 967.7648768424988,\n      \"avg\": 967.7648768424988,\n      \"last\": 967.7648768424988,\n      \"last-5-avg\": 967.7648768424988,\n      \"last-10-avg\": 967.7648768424988\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625102115,\n      \"min\": 1625102115,\n      \"avg\": 1625102115,\n      \"last\": 1625102115,\n      \"last-5-avg\": 1625102115,\n      \"last-10-avg\": 1625102115\n    },\n    \"time_total_s\": {\n      \"max\": 967.7648768424988,\n      \"min\": 967.7648768424988,\n      \"avg\": 967.7648768424988,\n      \"last\": 967.7648768424988,\n      \"last-5-avg\": 967.7648768424988,\n      \"last-10-avg\": 967.7648768424988\n    },\n    \"pid\": {\n      \"max\": 207528,\n      \"min\": 207528,\n      \"avg\": 207528,\n      \"last\": 207528,\n      \"last-5-avg\": 207528,\n      \"last-10-avg\": 207528\n    },\n    \"time_since_restore\": {\n      \"max\": 967.7648768424988,\n      \"min\": 967.7648768424988,\n      \"avg\": 967.7648768424988,\n      \"last\": 967.7648768424988,\n      \"last-5-avg\": 967.7648768424988,\n      \"last-10-avg\": 967.7648768424988\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.00011556624761474977,\n      \"min\": 0.00011556624761474977,\n      \"avg\": 0.00011556624761474977,\n      \"last\": 0.00011556624761474977,\n      \"last-5-avg\": 0.00011556624761474977,\n      \"last-10-avg\": 0.00011556624761474977\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474004756520000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474004756520000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fd84d84e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fd84d84e0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e3e1e77c00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e3e1e77c00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a2317dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a2317dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e3e1e77c00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e3e1e77c00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944aa82a0300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944aa82a0300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e3e1e77c00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e3e1e77c00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f1e4b85041f9930612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f1e4b85041f9930612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625101132.760029,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00030_30_batch_size=128,learning_rate=0.00011557_2021-07-01_02-36-38\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00031\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.00039519233971673255,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.00039519233971673255,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"31_batch_size=32,learning_rate=0.00039519\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.03659987449646,\n    \"mean_F1\": 0.5677831172943115,\n    \"time_this_iter_s\": 832.4857246875763,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"8197bb4801484d1b83957cd430050adc\",\n    \"date\": \"2021-07-01_03-16-45\",\n    \"timestamp\": 1625102205,\n    \"time_total_s\": 832.4857246875763,\n    \"pid\": 208608,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.00039519233971673255,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 832.4857246875763,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00031\",\n    \"experiment_tag\": \"31_batch_size=32,learning_rate=0.00039519\"\n  },\n  \"last_update_time\": 1625102205.2028995,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.03659987449646,\n      \"min\": 2.03659987449646,\n      \"avg\": 2.03659987449646,\n      \"last\": 2.03659987449646,\n      \"last-5-avg\": 2.03659987449646,\n      \"last-10-avg\": 2.03659987449646\n    },\n    \"mean_F1\": {\n      \"max\": 0.5677831172943115,\n      \"min\": 0.5677831172943115,\n      \"avg\": 0.5677831172943115,\n      \"last\": 0.5677831172943115,\n      \"last-5-avg\": 0.5677831172943115,\n      \"last-10-avg\": 0.5677831172943115\n    },\n    \"time_this_iter_s\": {\n      \"max\": 832.4857246875763,\n      \"min\": 832.4857246875763,\n      \"avg\": 832.4857246875763,\n      \"last\": 832.4857246875763,\n      \"last-5-avg\": 832.4857246875763,\n      \"last-10-avg\": 832.4857246875763\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625102205,\n      \"min\": 1625102205,\n      \"avg\": 1625102205,\n      \"last\": 1625102205,\n      \"last-5-avg\": 1625102205,\n      \"last-10-avg\": 1625102205\n    },\n    \"time_total_s\": {\n      \"max\": 832.4857246875763,\n      \"min\": 832.4857246875763,\n      \"avg\": 832.4857246875763,\n      \"last\": 832.4857246875763,\n      \"last-5-avg\": 832.4857246875763,\n      \"last-10-avg\": 832.4857246875763\n    },\n    \"pid\": {\n      \"max\": 208608,\n      \"min\": 208608,\n      \"avg\": 208608,\n      \"last\": 208608,\n      \"last-5-avg\": 208608,\n      \"last-10-avg\": 208608\n    },\n    \"time_since_restore\": {\n      \"max\": 832.4857246875763,\n      \"min\": 832.4857246875763,\n      \"avg\": 832.4857246875763,\n      \"last\": 832.4857246875763,\n      \"last-5-avg\": 832.4857246875763,\n      \"last-10-avg\": 832.4857246875763\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.00039519233971673255,\n      \"min\": 0.00039519233971673255,\n      \"avg\": 0.00039519233971673255,\n      \"last\": 0.00039519233971673255,\n      \"last-5-avg\": 0.00039519233971673255,\n      \"last-10-avg\": 0.00039519233971673255\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32,\n      \"last\": 32,\n      \"last-5-avg\": 32,\n      \"last-10-avg\": 32\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740004af4e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740004af4e0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe22b4780000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe22b4780000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408a03e2c3a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408a03e2c3a00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a7d17dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a7d17dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408a03e2c3a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408a03e2c3a00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ae02e0300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ae02e0300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408a03e2c3a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408a03e2c3a00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f39e63a2cb9ff38612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f39e63a2cb9ff38612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b20612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b20612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625101356.7746847,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00031_31_batch_size=32,learning_rate=0.00039519_2021-07-01_02-58-52\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00032\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0022231830631521078,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0022231830631521078,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"32_batch_size=32,learning_rate=0.0022232\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.6395838260650635,\n    \"mean_F1\": 0.6265112161636353,\n    \"time_this_iter_s\": 784.832109451294,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 4,\n    \"experiment_id\": \"b812dd1a21c6412494f43ce02a53b16c\",\n    \"date\": \"2021-07-01_04-08-05\",\n    \"timestamp\": 1625105285,\n    \"time_total_s\": 3155.5616703033447,\n    \"pid\": 211907,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0022231830631521078,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 3155.5616703033447,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 4,\n    \"trial_id\": \"deb7c_00032\",\n    \"experiment_tag\": \"32_batch_size=32,learning_rate=0.0022232\"\n  },\n  \"last_update_time\": 1625105285.9675124,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9304429292678833,\n      \"min\": 1.6395838260650635,\n      \"avg\": 1.744362235069275,\n      \"last\": 1.6395838260650635,\n      \"last-5-avg\": 1.744362235069275,\n      \"last-10-avg\": 1.744362235069275\n    },\n    \"mean_F1\": {\n      \"max\": 0.6265112161636353,\n      \"min\": 0.5478469133377075,\n      \"avg\": 0.5956414639949799,\n      \"last\": 0.6265112161636353,\n      \"last-5-avg\": 0.5956414639949799,\n      \"last-10-avg\": 0.5956414639949799\n    },\n    \"time_this_iter_s\": {\n      \"max\": 812.3745670318604,\n      \"min\": 777.6026201248169,\n      \"avg\": 788.8904175758362,\n      \"last\": 784.832109451294,\n      \"last-5-avg\": 788.8904175758362,\n      \"last-10-avg\": 788.8904175758362\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.25,\n      \"last\": true,\n      \"last-5-avg\": 0.25,\n      \"last-10-avg\": 0.25\n    },\n    \"training_iteration\": {\n      \"max\": 4,\n      \"min\": 1,\n      \"avg\": 2.5,\n      \"last\": 4,\n      \"last-5-avg\": 2.5,\n      \"last-10-avg\": 2.5\n    },\n    \"timestamp\": {\n      \"max\": 1625105285,\n      \"min\": 1625102942,\n      \"avg\": 1625104112.0,\n      \"last\": 1625105285,\n      \"last-5-avg\": 1625104112.0,\n      \"last-10-avg\": 1625104112.0\n    },\n    \"time_total_s\": {\n      \"max\": 3155.5616703033447,\n      \"min\": 812.3745670318604,\n      \"avg\": 1982.1607463359833,\n      \"last\": 3155.5616703033447,\n      \"last-5-avg\": 1982.1607463359833,\n      \"last-10-avg\": 1982.1607463359833\n    },\n    \"pid\": {\n      \"max\": 211907,\n      \"min\": 211907,\n      \"avg\": 211907.0,\n      \"last\": 211907,\n      \"last-5-avg\": 211907.0,\n      \"last-10-avg\": 211907.0\n    },\n    \"time_since_restore\": {\n      \"max\": 3155.5616703033447,\n      \"min\": 812.3745670318604,\n      \"avg\": 1982.1607463359833,\n      \"last\": 3155.5616703033447,\n      \"last-5-avg\": 1982.1607463359833,\n      \"last-10-avg\": 1982.1607463359833\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 4,\n      \"min\": 1,\n      \"avg\": 2.5,\n      \"last\": 4,\n      \"last-5-avg\": 2.5,\n      \"last-10-avg\": 2.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0022231830631521078,\n      \"min\": 0.0022231830631521078,\n      \"avg\": 0.0022231830631521078,\n      \"last\": 0.0022231830631521078,\n      \"last-5-avg\": 0.0022231830631521078,\n      \"last-10-avg\": 0.0022231830631521078\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffee31820000000473ffba3be20000000473ffae10f00000000473ffa3bbc40000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffee31820000000473ffba3be20000000473ffae10f00000000473ffa3bbc40000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe187f640000000473fe3030300000000473fe3a6a040000000473fe40c6140000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe187f640000000473fe3030300000000473fe3a6a040000000473fe40c6140000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408962ff1d0000004740884cd22a8000004740886604dc80000047408886a829000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408962ff1d0000004740884cd22a8000004740886604dc80000047408886a829000000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942889898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b024b034b04652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b04652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a5e1add604a681ddd604a7520dd604a8523dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a5e1add604a681ddd604a7520dd604a8523dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408962ff1d000000474098d7e8a3c000004740a28575890000004740a8a71f93400000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408962ff1d000000474098d7e8a3c000004740a28575890000004740a8a71f93400000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ac33b03004ac33b03004ac33b03004ac33b0300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ac33b03004ac33b03004ac33b03004ac33b0300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408962ff1d000000474098d7e8a3c000004740a28575890000004740a8a71f93400000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408962ff1d000000474098d7e8a3c000004740a28575890000004740a8a71f93400000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b024b034b04652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b04652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f62365a51938876473f62365a51938876473f62365a51938876473f62365a51938876652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f62365a51938876473f62365a51938876473f62365a51938876473f62365a51938876652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952e000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952e000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625102115.3771021,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00032_32_batch_size=32,learning_rate=0.0022232_2021-07-01_03-02-36\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00033\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0018110936005270853,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0018110936005270853,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"33_batch_size=16,learning_rate=0.0018111\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.9861152172088623,\n    \"mean_F1\": 0.5361344814300537,\n    \"time_this_iter_s\": 785.0282084941864,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"3c23ae599d1744eebb5ede68f1e8b372\",\n    \"date\": \"2021-07-01_03-30-03\",\n    \"timestamp\": 1625103003,\n    \"time_total_s\": 785.0282084941864,\n    \"pid\": 212305,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0018110936005270853,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 785.0282084941864,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00033\",\n    \"experiment_tag\": \"33_batch_size=16,learning_rate=0.0018111\"\n  },\n  \"last_update_time\": 1625103003.2446427,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9861152172088623,\n      \"min\": 1.9861152172088623,\n      \"avg\": 1.9861152172088623,\n      \"last\": 1.9861152172088623,\n      \"last-5-avg\": 1.9861152172088623,\n      \"last-10-avg\": 1.9861152172088623\n    },\n    \"mean_F1\": {\n      \"max\": 0.5361344814300537,\n      \"min\": 0.5361344814300537,\n      \"avg\": 0.5361344814300537,\n      \"last\": 0.5361344814300537,\n      \"last-5-avg\": 0.5361344814300537,\n      \"last-10-avg\": 0.5361344814300537\n    },\n    \"time_this_iter_s\": {\n      \"max\": 785.0282084941864,\n      \"min\": 785.0282084941864,\n      \"avg\": 785.0282084941864,\n      \"last\": 785.0282084941864,\n      \"last-5-avg\": 785.0282084941864,\n      \"last-10-avg\": 785.0282084941864\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625103003,\n      \"min\": 1625103003,\n      \"avg\": 1625103003,\n      \"last\": 1625103003,\n      \"last-5-avg\": 1625103003,\n      \"last-10-avg\": 1625103003\n    },\n    \"time_total_s\": {\n      \"max\": 785.0282084941864,\n      \"min\": 785.0282084941864,\n      \"avg\": 785.0282084941864,\n      \"last\": 785.0282084941864,\n      \"last-5-avg\": 785.0282084941864,\n      \"last-10-avg\": 785.0282084941864\n    },\n    \"pid\": {\n      \"max\": 212305,\n      \"min\": 212305,\n      \"avg\": 212305,\n      \"last\": 212305,\n      \"last-5-avg\": 212305,\n      \"last-10-avg\": 212305\n    },\n    \"time_since_restore\": {\n      \"max\": 785.0282084941864,\n      \"min\": 785.0282084941864,\n      \"avg\": 785.0282084941864,\n      \"last\": 785.0282084941864,\n      \"last-5-avg\": 785.0282084941864,\n      \"last-10-avg\": 785.0282084941864\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0018110936005270853,\n      \"min\": 0.0018110936005270853,\n      \"avg\": 0.0018110936005270853,\n      \"last\": 0.0018110936005270853,\n      \"last-5-avg\": 0.0018110936005270853,\n      \"last-10-avg\": 0.0018110936005270853\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16,\n      \"last\": 16,\n      \"last-5-avg\": 16,\n      \"last-10-avg\": 16\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fffc720c0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fffc720c0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe1280380000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe1280380000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740888839c5600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740888839c5600000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a9b1add60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a9b1add60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740888839c5600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740888839c5600000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a513d0300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a513d0300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740888839c5600000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740888839c5600000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f5dac46f2314b7c612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f5dac46f2314b7c612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b10612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b10612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625102205.343586,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00033_33_batch_size=16,learning_rate=0.0018111_2021-07-01_03-15-15\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00034\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0005587075181557085,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0005587075181557085,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"34_batch_size=32,learning_rate=0.00055871\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.7935374975204468,\n    \"mean_F1\": 0.5957983732223511,\n    \"time_this_iter_s\": 780.0910623073578,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"968a17ac0b894f91b2fe8ccb7adcc97e\",\n    \"date\": \"2021-07-01_03-56-47\",\n    \"timestamp\": 1625104607,\n    \"time_total_s\": 1588.3515450954437,\n    \"pid\": 215664,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0005587075181557085,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1588.3515450954437,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00034\",\n    \"experiment_tag\": \"34_batch_size=32,learning_rate=0.00055871\"\n  },\n  \"last_update_time\": 1625104607.3462338,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9347946643829346,\n      \"min\": 1.7935374975204468,\n      \"avg\": 1.8641660809516907,\n      \"last\": 1.7935374975204468,\n      \"last-5-avg\": 1.8641660809516907,\n      \"last-10-avg\": 1.8641660809516907\n    },\n    \"mean_F1\": {\n      \"max\": 0.5957983732223511,\n      \"min\": 0.5717703104019165,\n      \"avg\": 0.5837843418121338,\n      \"last\": 0.5957983732223511,\n      \"last-5-avg\": 0.5837843418121338,\n      \"last-10-avg\": 0.5837843418121338\n    },\n    \"time_this_iter_s\": {\n      \"max\": 808.2604827880859,\n      \"min\": 780.0910623073578,\n      \"avg\": 794.1757725477219,\n      \"last\": 780.0910623073578,\n      \"last-5-avg\": 794.1757725477219,\n      \"last-10-avg\": 794.1757725477219\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625104607,\n      \"min\": 1625103827,\n      \"avg\": 1625104217.0,\n      \"last\": 1625104607,\n      \"last-5-avg\": 1625104217.0,\n      \"last-10-avg\": 1625104217.0\n    },\n    \"time_total_s\": {\n      \"max\": 1588.3515450954437,\n      \"min\": 808.2604827880859,\n      \"avg\": 1198.3060139417648,\n      \"last\": 1588.3515450954437,\n      \"last-5-avg\": 1198.3060139417648,\n      \"last-10-avg\": 1198.3060139417648\n    },\n    \"pid\": {\n      \"max\": 215664,\n      \"min\": 215664,\n      \"avg\": 215664.0,\n      \"last\": 215664,\n      \"last-5-avg\": 215664.0,\n      \"last-10-avg\": 215664.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1588.3515450954437,\n      \"min\": 808.2604827880859,\n      \"avg\": 1198.3060139417648,\n      \"last\": 1588.3515450954437,\n      \"last-5-avg\": 1198.3060139417648,\n      \"last-10-avg\": 1198.3060139417648\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0005587075181557085,\n      \"min\": 0.0005587075181557085,\n      \"avg\": 0.0005587075181557085,\n      \"last\": 0.0005587075181557085,\n      \"last-5-avg\": 0.0005587075181557085,\n      \"last-10-avg\": 0.0005587075181557085\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffef4eb40000000473ffcb25460000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffef4eb40000000473ffcb25460000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe24bf140000000473fe310c7c0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe24bf140000000473fe310c7c0000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408942157800000047408860ba7ee00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408942157800000047408860ba7ee00000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284ad31ddd604adf20dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284ad31ddd604adf20dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474089421578000000474098d167fb700000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089421578000000474098d167fb700000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a704a03004a704a0300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a704a03004a704a0300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474089421578000000474098d167fb700000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089421578000000474098d167fb700000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f424ec7425e7926473f424ec7425e7926652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f424ec7425e7926473f424ec7425e7926652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625103003.3688614,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00034_34_batch_size=32,learning_rate=0.00055871_2021-07-01_03-16-45\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00035\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0004743432826631148,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0004743432826631148,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"35_batch_size=32,learning_rate=0.00047434\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.9794397354125977,\n    \"mean_F1\": 0.5749601125717163,\n    \"time_this_iter_s\": 800.8222119808197,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"07caec117c1a4fe4b64a29c93fdf15f3\",\n    \"date\": \"2021-07-01_04-09-28\",\n    \"timestamp\": 1625105368,\n    \"time_total_s\": 800.8222119808197,\n    \"pid\": 222348,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0004743432826631148,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 800.8222119808197,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00035\",\n    \"experiment_tag\": \"35_batch_size=32,learning_rate=0.00047434\"\n  },\n  \"last_update_time\": 1625105368.644902,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9794397354125977,\n      \"min\": 1.9794397354125977,\n      \"avg\": 1.9794397354125977,\n      \"last\": 1.9794397354125977,\n      \"last-5-avg\": 1.9794397354125977,\n      \"last-10-avg\": 1.9794397354125977\n    },\n    \"mean_F1\": {\n      \"max\": 0.5749601125717163,\n      \"min\": 0.5749601125717163,\n      \"avg\": 0.5749601125717163,\n      \"last\": 0.5749601125717163,\n      \"last-5-avg\": 0.5749601125717163,\n      \"last-10-avg\": 0.5749601125717163\n    },\n    \"time_this_iter_s\": {\n      \"max\": 800.8222119808197,\n      \"min\": 800.8222119808197,\n      \"avg\": 800.8222119808197,\n      \"last\": 800.8222119808197,\n      \"last-5-avg\": 800.8222119808197,\n      \"last-10-avg\": 800.8222119808197\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625105368,\n      \"min\": 1625105368,\n      \"avg\": 1625105368,\n      \"last\": 1625105368,\n      \"last-5-avg\": 1625105368,\n      \"last-10-avg\": 1625105368\n    },\n    \"time_total_s\": {\n      \"max\": 800.8222119808197,\n      \"min\": 800.8222119808197,\n      \"avg\": 800.8222119808197,\n      \"last\": 800.8222119808197,\n      \"last-5-avg\": 800.8222119808197,\n      \"last-10-avg\": 800.8222119808197\n    },\n    \"pid\": {\n      \"max\": 222348,\n      \"min\": 222348,\n      \"avg\": 222348,\n      \"last\": 222348,\n      \"last-5-avg\": 222348,\n      \"last-10-avg\": 222348\n    },\n    \"time_since_restore\": {\n      \"max\": 800.8222119808197,\n      \"min\": 800.8222119808197,\n      \"avg\": 800.8222119808197,\n      \"last\": 800.8222119808197,\n      \"last-5-avg\": 800.8222119808197,\n      \"last-10-avg\": 800.8222119808197\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0004743432826631148,\n      \"min\": 0.0004743432826631148,\n      \"avg\": 0.0004743432826631148,\n      \"last\": 0.0004743432826631148,\n      \"last-5-avg\": 0.0004743432826631148,\n      \"last-10-avg\": 0.0004743432826631148\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32,\n      \"last\": 32,\n      \"last-5-avg\": 32,\n      \"last-10-avg\": 32\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fffabc900000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fffabc900000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe26612c0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe26612c0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740890693e3e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740890693e3e00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ad823dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ad823dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740890693e3e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740890693e3e00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a8c640300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a8c640300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740890693e3e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740890693e3e00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f3f1628e2d874d4612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f3f1628e2d874d4612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b20612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b20612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625104553.425365,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00035_35_batch_size=32,learning_rate=0.00047434_2021-07-01_03-30-03\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00036\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.00020250444749100112,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.00020250444749100112,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"36_batch_size=32,learning_rate=0.0002025\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.2546944618225098,\n    \"mean_F1\": 0.5263158082962036,\n    \"time_this_iter_s\": 815.2531259059906,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"f94878963b7f4d8f84d8fdf411f9a36e\",\n    \"date\": \"2021-07-01_04-10-34\",\n    \"timestamp\": 1625105434,\n    \"time_total_s\": 815.2531259059906,\n    \"pid\": 222665,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.00020250444749100112,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 815.2531259059906,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00036\",\n    \"experiment_tag\": \"36_batch_size=32,learning_rate=0.0002025\"\n  },\n  \"last_update_time\": 1625105435.0873692,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.2546944618225098,\n      \"min\": 2.2546944618225098,\n      \"avg\": 2.2546944618225098,\n      \"last\": 2.2546944618225098,\n      \"last-5-avg\": 2.2546944618225098,\n      \"last-10-avg\": 2.2546944618225098\n    },\n    \"mean_F1\": {\n      \"max\": 0.5263158082962036,\n      \"min\": 0.5263158082962036,\n      \"avg\": 0.5263158082962036,\n      \"last\": 0.5263158082962036,\n      \"last-5-avg\": 0.5263158082962036,\n      \"last-10-avg\": 0.5263158082962036\n    },\n    \"time_this_iter_s\": {\n      \"max\": 815.2531259059906,\n      \"min\": 815.2531259059906,\n      \"avg\": 815.2531259059906,\n      \"last\": 815.2531259059906,\n      \"last-5-avg\": 815.2531259059906,\n      \"last-10-avg\": 815.2531259059906\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625105434,\n      \"min\": 1625105434,\n      \"avg\": 1625105434,\n      \"last\": 1625105434,\n      \"last-5-avg\": 1625105434,\n      \"last-10-avg\": 1625105434\n    },\n    \"time_total_s\": {\n      \"max\": 815.2531259059906,\n      \"min\": 815.2531259059906,\n      \"avg\": 815.2531259059906,\n      \"last\": 815.2531259059906,\n      \"last-5-avg\": 815.2531259059906,\n      \"last-10-avg\": 815.2531259059906\n    },\n    \"pid\": {\n      \"max\": 222665,\n      \"min\": 222665,\n      \"avg\": 222665,\n      \"last\": 222665,\n      \"last-5-avg\": 222665,\n      \"last-10-avg\": 222665\n    },\n    \"time_since_restore\": {\n      \"max\": 815.2531259059906,\n      \"min\": 815.2531259059906,\n      \"avg\": 815.2531259059906,\n      \"last\": 815.2531259059906,\n      \"last-5-avg\": 815.2531259059906,\n      \"last-10-avg\": 815.2531259059906\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.00020250444749100112,\n      \"min\": 0.00020250444749100112,\n      \"avg\": 0.00020250444749100112,\n      \"last\": 0.00020250444749100112,\n      \"last-5-avg\": 0.00020250444749100112,\n      \"last-10-avg\": 0.00020250444749100112\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32,\n      \"last\": 32,\n      \"last-5-avg\": 32,\n      \"last-10-avg\": 32\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474002099d40000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474002099d40000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe0d79440000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe0d79440000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740897a0666e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740897a0666e00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a1a24dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a1a24dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740897a0666e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740897a0666e00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944ac9650300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944ac9650300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740897a0666e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740897a0666e00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f2a8aebf562aae8612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f2a8aebf562aae8612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b20612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b20612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625104607.6004908,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00036_36_batch_size=32,learning_rate=0.0002025_2021-07-01_03-55-53\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00037\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0006225610143498796,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0006225610143498796,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"37_batch_size=64,learning_rate=0.00062256\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.0560243129730225,\n    \"mean_F1\": 0.5361794233322144,\n    \"time_this_iter_s\": 887.6222469806671,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"1f477a78eec24ad682e9d11d02a7d0a8\",\n    \"date\": \"2021-07-01_04-20-53\",\n    \"timestamp\": 1625106053,\n    \"time_total_s\": 887.6222469806671,\n    \"pid\": 225044,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0006225610143498796,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 887.6222469806671,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00037\",\n    \"experiment_tag\": \"37_batch_size=64,learning_rate=0.00062256\"\n  },\n  \"last_update_time\": 1625106053.760721,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.0560243129730225,\n      \"min\": 2.0560243129730225,\n      \"avg\": 2.0560243129730225,\n      \"last\": 2.0560243129730225,\n      \"last-5-avg\": 2.0560243129730225,\n      \"last-10-avg\": 2.0560243129730225\n    },\n    \"mean_F1\": {\n      \"max\": 0.5361794233322144,\n      \"min\": 0.5361794233322144,\n      \"avg\": 0.5361794233322144,\n      \"last\": 0.5361794233322144,\n      \"last-5-avg\": 0.5361794233322144,\n      \"last-10-avg\": 0.5361794233322144\n    },\n    \"time_this_iter_s\": {\n      \"max\": 887.6222469806671,\n      \"min\": 887.6222469806671,\n      \"avg\": 887.6222469806671,\n      \"last\": 887.6222469806671,\n      \"last-5-avg\": 887.6222469806671,\n      \"last-10-avg\": 887.6222469806671\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625106053,\n      \"min\": 1625106053,\n      \"avg\": 1625106053,\n      \"last\": 1625106053,\n      \"last-5-avg\": 1625106053,\n      \"last-10-avg\": 1625106053\n    },\n    \"time_total_s\": {\n      \"max\": 887.6222469806671,\n      \"min\": 887.6222469806671,\n      \"avg\": 887.6222469806671,\n      \"last\": 887.6222469806671,\n      \"last-5-avg\": 887.6222469806671,\n      \"last-10-avg\": 887.6222469806671\n    },\n    \"pid\": {\n      \"max\": 225044,\n      \"min\": 225044,\n      \"avg\": 225044,\n      \"last\": 225044,\n      \"last-5-avg\": 225044,\n      \"last-10-avg\": 225044\n    },\n    \"time_since_restore\": {\n      \"max\": 887.6222469806671,\n      \"min\": 887.6222469806671,\n      \"avg\": 887.6222469806671,\n      \"last\": 887.6222469806671,\n      \"last-5-avg\": 887.6222469806671,\n      \"last-10-avg\": 887.6222469806671\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0006225610143498796,\n      \"min\": 0.0006225610143498796,\n      \"avg\": 0.0006225610143498796,\n      \"last\": 0.0006225610143498796,\n      \"last-5-avg\": 0.0006225610143498796,\n      \"last-10-avg\": 0.0006225610143498796\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64,\n      \"last\": 64,\n      \"last-5-avg\": 64,\n      \"last-10-avg\": 64\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447400072bce0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447400072bce0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe12861c0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe12861c0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408bbcfa5ca00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408bbcfa5ca00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a8526dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a8526dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408bbcfa5ca00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408bbcfa5ca00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a146f0300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a146f0300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408bbcfa5ca00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408bbcfa5ca00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f44666b99238c20612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f44666b99238c20612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b40612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b40612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625105151.2501926,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00037_37_batch_size=64,learning_rate=0.00062256_2021-07-01_03-56-47\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00038\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 6.962676002974285e-05,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 6.962676002974285e-05,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"38_batch_size=64,learning_rate=6.9627e-05\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.567878484725952,\n    \"mean_F1\": 0.40955138206481934,\n    \"time_this_iter_s\": 883.8195436000824,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"a3b4de7557374c21966a5ed89939c915\",\n    \"date\": \"2021-07-01_04-23-02\",\n    \"timestamp\": 1625106182,\n    \"time_total_s\": 883.8195436000824,\n    \"pid\": 225648,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 6.962676002974285e-05,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 883.8195436000824,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00038\",\n    \"experiment_tag\": \"38_batch_size=64,learning_rate=6.9627e-05\"\n  },\n  \"last_update_time\": 1625106182.9022942,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.567878484725952,\n      \"min\": 2.567878484725952,\n      \"avg\": 2.567878484725952,\n      \"last\": 2.567878484725952,\n      \"last-5-avg\": 2.567878484725952,\n      \"last-10-avg\": 2.567878484725952\n    },\n    \"mean_F1\": {\n      \"max\": 0.40955138206481934,\n      \"min\": 0.40955138206481934,\n      \"avg\": 0.40955138206481934,\n      \"last\": 0.40955138206481934,\n      \"last-5-avg\": 0.40955138206481934,\n      \"last-10-avg\": 0.40955138206481934\n    },\n    \"time_this_iter_s\": {\n      \"max\": 883.8195436000824,\n      \"min\": 883.8195436000824,\n      \"avg\": 883.8195436000824,\n      \"last\": 883.8195436000824,\n      \"last-5-avg\": 883.8195436000824,\n      \"last-10-avg\": 883.8195436000824\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625106182,\n      \"min\": 1625106182,\n      \"avg\": 1625106182,\n      \"last\": 1625106182,\n      \"last-5-avg\": 1625106182,\n      \"last-10-avg\": 1625106182\n    },\n    \"time_total_s\": {\n      \"max\": 883.8195436000824,\n      \"min\": 883.8195436000824,\n      \"avg\": 883.8195436000824,\n      \"last\": 883.8195436000824,\n      \"last-5-avg\": 883.8195436000824,\n      \"last-10-avg\": 883.8195436000824\n    },\n    \"pid\": {\n      \"max\": 225648,\n      \"min\": 225648,\n      \"avg\": 225648,\n      \"last\": 225648,\n      \"last-5-avg\": 225648,\n      \"last-10-avg\": 225648\n    },\n    \"time_since_restore\": {\n      \"max\": 883.8195436000824,\n      \"min\": 883.8195436000824,\n      \"avg\": 883.8195436000824,\n      \"last\": 883.8195436000824,\n      \"last-5-avg\": 883.8195436000824,\n      \"last-10-avg\": 883.8195436000824\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 6.962676002974285e-05,\n      \"min\": 6.962676002974285e-05,\n      \"avg\": 6.962676002974285e-05,\n      \"last\": 6.962676002974285e-05,\n      \"last-5-avg\": 6.962676002974285e-05,\n      \"last-10-avg\": 6.962676002974285e-05\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64,\n      \"last\": 64,\n      \"last-5-avg\": 64,\n      \"last-10-avg\": 64\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740048b03e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740048b03e0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fda361700000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fda361700000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b9e8e6ce00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b9e8e6ce00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a0627dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a0627dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b9e8e6ce00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b9e8e6ce00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a70710300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a70710300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408b9e8e6ce00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408b9e8e6ce00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f124092a1073dc0612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f124092a1073dc0612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b40612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b40612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625105286.1652782,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00038_38_batch_size=64,learning_rate=6.9627e-05_2021-07-01_04-05-51\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00039\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.001322368082271312,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.001322368082271312,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"39_batch_size=32,learning_rate=0.0013224\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.5850101709365845,\n    \"mean_F1\": 0.6620452404022217,\n    \"time_this_iter_s\": 174.18988132476807,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 20,\n    \"experiment_id\": \"2b320688b5e944a8b4baa43c2e949760\",\n    \"date\": \"2021-07-01_05-54-02\",\n    \"timestamp\": 1625111642,\n    \"time_total_s\": 6261.7974462509155,\n    \"pid\": 226041,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.001322368082271312,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 6261.7974462509155,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 20,\n    \"trial_id\": \"deb7c_00039\",\n    \"experiment_tag\": \"39_batch_size=32,learning_rate=0.0013224\"\n  },\n  \"last_update_time\": 1625111642.6243448,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.899275779724121,\n      \"min\": 1.5311517715454102,\n      \"avg\": 1.6309977710247034,\n      \"last\": 1.5850101709365845,\n      \"last-5-avg\": 1.5815927505493164,\n      \"last-10-avg\": 1.5948739171028137\n    },\n    \"mean_F1\": {\n      \"max\": 0.6620452404022217,\n      \"min\": 0.5622009634971619,\n      \"avg\": 0.6406435579061509,\n      \"last\": 0.6620452404022217,\n      \"last-5-avg\": 0.6595324039459228,\n      \"last-10-avg\": 0.6556682765483857\n    },\n    \"time_this_iter_s\": {\n      \"max\": 835.1631236076355,\n      \"min\": 172.38808226585388,\n      \"avg\": 313.0898723125457,\n      \"last\": 174.18988132476807,\n      \"last-5-avg\": 173.55508117675782,\n      \"last-10-avg\": 173.38636829853058\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.05,\n      \"last\": true,\n      \"last-5-avg\": 0.2,\n      \"last-10-avg\": 0.1\n    },\n    \"training_iteration\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"timestamp\": {\n      \"max\": 1625111642,\n      \"min\": 1625106215,\n      \"avg\": 1625109723.5500002,\n      \"last\": 1625111642,\n      \"last-5-avg\": 1625111294.8,\n      \"last-10-avg\": 1625110861.4\n    },\n    \"time_total_s\": {\n      \"max\": 6261.7974462509155,\n      \"min\": 835.1631236076355,\n      \"avg\": 4343.295947694779,\n      \"last\": 6261.7974462509155,\n      \"last-5-avg\": 5914.491006374359,\n      \"last-10-avg\": 5481.063758540154\n    },\n    \"pid\": {\n      \"max\": 226041,\n      \"min\": 226041,\n      \"avg\": 226041.0,\n      \"last\": 226041,\n      \"last-5-avg\": 226041.0,\n      \"last-10-avg\": 226041.0\n    },\n    \"time_since_restore\": {\n      \"max\": 6261.7974462509155,\n      \"min\": 835.1631236076355,\n      \"avg\": 4343.295947694779,\n      \"last\": 6261.7974462509155,\n      \"last-5-avg\": 5914.491006374359,\n      \"last-10-avg\": 5481.063758540154\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 20,\n      \"min\": 1,\n      \"avg\": 10.5,\n      \"last\": 20,\n      \"last-5-avg\": 18.0,\n      \"last-10-avg\": 15.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.001322368082271312,\n      \"min\": 0.001322368082271312,\n      \"avg\": 0.0013223680822713115,\n      \"last\": 0.001322368082271312,\n      \"last-5-avg\": 0.001322368082271312,\n      \"last-10-avg\": 0.001322368082271312\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ff8e7b580000000473ffa8d4ec0000000473ff9363420000000473ff87f9900000000473ff95c33a0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ff9e72c40000000473ff96448a0000000473ff98f8da0000000473ff98ffd40000000473ffa3c0460000000473ff8e7b580000000473ffa8d4ec0000000473ff9363420000000473ff87f9900000000473ff95c33a0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe5089400000000473fe50ee7e0000000473fe5195aa0000000473fe5262280000000473fe52f7980000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe4bd0600000000473fe4cc3300000000473fe4de0dc0000000473fe4eb5100000000473fe4f74e20000000473fe5089400000000473fe50ee7e0000000473fe5195aa0000000473fe5262280000000473fe52f7980000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474065c9bcb88000004740658e9443800000474065a51d6a800000474065b54e37800000474065c61382000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474065ac097f00000047406593b34c000000474065caf9298000004740658c6b2b800000474065abb20a800000474065c9bcb88000004740658e9443800000474065a51d6a800000474065b54e37800000474065c61382000000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059527000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288989898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898989898989898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284aa539dd604a513add604afe3add604aac3bdd604a5a3cdd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a4236dd604aee36dd604a9d37dd604a4938dd604af638dd604aa539dd604a513add604afe3add604aac3bdd604a5a3cdd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740b5c0538a3400004740b66cc82c5000004740b719f117a400004740b7c79b896000004740b875cc25700000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740b25d4f571400004740b309ecf17400004740b3b844bac000004740b464a8141c00004740b51205a47000004740b5c0538a3400004740b66cc82c5000004740b719f117a400004740b7c79b896000004740b875cc25700000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005953b000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284af97203004af97203004af97203004af97203004af9720300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059554000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284af97203004af97203004af97203004af97203004af97203004af97203004af97203004af97203004af97203004af9720300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284740b5c0538a3400004740b66cc82c5000004740b719f117a400004740b7c79b896000004740b875cc25700000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284740b25d4f571400004740b309ecf17400004740b3b844bac000004740b464a8141c00004740b51205a47000004740b5c0538a3400004740b66cc82c5000004740b719f117a400004740b7c79b896000004740b875cc25700000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b004b004b004b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b114b124b134b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b0b4b0c4b0d4b0e4b0f4b104b114b124b134b14652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005954f000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005957c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf473f55aa69eaaa0edf652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b204b204b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b204b204b204b204b204b204b204b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b144b144b144b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b324b324b324b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059531000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059540000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625105368.8258636,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00039_39_batch_size=32,learning_rate=0.0013224_2021-07-01_04-08-06\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00040\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0011016894089691334,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0011016894089691334,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"40_batch_size=128,learning_rate=0.0011017\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.1016454696655273,\n    \"mean_F1\": 0.4511599540710449,\n    \"time_this_iter_s\": 979.5854151248932,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"2b2db1f01fda417f9201f508e7f1aacd\",\n    \"date\": \"2021-07-01_04-27-02\",\n    \"timestamp\": 1625106422,\n    \"time_total_s\": 979.5854151248932,\n    \"pid\": 226430,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0011016894089691334,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 979.5854151248932,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00040\",\n    \"experiment_tag\": \"40_batch_size=128,learning_rate=0.0011017\"\n  },\n  \"last_update_time\": 1625106422.9504933,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.1016454696655273,\n      \"min\": 2.1016454696655273,\n      \"avg\": 2.1016454696655273,\n      \"last\": 2.1016454696655273,\n      \"last-5-avg\": 2.1016454696655273,\n      \"last-10-avg\": 2.1016454696655273\n    },\n    \"mean_F1\": {\n      \"max\": 0.4511599540710449,\n      \"min\": 0.4511599540710449,\n      \"avg\": 0.4511599540710449,\n      \"last\": 0.4511599540710449,\n      \"last-5-avg\": 0.4511599540710449,\n      \"last-10-avg\": 0.4511599540710449\n    },\n    \"time_this_iter_s\": {\n      \"max\": 979.5854151248932,\n      \"min\": 979.5854151248932,\n      \"avg\": 979.5854151248932,\n      \"last\": 979.5854151248932,\n      \"last-5-avg\": 979.5854151248932,\n      \"last-10-avg\": 979.5854151248932\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625106422,\n      \"min\": 1625106422,\n      \"avg\": 1625106422,\n      \"last\": 1625106422,\n      \"last-5-avg\": 1625106422,\n      \"last-10-avg\": 1625106422\n    },\n    \"time_total_s\": {\n      \"max\": 979.5854151248932,\n      \"min\": 979.5854151248932,\n      \"avg\": 979.5854151248932,\n      \"last\": 979.5854151248932,\n      \"last-5-avg\": 979.5854151248932,\n      \"last-10-avg\": 979.5854151248932\n    },\n    \"pid\": {\n      \"max\": 226430,\n      \"min\": 226430,\n      \"avg\": 226430,\n      \"last\": 226430,\n      \"last-5-avg\": 226430,\n      \"last-10-avg\": 226430\n    },\n    \"time_since_restore\": {\n      \"max\": 979.5854151248932,\n      \"min\": 979.5854151248932,\n      \"avg\": 979.5854151248932,\n      \"last\": 979.5854151248932,\n      \"last-5-avg\": 979.5854151248932,\n      \"last-10-avg\": 979.5854151248932\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0011016894089691334,\n      \"min\": 0.0011016894089691334,\n      \"avg\": 0.0011016894089691334,\n      \"last\": 0.0011016894089691334,\n      \"last-5-avg\": 0.0011016894089691334,\n      \"last-10-avg\": 0.0011016894089691334\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474000d02b80000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474000d02b80000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fdcdfce00000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fdcdfce00000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e9caeee200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e9caeee200000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944af627dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944af627dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e9caeee200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e9caeee200000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a7e740300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a7e740300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e9caeee200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e9caeee200000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f520cd1fed6fda5612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f520cd1fed6fda5612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625105435.543339,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00040_40_batch_size=128,learning_rate=0.0011017_2021-07-01_04-09-28\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00041\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0010384228374043728,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0010384228374043728,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"41_batch_size=64,learning_rate=0.0010384\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.702620029449463,\n    \"mean_F1\": 0.6113445162773132,\n    \"time_this_iter_s\": 501.39278864860535,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 4,\n    \"experiment_id\": \"1d8a69de26294f38a6cc988505d1ff9f\",\n    \"date\": \"2021-07-01_05-05-58\",\n    \"timestamp\": 1625108758,\n    \"time_total_s\": 3023.426914691925,\n    \"pid\": 227700,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0010384228374043728,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 3023.426914691925,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 4,\n    \"trial_id\": \"deb7c_00041\",\n    \"experiment_tag\": \"41_batch_size=64,learning_rate=0.0010384\"\n  },\n  \"last_update_time\": 1625108758.5955455,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9832614660263062,\n      \"min\": 1.702620029449463,\n      \"avg\": 1.7971824407577515,\n      \"last\": 1.702620029449463,\n      \"last-5-avg\": 1.7971824407577515,\n      \"last-10-avg\": 1.7971824407577515\n    },\n    \"mean_F1\": {\n      \"max\": 0.6113445162773132,\n      \"min\": 0.545586109161377,\n      \"avg\": 0.5867302417755127,\n      \"last\": 0.6113445162773132,\n      \"last-5-avg\": 0.5867302417755127,\n      \"last-10-avg\": 0.5867302417755127\n    },\n    \"time_this_iter_s\": {\n      \"max\": 901.2331495285034,\n      \"min\": 501.39278864860535,\n      \"avg\": 755.8567286729813,\n      \"last\": 501.39278864860535,\n      \"last-5-avg\": 755.8567286729813,\n      \"last-10-avg\": 755.8567286729813\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.25,\n      \"last\": true,\n      \"last-5-avg\": 0.25,\n      \"last-10-avg\": 0.25\n    },\n    \"training_iteration\": {\n      \"max\": 4,\n      \"min\": 1,\n      \"avg\": 2.5,\n      \"last\": 4,\n      \"last-5-avg\": 2.5,\n      \"last-10-avg\": 2.5\n    },\n    \"timestamp\": {\n      \"max\": 1625108758,\n      \"min\": 1625106636,\n      \"avg\": 1625107780.0,\n      \"last\": 1625108758,\n      \"last-5-avg\": 1625107780.0,\n      \"last-10-avg\": 1625107780.0\n    },\n    \"time_total_s\": {\n      \"max\": 3023.426914691925,\n      \"min\": 901.2331495285034,\n      \"avg\": 2045.3110486268997,\n      \"last\": 3023.426914691925,\n      \"last-5-avg\": 2045.3110486268997,\n      \"last-10-avg\": 2045.3110486268997\n    },\n    \"pid\": {\n      \"max\": 227700,\n      \"min\": 227700,\n      \"avg\": 227700.0,\n      \"last\": 227700,\n      \"last-5-avg\": 227700.0,\n      \"last-10-avg\": 227700.0\n    },\n    \"time_since_restore\": {\n      \"max\": 3023.426914691925,\n      \"min\": 901.2331495285034,\n      \"avg\": 2045.3110486268997,\n      \"last\": 3023.426914691925,\n      \"last-5-avg\": 2045.3110486268997,\n      \"last-10-avg\": 2045.3110486268997\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 4,\n      \"min\": 1,\n      \"avg\": 2.5,\n      \"last\": 4,\n      \"last-5-avg\": 2.5,\n      \"last-10-avg\": 2.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0010384228374043728,\n      \"min\": 0.0010384228374043728,\n      \"avg\": 0.0010384228374043726,\n      \"last\": 0.0010384228374043728,\n      \"last-5-avg\": 0.0010384228374043728,\n      \"last-10-avg\": 0.0010384228374043728\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64.0,\n      \"last\": 64,\n      \"last-5-avg\": 64.0,\n      \"last-10-avg\": 64.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fffbb7060000000473ffc36a360000000473ffbd50740000000473ffb3dee80000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fffbb7060000000473ffc36a360000000473ffbd50740000000473ffb3dee80000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe1757100000000473fe2c4cc20000000473fe34f9a80000000473fe3902260000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe1757100000000473fe2c4cc20000000473fe34f9a80000000473fe3902260000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408c29dd7d80000047408a0a88eb2000004740889bdf7b40000047407f5648dcc00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408c29dd7d80000047408a0a88eb2000004740889bdf7b40000047407f5648dcc00000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942889898988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942889898988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b024b034b04652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b04652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284acc28dd604a0d2cdd604a212fdd604a1631dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284acc28dd604a0d2cdd604a212fdd604a1631dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408c29dd7d80000047409b1a33345000004740a3b41178f800004740a79eda94900000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408c29dd7d80000047409b1a33345000004740a3b41178f800004740a79eda94900000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a747903004a747903004a747903004a74790300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059536000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a747903004a747903004a747903004a74790300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408c29dd7d80000047409b1a33345000004740a3b41178f800004740a79eda94900000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408c29dd7d80000047409b1a33345000004740a3b41178f800004740a79eda94900000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b004b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b004b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b024b034b04652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b024b034b04652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f51037608118d63473f51037608118d63473f51037608118d63473f51037608118d63652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059546000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f51037608118d63473f51037608118d63473f51037608118d63473f51037608118d63652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b404b404b404b40652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b404b404b404b40652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b144b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b144b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b324b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b324b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952e000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a774d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952e000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a774d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625105721.4790375,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00041_41_batch_size=64,learning_rate=0.0010384_2021-07-01_04-10-35\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00042\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0009358187998962028,\n    \"batch_size\": 32,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0009358187998962028,\n    \"batch_size\": 32\n  },\n  \"experiment_tag\": \"42_batch_size=32,learning_rate=0.00093582\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.7757059335708618,\n    \"mean_F1\": 0.5983192920684814,\n    \"time_this_iter_s\": 783.965366601944,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"8a23b0ab362746dba05e8a66af41a570\",\n    \"date\": \"2021-07-01_04-44-34\",\n    \"timestamp\": 1625107474,\n    \"time_total_s\": 1615.353111743927,\n    \"pid\": 228204,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0009358187998962028,\n      \"batch_size\": 32,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1615.353111743927,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00042\",\n    \"experiment_tag\": \"42_batch_size=32,learning_rate=0.00093582\"\n  },\n  \"last_update_time\": 1625107474.8332672,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.930653691291809,\n      \"min\": 1.7757059335708618,\n      \"avg\": 1.8531798124313354,\n      \"last\": 1.7757059335708618,\n      \"last-5-avg\": 1.8531798124313354,\n      \"last-10-avg\": 1.8531798124313354\n    },\n    \"mean_F1\": {\n      \"max\": 0.5983192920684814,\n      \"min\": 0.5725677609443665,\n      \"avg\": 0.585443526506424,\n      \"last\": 0.5983192920684814,\n      \"last-5-avg\": 0.585443526506424,\n      \"last-10-avg\": 0.585443526506424\n    },\n    \"time_this_iter_s\": {\n      \"max\": 831.387745141983,\n      \"min\": 783.965366601944,\n      \"avg\": 807.6765558719635,\n      \"last\": 783.965366601944,\n      \"last-5-avg\": 807.6765558719635,\n      \"last-10-avg\": 807.6765558719635\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625107474,\n      \"min\": 1625106690,\n      \"avg\": 1625107082.0,\n      \"last\": 1625107474,\n      \"last-5-avg\": 1625107082.0,\n      \"last-10-avg\": 1625107082.0\n    },\n    \"time_total_s\": {\n      \"max\": 1615.353111743927,\n      \"min\": 831.387745141983,\n      \"avg\": 1223.370428442955,\n      \"last\": 1615.353111743927,\n      \"last-5-avg\": 1223.370428442955,\n      \"last-10-avg\": 1223.370428442955\n    },\n    \"pid\": {\n      \"max\": 228204,\n      \"min\": 228204,\n      \"avg\": 228204.0,\n      \"last\": 228204,\n      \"last-5-avg\": 228204.0,\n      \"last-10-avg\": 228204.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1615.353111743927,\n      \"min\": 831.387745141983,\n      \"avg\": 1223.370428442955,\n      \"last\": 1615.353111743927,\n      \"last-5-avg\": 1223.370428442955,\n      \"last-10-avg\": 1223.370428442955\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0009358187998962028,\n      \"min\": 0.0009358187998962028,\n      \"avg\": 0.0009358187998962028,\n      \"last\": 0.0009358187998962028,\n      \"last-5-avg\": 0.0009358187998962028,\n      \"last-10-avg\": 0.0009358187998962028\n    },\n    \"config/batch_size\": {\n      \"max\": 32,\n      \"min\": 32,\n      \"avg\": 32.0,\n      \"last\": 32,\n      \"last-5-avg\": 32.0,\n      \"last-10-avg\": 32.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffee3f520000000473ffc694aa0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffee3f520000000473ffc694aa0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe25279a0000000473fe3256e80000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe25279a0000000473fe3256e80000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474089fb1a1a2000004740887fb912200000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089fb1a1a2000004740887fb912200000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a0229dd604a122cdd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a0229dd604a122cdd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474089fb1a1a2000004740993d6996200000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089fb1a1a2000004740993d6996200000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a6c7b03004a6c7b0300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a6c7b03004a6c7b0300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474089fb1a1a2000004740993d6996200000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474089fb1a1a2000004740993d6996200000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f4eaa3791fd16c0473f4eaa3791fd16c0652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f4eaa3791fd16c0473f4eaa3791fd16c0652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b204b20652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b204b20652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625105844.9943442,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00042_42_batch_size=32,learning_rate=0.00093582_2021-07-01_04-15-21\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00043\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.001449162587463416,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.001449162587463416,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"43_batch_size=128,learning_rate=0.0014492\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.046736478805542,\n    \"mean_F1\": 0.4774114787578583,\n    \"time_this_iter_s\": 979.3126831054688,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"dcb90704e70f46a08d372b62fff50afa\",\n    \"date\": \"2021-07-01_04-37-27\",\n    \"timestamp\": 1625107047,\n    \"time_total_s\": 979.3126831054688,\n    \"pid\": 229133,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.001449162587463416,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 979.3126831054688,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00043\",\n    \"experiment_tag\": \"43_batch_size=128,learning_rate=0.0014492\"\n  },\n  \"last_update_time\": 1625107048.0365229,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.046736478805542,\n      \"min\": 2.046736478805542,\n      \"avg\": 2.046736478805542,\n      \"last\": 2.046736478805542,\n      \"last-5-avg\": 2.046736478805542,\n      \"last-10-avg\": 2.046736478805542\n    },\n    \"mean_F1\": {\n      \"max\": 0.4774114787578583,\n      \"min\": 0.4774114787578583,\n      \"avg\": 0.4774114787578583,\n      \"last\": 0.4774114787578583,\n      \"last-5-avg\": 0.4774114787578583,\n      \"last-10-avg\": 0.4774114787578583\n    },\n    \"time_this_iter_s\": {\n      \"max\": 979.3126831054688,\n      \"min\": 979.3126831054688,\n      \"avg\": 979.3126831054688,\n      \"last\": 979.3126831054688,\n      \"last-5-avg\": 979.3126831054688,\n      \"last-10-avg\": 979.3126831054688\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625107047,\n      \"min\": 1625107047,\n      \"avg\": 1625107047,\n      \"last\": 1625107047,\n      \"last-5-avg\": 1625107047,\n      \"last-10-avg\": 1625107047\n    },\n    \"time_total_s\": {\n      \"max\": 979.3126831054688,\n      \"min\": 979.3126831054688,\n      \"avg\": 979.3126831054688,\n      \"last\": 979.3126831054688,\n      \"last-5-avg\": 979.3126831054688,\n      \"last-10-avg\": 979.3126831054688\n    },\n    \"pid\": {\n      \"max\": 229133,\n      \"min\": 229133,\n      \"avg\": 229133,\n      \"last\": 229133,\n      \"last-5-avg\": 229133,\n      \"last-10-avg\": 229133\n    },\n    \"time_since_restore\": {\n      \"max\": 979.3126831054688,\n      \"min\": 979.3126831054688,\n      \"avg\": 979.3126831054688,\n      \"last\": 979.3126831054688,\n      \"last-5-avg\": 979.3126831054688,\n      \"last-10-avg\": 979.3126831054688\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.001449162587463416,\n      \"min\": 0.001449162587463416,\n      \"avg\": 0.001449162587463416,\n      \"last\": 0.001449162587463416,\n      \"last-5-avg\": 0.001449162587463416,\n      \"last-10-avg\": 0.001449162587463416\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740005fb760000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740005fb760000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fde8de8e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fde8de8e0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e9a8060000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e9a8060000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a672add60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a672add60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e9a8060000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e9a8060000000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a0d7f0300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a0d7f0300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408e9a8060000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408e9a8060000000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f57be3a7add0e04612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f57be3a7add0e04612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625106054.1236408,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00043_43_batch_size=128,learning_rate=0.0014492_2021-07-01_04-17-25\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00044\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0025992540322068306,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0025992540322068306,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"44_batch_size=16,learning_rate=0.0025993\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.0078787803649902,\n    \"mean_F1\": 0.5327731370925903,\n    \"time_this_iter_s\": 783.1761150360107,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"e06c8cfe00a745949716a6061b03090d\",\n    \"date\": \"2021-07-01_04-36-21\",\n    \"timestamp\": 1625106981,\n    \"time_total_s\": 783.1761150360107,\n    \"pid\": 229730,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0025992540322068306,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 783.1761150360107,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00044\",\n    \"experiment_tag\": \"44_batch_size=16,learning_rate=0.0025993\"\n  },\n  \"last_update_time\": 1625106981.3282478,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.0078787803649902,\n      \"min\": 2.0078787803649902,\n      \"avg\": 2.0078787803649902,\n      \"last\": 2.0078787803649902,\n      \"last-5-avg\": 2.0078787803649902,\n      \"last-10-avg\": 2.0078787803649902\n    },\n    \"mean_F1\": {\n      \"max\": 0.5327731370925903,\n      \"min\": 0.5327731370925903,\n      \"avg\": 0.5327731370925903,\n      \"last\": 0.5327731370925903,\n      \"last-5-avg\": 0.5327731370925903,\n      \"last-10-avg\": 0.5327731370925903\n    },\n    \"time_this_iter_s\": {\n      \"max\": 783.1761150360107,\n      \"min\": 783.1761150360107,\n      \"avg\": 783.1761150360107,\n      \"last\": 783.1761150360107,\n      \"last-5-avg\": 783.1761150360107,\n      \"last-10-avg\": 783.1761150360107\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625106981,\n      \"min\": 1625106981,\n      \"avg\": 1625106981,\n      \"last\": 1625106981,\n      \"last-5-avg\": 1625106981,\n      \"last-10-avg\": 1625106981\n    },\n    \"time_total_s\": {\n      \"max\": 783.1761150360107,\n      \"min\": 783.1761150360107,\n      \"avg\": 783.1761150360107,\n      \"last\": 783.1761150360107,\n      \"last-5-avg\": 783.1761150360107,\n      \"last-10-avg\": 783.1761150360107\n    },\n    \"pid\": {\n      \"max\": 229730,\n      \"min\": 229730,\n      \"avg\": 229730,\n      \"last\": 229730,\n      \"last-5-avg\": 229730,\n      \"last-10-avg\": 229730\n    },\n    \"time_since_restore\": {\n      \"max\": 783.1761150360107,\n      \"min\": 783.1761150360107,\n      \"avg\": 783.1761150360107,\n      \"last\": 783.1761150360107,\n      \"last-5-avg\": 783.1761150360107,\n      \"last-10-avg\": 783.1761150360107\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0025992540322068306,\n      \"min\": 0.0025992540322068306,\n      \"avg\": 0.0025992540322068306,\n      \"last\": 0.0025992540322068306,\n      \"last-5-avg\": 0.0025992540322068306,\n      \"last-10-avg\": 0.0025992540322068306\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16,\n      \"last\": 16,\n      \"last-5-avg\": 16,\n      \"last-10-avg\": 16\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740001022c0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740001022c0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe10c7a40000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe10c7a40000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740887968af000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740887968af000000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a252add60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a252add60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740887968af000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740887968af000000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a62810300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a62810300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944740887968af000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944740887968af000000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f654b07e1fe8fe2612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f654b07e1fe8fe2612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b10612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b10612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625106183.2204485,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00044_44_batch_size=16,learning_rate=0.0025993_2021-07-01_04-20-54\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00045\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.000659986374687523,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.000659986374687523,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"45_batch_size=16,learning_rate=0.00065999\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.8503444194793701,\n    \"mean_F1\": 0.5980138182640076,\n    \"time_this_iter_s\": 762.3994874954224,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"3d538055e8cf429c94b81945e7f326eb\",\n    \"date\": \"2021-07-01_04-53-09\",\n    \"timestamp\": 1625107989,\n    \"time_total_s\": 1552.0488460063934,\n    \"pid\": 230757,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.000659986374687523,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1552.0488460063934,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00045\",\n    \"experiment_tag\": \"45_batch_size=16,learning_rate=0.00065999\"\n  },\n  \"last_update_time\": 1625107989.3658278,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.8966596126556396,\n      \"min\": 1.8503444194793701,\n      \"avg\": 1.8735020160675049,\n      \"last\": 1.8503444194793701,\n      \"last-5-avg\": 1.8735020160675049,\n      \"last-10-avg\": 1.8735020160675049\n    },\n    \"mean_F1\": {\n      \"max\": 0.5980138182640076,\n      \"min\": 0.5815126299858093,\n      \"avg\": 0.5897632241249084,\n      \"last\": 0.5980138182640076,\n      \"last-5-avg\": 0.5897632241249084,\n      \"last-10-avg\": 0.5897632241249084\n    },\n    \"time_this_iter_s\": {\n      \"max\": 789.6493585109711,\n      \"min\": 762.3994874954224,\n      \"avg\": 776.0244230031967,\n      \"last\": 762.3994874954224,\n      \"last-5-avg\": 776.0244230031967,\n      \"last-10-avg\": 776.0244230031967\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625107989,\n      \"min\": 1625107226,\n      \"avg\": 1625107607.5,\n      \"last\": 1625107989,\n      \"last-5-avg\": 1625107607.5,\n      \"last-10-avg\": 1625107607.5\n    },\n    \"time_total_s\": {\n      \"max\": 1552.0488460063934,\n      \"min\": 789.6493585109711,\n      \"avg\": 1170.8491022586823,\n      \"last\": 1552.0488460063934,\n      \"last-5-avg\": 1170.8491022586823,\n      \"last-10-avg\": 1170.8491022586823\n    },\n    \"pid\": {\n      \"max\": 230757,\n      \"min\": 230757,\n      \"avg\": 230757.0,\n      \"last\": 230757,\n      \"last-5-avg\": 230757.0,\n      \"last-10-avg\": 230757.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1552.0488460063934,\n      \"min\": 789.6493585109711,\n      \"avg\": 1170.8491022586823,\n      \"last\": 1552.0488460063934,\n      \"last-5-avg\": 1170.8491022586823,\n      \"last-10-avg\": 1170.8491022586823\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.000659986374687523,\n      \"min\": 0.000659986374687523,\n      \"avg\": 0.000659986374687523,\n      \"last\": 0.000659986374687523,\n      \"last-5-avg\": 0.000659986374687523,\n      \"last-10-avg\": 0.000659986374687523\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16.0,\n      \"last\": 16,\n      \"last-5-avg\": 16.0,\n      \"last-10-avg\": 16.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473ffe58b7c0000000473ffd9b02c0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473ffe58b7c0000000473ffd9b02c0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe29bc060000000473fe322ede0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe29bc060000000473fe322ede0000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474088ad31e2e00000474087d33226800000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088ad31e2e00000474087d33226800000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a1a2bdd604a152edd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a1a2bdd604a152edd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474088ad31e2e00000474098403204b00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088ad31e2e00000474098403204b00000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a658503004a65850300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a658503004a65850300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428474088ad31e2e00000474098403204b00000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428474088ad31e2e00000474098403204b00000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f45a05df29242aa473f45a05df29242aa652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f45a05df29242aa473f45a05df29242aa652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b104b10652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b104b10652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625106423.236755,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00045_45_batch_size=16,learning_rate=0.00065999_2021-07-01_04-23-03\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00046\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0021494260743809284,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0021494260743809284,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"46_batch_size=64,learning_rate=0.0021494\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 1.7856978178024292,\n    \"mean_F1\": 0.5917065143585205,\n    \"time_this_iter_s\": 692.0138819217682,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 2,\n    \"experiment_id\": \"87fdc49abd214a68ab0d46f70eb5112b\",\n    \"date\": \"2021-07-01_05-02-47\",\n    \"timestamp\": 1625108567,\n    \"time_total_s\": 1569.9303939342499,\n    \"pid\": 233064,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0021494260743809284,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 1569.9303939342499,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 2,\n    \"trial_id\": \"deb7c_00046\",\n    \"experiment_tag\": \"46_batch_size=64,learning_rate=0.0021494\"\n  },\n  \"last_update_time\": 1625108567.4391227,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 1.9659699201583862,\n      \"min\": 1.7856978178024292,\n      \"avg\": 1.8758338689804077,\n      \"last\": 1.7856978178024292,\n      \"last-5-avg\": 1.8758338689804077,\n      \"last-10-avg\": 1.8758338689804077\n    },\n    \"mean_F1\": {\n      \"max\": 0.5917065143585205,\n      \"min\": 0.5600578784942627,\n      \"avg\": 0.5758821964263916,\n      \"last\": 0.5917065143585205,\n      \"last-5-avg\": 0.5758821964263916,\n      \"last-10-avg\": 0.5758821964263916\n    },\n    \"time_this_iter_s\": {\n      \"max\": 877.9165120124817,\n      \"min\": 692.0138819217682,\n      \"avg\": 784.9651969671249,\n      \"last\": 692.0138819217682,\n      \"last-5-avg\": 784.9651969671249,\n      \"last-10-avg\": 784.9651969671249\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": false,\n      \"avg\": 0.5,\n      \"last\": true,\n      \"last-5-avg\": 0.5,\n      \"last-10-avg\": 0.5\n    },\n    \"training_iteration\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"timestamp\": {\n      \"max\": 1625108567,\n      \"min\": 1625107875,\n      \"avg\": 1625108221.0,\n      \"last\": 1625108567,\n      \"last-5-avg\": 1625108221.0,\n      \"last-10-avg\": 1625108221.0\n    },\n    \"time_total_s\": {\n      \"max\": 1569.9303939342499,\n      \"min\": 877.9165120124817,\n      \"avg\": 1223.9234529733658,\n      \"last\": 1569.9303939342499,\n      \"last-5-avg\": 1223.9234529733658,\n      \"last-10-avg\": 1223.9234529733658\n    },\n    \"pid\": {\n      \"max\": 233064,\n      \"min\": 233064,\n      \"avg\": 233064.0,\n      \"last\": 233064,\n      \"last-5-avg\": 233064.0,\n      \"last-10-avg\": 233064.0\n    },\n    \"time_since_restore\": {\n      \"max\": 1569.9303939342499,\n      \"min\": 877.9165120124817,\n      \"avg\": 1223.9234529733658,\n      \"last\": 1569.9303939342499,\n      \"last-5-avg\": 1223.9234529733658,\n      \"last-10-avg\": 1223.9234529733658\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0.0,\n      \"last\": 0,\n      \"last-5-avg\": 0.0,\n      \"last-10-avg\": 0.0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 2,\n      \"min\": 1,\n      \"avg\": 1.5,\n      \"last\": 2,\n      \"last-5-avg\": 1.5,\n      \"last-10-avg\": 1.5\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0021494260743809284,\n      \"min\": 0.0021494260743809284,\n      \"avg\": 0.0021494260743809284,\n      \"last\": 0.0021494260743809284,\n      \"last-5-avg\": 0.0021494260743809284,\n      \"last-10-avg\": 0.0021494260743809284\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64.0,\n      \"last\": 64,\n      \"last-5-avg\": 64.0,\n      \"last-10-avg\": 64.0\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20.0,\n      \"last\": 20,\n      \"last-5-avg\": 20.0,\n      \"last-10-avg\": 20.0\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50.0,\n      \"last\": 50,\n      \"last-5-avg\": 50.0,\n      \"last-10-avg\": 50.0\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522.0,\n      \"last\": 30522,\n      \"last-5-avg\": 30522.0,\n      \"last-10-avg\": 30522.0\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fff749ce0000000473ffc9237e0000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fff749ce0000000473ffc9237e0000000652e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473fe1ebfe80000000473fe2ef4280000000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473fe1ebfe80000000473fe2ef4280000000652e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b6f5504400000474085a01c6e200000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b6f5504400000474085a01c6e200000652e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294288988652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294288988652e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284aa32ddd604a5730dd60652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284aa32ddd604a5730dd60652e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b6f550440000047409887b8b9300000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b6f550440000047409887b8b9300000652e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284a688e03004a688e0300652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952c000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284a688e03004a688e0300652e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452942847408b6f550440000047409887b8b9300000652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452942847408b6f550440000047409887b8b9300000652e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b004b00652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b004b00652e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b014b02652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b014b02652e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529428473f619bac563a74c1473f619bac563a74c1652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059534000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529428473f619bac563a74c1473f619bac563a74c1652e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b404b40652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b404b40652e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b144b14652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b144b14652e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284b324b32652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284b324b32652e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294284d3a774d3a77652e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059528000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294284d3a774d3a77652e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625106981.6811168,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00046_46_batch_size=64,learning_rate=0.0021494_2021-07-01_04-27-03\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00047\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0020831638794788776,\n    \"batch_size\": 16,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0020831638794788776,\n    \"batch_size\": 16\n  },\n  \"experiment_tag\": \"47_batch_size=16,learning_rate=0.0020832\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.1001663208007812,\n    \"mean_F1\": 0.5050420165061951,\n    \"time_this_iter_s\": 773.0828325748444,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"0da8b719410b44ea9de55c63252d76a8\",\n    \"date\": \"2021-07-01_04-50-34\",\n    \"timestamp\": 1625107834,\n    \"time_total_s\": 773.0828325748444,\n    \"pid\": 233423,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0020831638794788776,\n      \"batch_size\": 16,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 773.0828325748444,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00047\",\n    \"experiment_tag\": \"47_batch_size=16,learning_rate=0.0020832\"\n  },\n  \"last_update_time\": 1625107834.5796363,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.1001663208007812,\n      \"min\": 2.1001663208007812,\n      \"avg\": 2.1001663208007812,\n      \"last\": 2.1001663208007812,\n      \"last-5-avg\": 2.1001663208007812,\n      \"last-10-avg\": 2.1001663208007812\n    },\n    \"mean_F1\": {\n      \"max\": 0.5050420165061951,\n      \"min\": 0.5050420165061951,\n      \"avg\": 0.5050420165061951,\n      \"last\": 0.5050420165061951,\n      \"last-5-avg\": 0.5050420165061951,\n      \"last-10-avg\": 0.5050420165061951\n    },\n    \"time_this_iter_s\": {\n      \"max\": 773.0828325748444,\n      \"min\": 773.0828325748444,\n      \"avg\": 773.0828325748444,\n      \"last\": 773.0828325748444,\n      \"last-5-avg\": 773.0828325748444,\n      \"last-10-avg\": 773.0828325748444\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625107834,\n      \"min\": 1625107834,\n      \"avg\": 1625107834,\n      \"last\": 1625107834,\n      \"last-5-avg\": 1625107834,\n      \"last-10-avg\": 1625107834\n    },\n    \"time_total_s\": {\n      \"max\": 773.0828325748444,\n      \"min\": 773.0828325748444,\n      \"avg\": 773.0828325748444,\n      \"last\": 773.0828325748444,\n      \"last-5-avg\": 773.0828325748444,\n      \"last-10-avg\": 773.0828325748444\n    },\n    \"pid\": {\n      \"max\": 233423,\n      \"min\": 233423,\n      \"avg\": 233423,\n      \"last\": 233423,\n      \"last-5-avg\": 233423,\n      \"last-10-avg\": 233423\n    },\n    \"time_since_restore\": {\n      \"max\": 773.0828325748444,\n      \"min\": 773.0828325748444,\n      \"avg\": 773.0828325748444,\n      \"last\": 773.0828325748444,\n      \"last-5-avg\": 773.0828325748444,\n      \"last-10-avg\": 773.0828325748444\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0020831638794788776,\n      \"min\": 0.0020831638794788776,\n      \"avg\": 0.0020831638794788776,\n      \"last\": 0.0020831638794788776,\n      \"last-5-avg\": 0.0020831638794788776,\n      \"last-10-avg\": 0.0020831638794788776\n    },\n    \"config/batch_size\": {\n      \"max\": 16,\n      \"min\": 16,\n      \"avg\": 16,\n      \"last\": 16,\n      \"last-5-avg\": 16,\n      \"last-10-avg\": 16\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474000cd2400000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474000cd2400000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe0294de0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe0294de0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408828a9a4200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408828a9a4200000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a7a2ddd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a7a2ddd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408828a9a4200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408828a9a4200000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944acf8f0300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944acf8f0300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408828a9a4200000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408828a9a4200000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f6110b617819980612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f6110b617819980612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b10612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b10612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625107048.1979487,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00047_47_batch_size=16,learning_rate=0.0020832_2021-07-01_04-36-21\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00048\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0016807853000701104,\n    \"batch_size\": 128,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0016807853000701104,\n    \"batch_size\": 128\n  },\n  \"experiment_tag\": \"48_batch_size=128,learning_rate=0.0016808\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.0399088859558105,\n    \"mean_F1\": 0.45787546038627625,\n    \"time_this_iter_s\": 888.0504639148712,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"1739fa28854b40ab83aa9af22e9ba79b\",\n    \"date\": \"2021-07-01_04-59-38\",\n    \"timestamp\": 1625108378,\n    \"time_total_s\": 888.0504639148712,\n    \"pid\": 235338,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0016807853000701104,\n      \"batch_size\": 128,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 888.0504639148712,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00048\",\n    \"experiment_tag\": \"48_batch_size=128,learning_rate=0.0016808\"\n  },\n  \"last_update_time\": 1625108378.675282,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.0399088859558105,\n      \"min\": 2.0399088859558105,\n      \"avg\": 2.0399088859558105,\n      \"last\": 2.0399088859558105,\n      \"last-5-avg\": 2.0399088859558105,\n      \"last-10-avg\": 2.0399088859558105\n    },\n    \"mean_F1\": {\n      \"max\": 0.45787546038627625,\n      \"min\": 0.45787546038627625,\n      \"avg\": 0.45787546038627625,\n      \"last\": 0.45787546038627625,\n      \"last-5-avg\": 0.45787546038627625,\n      \"last-10-avg\": 0.45787546038627625\n    },\n    \"time_this_iter_s\": {\n      \"max\": 888.0504639148712,\n      \"min\": 888.0504639148712,\n      \"avg\": 888.0504639148712,\n      \"last\": 888.0504639148712,\n      \"last-5-avg\": 888.0504639148712,\n      \"last-10-avg\": 888.0504639148712\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625108378,\n      \"min\": 1625108378,\n      \"avg\": 1625108378,\n      \"last\": 1625108378,\n      \"last-5-avg\": 1625108378,\n      \"last-10-avg\": 1625108378\n    },\n    \"time_total_s\": {\n      \"max\": 888.0504639148712,\n      \"min\": 888.0504639148712,\n      \"avg\": 888.0504639148712,\n      \"last\": 888.0504639148712,\n      \"last-5-avg\": 888.0504639148712,\n      \"last-10-avg\": 888.0504639148712\n    },\n    \"pid\": {\n      \"max\": 235338,\n      \"min\": 235338,\n      \"avg\": 235338,\n      \"last\": 235338,\n      \"last-5-avg\": 235338,\n      \"last-10-avg\": 235338\n    },\n    \"time_since_restore\": {\n      \"max\": 888.0504639148712,\n      \"min\": 888.0504639148712,\n      \"avg\": 888.0504639148712,\n      \"last\": 888.0504639148712,\n      \"last-5-avg\": 888.0504639148712,\n      \"last-10-avg\": 888.0504639148712\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0016807853000701104,\n      \"min\": 0.0016807853000701104,\n      \"avg\": 0.0016807853000701104,\n      \"last\": 0.0016807853000701104,\n      \"last-5-avg\": 0.0016807853000701104,\n      \"last-10-avg\": 0.0016807853000701104\n    },\n    \"config/batch_size\": {\n      \"max\": 128,\n      \"min\": 128,\n      \"avg\": 128,\n      \"last\": 128,\n      \"last-5-avg\": 128,\n      \"last-10-avg\": 128\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447400051bbc0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447400051bbc0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fdd4dd4e0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fdd4dd4e0000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408bc06759a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408bc06759a00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a9a2fdd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a9a2fdd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408bc06759a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408bc06759a00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a4a970300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a4a970300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447408bc06759a00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447408bc06759a00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f5b89b9794e363a612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f5b89b9794e363a612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b80612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b80612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625107474.9746706,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00048_48_batch_size=128,learning_rate=0.0016808_2021-07-01_04-37-28\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}",
    "{\n  \"trainable_name\": \"train_tune\",\n  \"trial_id\": \"deb7c_00049\",\n  \"config\": {\n    \"dataset_path\": \"../../data/raw/SemEval/\",\n    \"learning_rate\": 0.0007310654083454644,\n    \"batch_size\": 64,\n    \"epochs\": 20,\n    \"num_trials\": 50\n  },\n  \"local_dir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6\",\n  \"evaluated_params\": {\n    \"learning_rate\": 0.0007310654083454644,\n    \"batch_size\": 64\n  },\n  \"experiment_tag\": \"49_batch_size=64,learning_rate=0.00073107\",\n  \"location\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"8005953a000000000000008c0e7261792e74756e652e747269616c948c084c6f636174696f6e9493942981947d94288c08686f73746e616d65944e8c03706964944e75622e\"\n  },\n  \"resources\": {\n    \"cpu\": 1,\n    \"gpu\": 0,\n    \"memory\": 0,\n    \"object_store_memory\": 0,\n    \"extra_cpu\": 0,\n    \"extra_gpu\": 0,\n    \"extra_memory\": 0,\n    \"extra_object_store_memory\": 0,\n    \"custom_resources\": {},\n    \"extra_custom_resources\": {}\n  },\n  \"placement_group_factory\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"800595b2000000000000008c1f7261792e74756e652e7574696c732e706c6163656d656e745f67726f757073948c15506c6163656d656e7447726f7570466163746f72799493942981947d94288c085f62756e646c6573945d947d94288c03435055944b018c03475055944b008c066d656d6f7279944b008c136f626a6563745f73746f72655f6d656d6f7279944b0075618c095f7374726174656779948c045041434b948c055f6172677394298c075f6b7761726773947d9475622e\"\n  },\n  \"stopping_criterion\": {},\n  \"log_to_file\": [\n    null,\n    null\n  ],\n  \"max_failures\": 0,\n  \"last_result\": {\n    \"loss\": 2.040360927581787,\n    \"mean_F1\": 0.5412445664405823,\n    \"time_this_iter_s\": 730.9431931972504,\n    \"done\": true,\n    \"timesteps_total\": null,\n    \"episodes_total\": null,\n    \"training_iteration\": 1,\n    \"experiment_id\": \"5e90bf34896342caab4696e0bd941dce\",\n    \"date\": \"2021-07-01_05-02-59\",\n    \"timestamp\": 1625108579,\n    \"time_total_s\": 730.9431931972504,\n    \"pid\": 236834,\n    \"hostname\": \"TP-X1-C7\",\n    \"node_ip\": \"192.168.178.28\",\n    \"config\": {\n      \"dataset_path\": \"../../data/raw/SemEval/\",\n      \"learning_rate\": 0.0007310654083454644,\n      \"batch_size\": 64,\n      \"epochs\": 20,\n      \"num_trials\": 50,\n      \"vocab_size\": 30522,\n      \"target_encoding\": {\n        \"0\": \"Atheism\",\n        \"1\": \"Climate Change is a Real Concern\",\n        \"2\": \"Feminist Movement\",\n        \"3\": \"Hillary Clinton\",\n        \"4\": \"Legalization of Abortion\"\n      },\n      \"stance_encoding\": {\n        \"0\": \"AGAINST\",\n        \"1\": \"FAVOR\",\n        \"2\": \"NONE\",\n        \"3\": \"UNKNOWN\"\n      }\n    },\n    \"time_since_restore\": 730.9431931972504,\n    \"timesteps_since_restore\": 0,\n    \"iterations_since_restore\": 1,\n    \"trial_id\": \"deb7c_00049\",\n    \"experiment_tag\": \"49_batch_size=64,learning_rate=0.00073107\"\n  },\n  \"last_update_time\": 1625108579.680181,\n  \"metric_analysis\": {\n    \"loss\": {\n      \"max\": 2.040360927581787,\n      \"min\": 2.040360927581787,\n      \"avg\": 2.040360927581787,\n      \"last\": 2.040360927581787,\n      \"last-5-avg\": 2.040360927581787,\n      \"last-10-avg\": 2.040360927581787\n    },\n    \"mean_F1\": {\n      \"max\": 0.5412445664405823,\n      \"min\": 0.5412445664405823,\n      \"avg\": 0.5412445664405823,\n      \"last\": 0.5412445664405823,\n      \"last-5-avg\": 0.5412445664405823,\n      \"last-10-avg\": 0.5412445664405823\n    },\n    \"time_this_iter_s\": {\n      \"max\": 730.9431931972504,\n      \"min\": 730.9431931972504,\n      \"avg\": 730.9431931972504,\n      \"last\": 730.9431931972504,\n      \"last-5-avg\": 730.9431931972504,\n      \"last-10-avg\": 730.9431931972504\n    },\n    \"done\": {\n      \"max\": true,\n      \"min\": true,\n      \"avg\": true,\n      \"last\": true,\n      \"last-5-avg\": true,\n      \"last-10-avg\": true\n    },\n    \"training_iteration\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"timestamp\": {\n      \"max\": 1625108579,\n      \"min\": 1625108579,\n      \"avg\": 1625108579,\n      \"last\": 1625108579,\n      \"last-5-avg\": 1625108579,\n      \"last-10-avg\": 1625108579\n    },\n    \"time_total_s\": {\n      \"max\": 730.9431931972504,\n      \"min\": 730.9431931972504,\n      \"avg\": 730.9431931972504,\n      \"last\": 730.9431931972504,\n      \"last-5-avg\": 730.9431931972504,\n      \"last-10-avg\": 730.9431931972504\n    },\n    \"pid\": {\n      \"max\": 236834,\n      \"min\": 236834,\n      \"avg\": 236834,\n      \"last\": 236834,\n      \"last-5-avg\": 236834,\n      \"last-10-avg\": 236834\n    },\n    \"time_since_restore\": {\n      \"max\": 730.9431931972504,\n      \"min\": 730.9431931972504,\n      \"avg\": 730.9431931972504,\n      \"last\": 730.9431931972504,\n      \"last-5-avg\": 730.9431931972504,\n      \"last-10-avg\": 730.9431931972504\n    },\n    \"timesteps_since_restore\": {\n      \"max\": 0,\n      \"min\": 0,\n      \"avg\": 0,\n      \"last\": 0,\n      \"last-5-avg\": 0,\n      \"last-10-avg\": 0\n    },\n    \"iterations_since_restore\": {\n      \"max\": 1,\n      \"min\": 1,\n      \"avg\": 1,\n      \"last\": 1,\n      \"last-5-avg\": 1,\n      \"last-10-avg\": 1\n    },\n    \"config/learning_rate\": {\n      \"max\": 0.0007310654083454644,\n      \"min\": 0.0007310654083454644,\n      \"avg\": 0.0007310654083454644,\n      \"last\": 0.0007310654083454644,\n      \"last-5-avg\": 0.0007310654083454644,\n      \"last-10-avg\": 0.0007310654083454644\n    },\n    \"config/batch_size\": {\n      \"max\": 64,\n      \"min\": 64,\n      \"avg\": 64,\n      \"last\": 64,\n      \"last-5-avg\": 64,\n      \"last-10-avg\": 64\n    },\n    \"config/epochs\": {\n      \"max\": 20,\n      \"min\": 20,\n      \"avg\": 20,\n      \"last\": 20,\n      \"last-5-avg\": 20,\n      \"last-10-avg\": 20\n    },\n    \"config/num_trials\": {\n      \"max\": 50,\n      \"min\": 50,\n      \"avg\": 50,\n      \"last\": 50,\n      \"last-5-avg\": 50,\n      \"last-10-avg\": 50\n    },\n    \"config/vocab_size\": {\n      \"max\": 30522,\n      \"min\": 30522,\n      \"avg\": 30522,\n      \"last\": 30522,\n      \"last-5-avg\": 30522,\n      \"last-10-avg\": 30522\n    }\n  },\n  \"n_steps\": [\n    5,\n    10\n  ],\n  \"metric_n_steps\": {\n    \"loss\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529447400052a8c0000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529447400052a8c0000000612e\"\n      }\n    },\n    \"mean_F1\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473fe151e020000000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473fe151e020000000612e\"\n      }\n    },\n    \"time_this_iter_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474086d78ba8e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474086d78ba8e00000612e\"\n      }\n    },\n    \"done\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b058694529488612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059522000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a8694529488612e\"\n      }\n    },\n    \"training_iteration\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"timestamp\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a6330dd60612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a6330dd60612e\"\n      }\n    },\n    \"time_total_s\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474086d78ba8e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474086d78ba8e00000612e\"\n      }\n    },\n    \"pid\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944a229d0300612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059526000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944a229d0300612e\"\n      }\n    },\n    \"time_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294474086d78ba8e00000612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294474086d78ba8e00000612e\"\n      }\n    },\n    \"timesteps_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b00612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b00612e\"\n      }\n    },\n    \"iterations_since_restore\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b01612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b01612e\"\n      }\n    },\n    \"config/learning_rate\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0586945294473f47f49f029200be612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"8005952a000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a86945294473f47f49f029200be612e\"\n      }\n    },\n    \"config/batch_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b40612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b40612e\"\n      }\n    },\n    \"config/epochs\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b14612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b14612e\"\n      }\n    },\n    \"config/num_trials\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944b32612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059523000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944b32612e\"\n      }\n    },\n    \"config/vocab_size\": {\n      \"5\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b05869452944d3a77612e\"\n      },\n      \"10\": {\n        \"_type\": \"CLOUDPICKLE_FALLBACK\",\n        \"value\": \"80059524000000000000008c0b636f6c6c656374696f6e73948c056465717565949394294b0a869452944d3a77612e\"\n      }\n    }\n  },\n  \"export_formats\": [],\n  \"status\": \"TERMINATED\",\n  \"start_time\": 1625107834.7853107,\n  \"logdir\": \"/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/train_tune_deb7c_00049_49_batch_size=64,learning_rate=0.00073107_2021-07-01_04-44-35\",\n  \"runner\": null,\n  \"last_debug\": 0,\n  \"error_file\": null,\n  \"error_msg\": null,\n  \"trial_name_creator\": null,\n  \"custom_trial_name\": null,\n  \"custom_dirname\": null,\n  \"saving_to\": null,\n  \"remote_checkpoint_dir_prefix\": null,\n  \"checkpoint_freq\": 0,\n  \"checkpoint_at_end\": false,\n  \"keep_checkpoints_num\": null,\n  \"checkpoint_score_attr\": \"training_iteration\",\n  \"sync_on_checkpoint\": true,\n  \"checkpoint_manager\": {\n    \"_type\": \"CLOUDPICKLE_FALLBACK\",\n    \"value\": \"80059566010000000000008c1b7261792e74756e652e636865636b706f696e745f6d616e61676572948c11436865636b706f696e744d616e616765729493942981947d94288c146b6565705f636865636b706f696e74735f6e756d94477ff00000000000008c165f636865636b706f696e745f73636f72655f6465736394898c165f636865636b706f696e745f73636f72655f61747472948c12747261696e696e675f697465726174696f6e948c1c6e65776573745f70657273697374656e745f636865636b706f696e749468008c0a436865636b706f696e749493942981947d94288c0773746f72616765948c0a70657273697374656e74948c0576616c7565944e8c06726573756c74947d9475628c195f6e65776573745f6d656d6f72795f636865636b706f696e7494680b2981947d9428680e8c066d656d6f72799468104e68117d9475628c115f626573745f636865636b706f696e7473945d948c0b5f6d656d62657273686970948f9475622e\"\n  },\n  \"restore_path\": null,\n  \"restoring_from\": null,\n  \"num_failures\": 0,\n  \"results\": \"80054e2e\",\n  \"best_result\": \"80054e2e\",\n  \"param_config\": \"80054e2e\",\n  \"extra_arg\": \"80054e2e\",\n  \"_state_json\": null,\n  \"_state_valid\": false\n}"
  ],
  "runner_data": {
    "_max_pending_trials": 16,
    "_metric": "loss",
    "_total_time": 157734.31222105026,
    "_iteration": 2739,
    "_has_errored": false,
    "_fail_fast": false,
    "_server_port": null,
    "_cached_trial_decisions": {},
    "_queued_trial_decisions": {},
    "_updated_queue": true,
    "_should_stop_experiment": false,
    "_local_checkpoint_dir": "/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6",
    "_remote_checkpoint_dir": null,
    "_stopper": {
      "_type": "CLOUDPICKLE_FALLBACK",
      "value": "80059527000000000000008c107261792e74756e652e73746f70706572948c0b4e6f6f7053746f707065729493942981942e"
    },
    "_resumed": false,
    "_start_time": 1625089264.277035,
    "_last_checkpoint_time": -Infinity,
    "_session_str": "2021-06-30_23-41-04",
    "checkpoint_file": "/home/user/Documents/Github/Uni/Master/TUM_Praktikum_NLP_Explainability/understanding-opinions-on-social-media/logs/StancePrediction_SemEval/ray_results/version_6/experiment_state-2021-06-30_23-41-04.json",
    "_checkpoint_period": "auto",
    "launch_web_server": false
  },
  "stats": {
    "start_time": 1625089264.277035,
    "timestamp": 1625111642.6396089
  }
}